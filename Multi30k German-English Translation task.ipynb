{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T07:52:06.903790Z",
     "start_time": "2023-02-23T07:52:02.854054Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import spacy # 一个自然语言文本处理库\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "from torchtext.data.functional import to_map_style_dataset  # 将迭代器转化为 Dataset 类型，可直接索引\n",
    "\n",
    "\n",
    "from transformer_package import make_model, scheduler, LabelSmoothingKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T07:52:36.031152Z",
     "start_time": "2023-02-23T07:52:36.018154Z"
    }
   },
   "outputs": [],
   "source": [
    "class ComputeLoss:\n",
    "    def __init__(self, criterion):\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        x = F.log_softmax(x, dim=-1)  # if KL divergence is used\n",
    "        loss = (\n",
    "            self.criterion(    # x: [b, len, vocab_size] ---> [b*len, vocab_size];  y: [b, len] ---> [b*len, ]\n",
    "                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n",
    "            )\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T07:52:36.062064Z",
     "start_time": "2023-02-23T07:52:36.035108Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(epoch, data_loader, model, loss_compute, optimizer, scheduler, padding_idx):\n",
    "    \"\"\"Train a single epoch.\"\"\"\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    model.train()\n",
    "    for i, (src, tgt) in enumerate(data_loader):\n",
    "        # src, tgt shape: [batch_size, max_len]\n",
    "        tgt_y = copy.deepcopy(tgt[:, 1:])  # 真实的序列，用来构建loss，第一位往往是起始符\n",
    "        tgt_seq = copy.deepcopy(tgt[:, :-1])   # 输入decoder的序列，最后一位的token是用不到的。因为按照decoder的工作原理，最后一个token的生成是不会依赖到最后一个token的信息的。\n",
    "        ntokens = (tgt_y != padding_idx).data.sum()\n",
    "        \n",
    "        # get padding mask and sequence mask\n",
    "        src_mask = model.padding_mask(src, padding_idx)\n",
    "        tgt_mask = model.padding_mask(tgt_seq, padding_idx) & model.sequence_mask(tgt_seq.size(-1))\n",
    "        \n",
    "        # train\n",
    "        logit = model(src, tgt_seq, src_mask, tgt_mask)\n",
    "        loss = loss_compute(logit, tgt_y)\n",
    "        \n",
    "        \n",
    "        total_loss += loss\n",
    "        total_tokens += ntokens\n",
    "        \n",
    "        loss /= ntokens # mean loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True) # 优化内存使用\n",
    "        scheduler.step()\n",
    "\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        elapsed = time.time() - start\n",
    "        print(\n",
    "            (\n",
    "                \"| Epoch {:3d} | {:5d}/{:5d} batches | Loss: {:6.2f} \"\n",
    "                + \"| Tokens: {:5d} | Learning Rate: {:6.1e} | Time: {} |\"\n",
    "            ).format(epoch, i, len(data_loader), loss, ntokens, lr, timedelta(seconds=elapsed))\n",
    "        )\n",
    "        \n",
    "        del loss\n",
    "\n",
    "    return total_loss / total_tokens\n",
    "\n",
    "\n",
    "def eval_epoch(epoch, data_loader, model, loss_compute, padding_idx):\n",
    "    \"\"\"Eval a single epoch.\"\"\"\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (src, tgt) in enumerate(data_loader):\n",
    "            # src, tgt shape: [batch_size, max_len]\n",
    "            tgt_y = copy.deepcopy(tgt[:, 1:])  # 真实的序列，用来构建loss，第一位往往是起始符\n",
    "            tgt_seq = copy.deepcopy(tgt[:, :-1])   # 输入decoder的序列，最后一位的token是用不到的。因为按照decoder的工作原理，最后一个token的生成是不会依赖到最后一个token的信息的。\n",
    "            ntokens = (tgt_y != padding_idx).data.sum()\n",
    "            \n",
    "            # get padding mask and sequence mask\n",
    "            src_mask = model.padding_mask(src, padding_idx)\n",
    "            tgt_mask = model.padding_mask(tgt_seq, padding_idx) & model.sequence_mask(tgt_seq.size(-1))\n",
    "            \n",
    "            # train\n",
    "            logit = model(src, tgt_seq, src_mask, tgt_mask)\n",
    "            loss = loss_compute(logit, tgt_y)\n",
    "        \n",
    "\n",
    "            total_loss += loss\n",
    "            total_tokens += ntokens\n",
    "            \n",
    "            \n",
    "            loss /= ntokens # mean loss\n",
    "    \n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                (\n",
    "                    \"| Epoch {:3d} | {:5d}/{:5d} batches | Loss: {:6.2f} \"\n",
    "                    + \"| Tokens: {:5d} | Time: {} |\"\n",
    "                ).format(epoch, i, len(data_loader), loss, ntokens, timedelta(seconds=elapsed))\n",
    "            )\n",
    "            \n",
    "            del loss\n",
    "            \n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi30k German-English Translation task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T07:52:38.158345Z",
     "start_time": "2023-02-23T07:52:38.150334Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_tokenizers():\n",
    "\n",
    "    try:\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download de_core_news_sm\")  # 大概率要翻墙才行\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    try:\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    return spacy_de, spacy_en\n",
    "\n",
    "# spacy_de, spacy_en = load_tokenizers()\n",
    "# doc = spacy_en.tokenizer(\"This is a sentence.\")\n",
    "# print([(w.text, w.pos_) for w in doc])\n",
    "\n",
    "\n",
    "def tokenize(text, tokenizer):\n",
    "    return [tok.text for tok in tokenizer.tokenizer(text)]\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter, tokenizer, index):\n",
    "    for from_to_tuple in data_iter:\n",
    "        yield tokenizer(from_to_tuple[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T07:52:39.583589Z",
     "start_time": "2023-02-23T07:52:39.572618Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(spacy_de, spacy_en):\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(text, spacy_de)\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return tokenize(text, spacy_en)\n",
    "\n",
    "    print(\"Building German Vocabulary ...\")\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    vocab_src = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val + test, tokenize_de, index=0),  # 'de'\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],   # 分别代表起始符、终止符、padding字符、未知字符\n",
    "    )\n",
    "\n",
    "    print(\"Building English Vocabulary ...\")\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    vocab_tgt = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val + test, tokenize_en, index=1),  # 'en'\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    vocab_src.set_default_index(vocab_src[\"<unk>\"])  # This index will be returned when OOV token is queried.\n",
    "    vocab_tgt.set_default_index(vocab_tgt[\"<unk>\"])\n",
    "\n",
    "    return vocab_src, vocab_tgt\n",
    "\n",
    "\n",
    "\n",
    "def load_vocab(spacy_de, spacy_en):\n",
    "    if not os.path.exists(\"vocab.pt\"):\n",
    "        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)\n",
    "        torch.save((vocab_src, vocab_tgt), \"vocab.pt\")\n",
    "    else:\n",
    "        vocab_src, vocab_tgt = torch.load(\"vocab.pt\")\n",
    "    print(\"Finished.\\nVocabulary sizes:\")\n",
    "    print(len(vocab_src))\n",
    "    print(len(vocab_tgt))\n",
    "    return vocab_src, vocab_tgt\n",
    "\n",
    "\n",
    "\n",
    "# vocab_src, vocab_tgt = load_vocab(spacy_de, spacy_en)\n",
    "# print(vocab_src.get_stoi())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T07:52:41.112795Z",
     "start_time": "2023-02-23T07:52:41.088857Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch, src_pipline, tgt_pipline, src_vocab, tgt_vocab, max_padding=128, pad_id=2):  # <blank> token id\n",
    "    '''\n",
    "    负责在 DataLoad 提取一个 batch 的样本时，完成一系列预处理工作。\n",
    "    所以，我们将 collate_batch 函数通过参数 collate_fn 传入 DataLoader，\n",
    "    即可实现对变长数据的处理。\n",
    "    '''\n",
    "    bs_id = torch.tensor([0])  # <s> token id\n",
    "    eos_id = torch.tensor([1])  # </s> token id\n",
    "    src_list, tgt_list = [], []\n",
    "    \n",
    "    for (src, tgt) in batch:\n",
    "        # 为每一句话添加起始符和结束符\n",
    "        processed_src = torch.cat([bs_id, \n",
    "                                  torch.as_tensor(src_vocab(src_pipline(src)), dtype=torch.int64),\n",
    "                                  eos_id],\n",
    "                                  dim=0)\n",
    "        processed_tgt = torch.cat([bs_id, \n",
    "                                  torch.as_tensor(tgt_vocab(tgt_pipline(tgt)), dtype=torch.int64),\n",
    "                                  eos_id],\n",
    "                                  dim=0)\n",
    "        \n",
    "        # 给长度不足max_padding的句子打padding\n",
    "        processed_src = F.pad(processed_src, (0, max_padding - len(processed_src)), value=pad_id)  # # warning - overwrites values for negative values of padding - len\n",
    "        src_list.append(processed_src)\n",
    "        processed_tgt = F.pad(processed_tgt, (0, max_padding - len(processed_tgt)), value=pad_id)\n",
    "        tgt_list.append(processed_tgt)\n",
    "        \n",
    "       \n",
    "    src_batch = torch.stack(src_list)\n",
    "    tgt_batch = torch.stack(tgt_list)\n",
    "    \n",
    "    return (src_batch, tgt_batch)\n",
    "\n",
    "\n",
    "\n",
    "def create_dataloaders(vocab_src, vocab_tgt, spacy_de, spacy_en, batch_size=512, max_padding=128):\n",
    "    \n",
    "    def tokenize_de(text):    # src_pipline\n",
    "        return tokenize(text, spacy_de)\n",
    "\n",
    "    def tokenize_en(text):   # tgt_pipline\n",
    "        return tokenize(text, spacy_en)\n",
    "        \n",
    "    def collate_fn(batch):\n",
    "        return collate_batch(batch, \n",
    "                             tokenize_de, \n",
    "                             tokenize_en, \n",
    "                             vocab_src, \n",
    "                             vocab_tgt,\n",
    "                             max_padding=max_padding,\n",
    "                             pad_id=vocab_src.get_stoi()['<blank>'])\n",
    "    \n",
    "    train_iter, valid_iter, test_iter = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    \n",
    "    train_iter_map = to_map_style_dataset(train_iter)\n",
    "    valid_iter_map = to_map_style_dataset(valid_iter)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_iter_map,\n",
    "                                  batch_size,\n",
    "                                  collate_fn=collate_fn)\n",
    "\n",
    "    valid_dataloader = DataLoader(valid_iter_map,\n",
    "                                  batch_size,\n",
    "                                  collate_fn=collate_fn)\n",
    "\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "\n",
    "# train_dataloader, valid_dataloader = create_dataloaders(vocab_src, vocab_tgt, spacy_de, spacy_en, batch_size=512, max_padding=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-02-23T07:53:04.769Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "Vocabulary sizes:\n",
      "8185\n",
      "6291\n",
      "| Epoch   0 |     0/  907 batches | Loss:   7.67 | Tokens:   434 | Learning Rate: 2.7e-07 | Time: 0:00:08.505929 |\n",
      "| Epoch   0 |     1/  907 batches | Loss:   7.67 | Tokens:   457 | Learning Rate: 5.4e-07 | Time: 0:00:17.537343 |\n",
      "| Epoch   0 |     2/  907 batches | Loss:   7.66 | Tokens:   451 | Learning Rate: 8.1e-07 | Time: 0:00:27.362524 |\n",
      "| Epoch   0 |     3/  907 batches | Loss:   7.67 | Tokens:   450 | Learning Rate: 1.1e-06 | Time: 0:00:36.351264 |\n",
      "| Epoch   0 |     4/  907 batches | Loss:   7.66 | Tokens:   461 | Learning Rate: 1.3e-06 | Time: 0:00:45.246767 |\n",
      "| Epoch   0 |     5/  907 batches | Loss:   7.68 | Tokens:   428 | Learning Rate: 1.6e-06 | Time: 0:00:54.132383 |\n",
      "| Epoch   0 |     6/  907 batches | Loss:   7.64 | Tokens:   426 | Learning Rate: 1.9e-06 | Time: 0:01:02.069751 |\n",
      "| Epoch   0 |     7/  907 batches | Loss:   7.64 | Tokens:   480 | Learning Rate: 2.2e-06 | Time: 0:01:10.344749 |\n",
      "| Epoch   0 |     8/  907 batches | Loss:   7.59 | Tokens:   473 | Learning Rate: 2.4e-06 | Time: 0:01:18.167197 |\n",
      "| Epoch   0 |     9/  907 batches | Loss:   7.58 | Tokens:   439 | Learning Rate: 2.7e-06 | Time: 0:01:26.206389 |\n",
      "| Epoch   0 |    10/  907 batches | Loss:   7.57 | Tokens:   430 | Learning Rate: 3.0e-06 | Time: 0:01:34.387588 |\n",
      "| Epoch   0 |    11/  907 batches | Loss:   7.55 | Tokens:   449 | Learning Rate: 3.2e-06 | Time: 0:01:42.626868 |\n",
      "| Epoch   0 |    12/  907 batches | Loss:   7.51 | Tokens:   446 | Learning Rate: 3.5e-06 | Time: 0:01:50.453544 |\n",
      "| Epoch   0 |    13/  907 batches | Loss:   7.49 | Tokens:   459 | Learning Rate: 3.8e-06 | Time: 0:01:59.536443 |\n",
      "| Epoch   0 |    14/  907 batches | Loss:   7.48 | Tokens:   451 | Learning Rate: 4.0e-06 | Time: 0:02:08.070256 |\n",
      "| Epoch   0 |    15/  907 batches | Loss:   7.47 | Tokens:   457 | Learning Rate: 4.3e-06 | Time: 0:02:16.227470 |\n",
      "| Epoch   0 |    16/  907 batches | Loss:   7.42 | Tokens:   488 | Learning Rate: 4.6e-06 | Time: 0:02:24.743376 |\n",
      "| Epoch   0 |    17/  907 batches | Loss:   7.39 | Tokens:   465 | Learning Rate: 4.8e-06 | Time: 0:02:32.787668 |\n",
      "| Epoch   0 |    18/  907 batches | Loss:   7.34 | Tokens:   428 | Learning Rate: 5.1e-06 | Time: 0:02:40.776692 |\n",
      "| Epoch   0 |    19/  907 batches | Loss:   7.33 | Tokens:   426 | Learning Rate: 5.4e-06 | Time: 0:02:48.897703 |\n",
      "| Epoch   0 |    20/  907 batches | Loss:   7.27 | Tokens:   451 | Learning Rate: 5.6e-06 | Time: 0:02:57.244530 |\n",
      "| Epoch   0 |    21/  907 batches | Loss:   7.23 | Tokens:   474 | Learning Rate: 5.9e-06 | Time: 0:03:05.649519 |\n",
      "| Epoch   0 |    22/  907 batches | Loss:   7.24 | Tokens:   454 | Learning Rate: 6.2e-06 | Time: 0:03:13.693782 |\n",
      "| Epoch   0 |    23/  907 batches | Loss:   7.21 | Tokens:   445 | Learning Rate: 6.5e-06 | Time: 0:03:22.447760 |\n",
      "| Epoch   0 |    24/  907 batches | Loss:   7.17 | Tokens:   434 | Learning Rate: 6.7e-06 | Time: 0:03:30.929862 |\n",
      "| Epoch   0 |    25/  907 batches | Loss:   7.09 | Tokens:   462 | Learning Rate: 7.0e-06 | Time: 0:03:39.447816 |\n",
      "| Epoch   0 |    26/  907 batches | Loss:   7.10 | Tokens:   444 | Learning Rate: 7.3e-06 | Time: 0:03:47.523577 |\n",
      "| Epoch   0 |    27/  907 batches | Loss:   7.09 | Tokens:   454 | Learning Rate: 7.5e-06 | Time: 0:03:56.023283 |\n",
      "| Epoch   0 |    28/  907 batches | Loss:   7.01 | Tokens:   443 | Learning Rate: 7.8e-06 | Time: 0:04:05.048739 |\n",
      "| Epoch   0 |    29/  907 batches | Loss:   6.97 | Tokens:   450 | Learning Rate: 8.1e-06 | Time: 0:04:13.748132 |\n",
      "| Epoch   0 |    30/  907 batches | Loss:   6.99 | Tokens:   454 | Learning Rate: 8.3e-06 | Time: 0:04:22.436585 |\n",
      "| Epoch   0 |    31/  907 batches | Loss:   6.94 | Tokens:   449 | Learning Rate: 8.6e-06 | Time: 0:04:31.443076 |\n",
      "| Epoch   0 |    32/  907 batches | Loss:   6.88 | Tokens:   438 | Learning Rate: 8.9e-06 | Time: 0:04:39.610551 |\n",
      "| Epoch   0 |    33/  907 batches | Loss:   6.84 | Tokens:   431 | Learning Rate: 9.1e-06 | Time: 0:04:47.969449 |\n",
      "| Epoch   0 |    34/  907 batches | Loss:   6.84 | Tokens:   439 | Learning Rate: 9.4e-06 | Time: 0:04:56.534519 |\n",
      "| Epoch   0 |    35/  907 batches | Loss:   6.77 | Tokens:   440 | Learning Rate: 9.7e-06 | Time: 0:05:04.898370 |\n",
      "| Epoch   0 |    36/  907 batches | Loss:   6.83 | Tokens:   483 | Learning Rate: 1.0e-05 | Time: 0:05:14.529599 |\n",
      "| Epoch   0 |    37/  907 batches | Loss:   6.78 | Tokens:   442 | Learning Rate: 1.0e-05 | Time: 0:05:23.994632 |\n",
      "| Epoch   0 |    38/  907 batches | Loss:   6.77 | Tokens:   440 | Learning Rate: 1.0e-05 | Time: 0:05:33.644001 |\n",
      "| Epoch   0 |    39/  907 batches | Loss:   6.70 | Tokens:   469 | Learning Rate: 1.1e-05 | Time: 0:05:42.710160 |\n",
      "| Epoch   0 |    40/  907 batches | Loss:   6.75 | Tokens:   451 | Learning Rate: 1.1e-05 | Time: 0:05:51.480598 |\n",
      "| Epoch   0 |    41/  907 batches | Loss:   6.63 | Tokens:   455 | Learning Rate: 1.1e-05 | Time: 0:05:59.441617 |\n",
      "| Epoch   0 |    42/  907 batches | Loss:   6.68 | Tokens:   449 | Learning Rate: 1.2e-05 | Time: 0:06:08.501848 |\n",
      "| Epoch   0 |    43/  907 batches | Loss:   6.63 | Tokens:   442 | Learning Rate: 1.2e-05 | Time: 0:06:16.678821 |\n",
      "| Epoch   0 |    44/  907 batches | Loss:   6.66 | Tokens:   419 | Learning Rate: 1.2e-05 | Time: 0:06:25.063090 |\n",
      "| Epoch   0 |    45/  907 batches | Loss:   6.62 | Tokens:   444 | Learning Rate: 1.2e-05 | Time: 0:06:33.351696 |\n",
      "| Epoch   0 |    46/  907 batches | Loss:   6.63 | Tokens:   441 | Learning Rate: 1.3e-05 | Time: 0:06:41.795597 |\n",
      "| Epoch   0 |    47/  907 batches | Loss:   6.54 | Tokens:   441 | Learning Rate: 1.3e-05 | Time: 0:06:50.304781 |\n",
      "| Epoch   0 |    48/  907 batches | Loss:   6.60 | Tokens:   416 | Learning Rate: 1.3e-05 | Time: 0:06:58.067598 |\n",
      "| Epoch   0 |    49/  907 batches | Loss:   6.54 | Tokens:   434 | Learning Rate: 1.3e-05 | Time: 0:07:06.440421 |\n",
      "| Epoch   0 |    50/  907 batches | Loss:   6.57 | Tokens:   474 | Learning Rate: 1.4e-05 | Time: 0:07:15.086185 |\n",
      "| Epoch   0 |    51/  907 batches | Loss:   6.45 | Tokens:   452 | Learning Rate: 1.4e-05 | Time: 0:07:23.803856 |\n",
      "| Epoch   0 |    52/  907 batches | Loss:   6.45 | Tokens:   421 | Learning Rate: 1.4e-05 | Time: 0:07:33.619104 |\n",
      "| Epoch   0 |    53/  907 batches | Loss:   6.43 | Tokens:   425 | Learning Rate: 1.5e-05 | Time: 0:07:42.246606 |\n",
      "| Epoch   0 |    54/  907 batches | Loss:   6.47 | Tokens:   441 | Learning Rate: 1.5e-05 | Time: 0:07:50.226434 |\n",
      "| Epoch   0 |    55/  907 batches | Loss:   6.42 | Tokens:   435 | Learning Rate: 1.5e-05 | Time: 0:07:58.426636 |\n",
      "| Epoch   0 |    56/  907 batches | Loss:   6.39 | Tokens:   437 | Learning Rate: 1.5e-05 | Time: 0:08:06.414630 |\n",
      "| Epoch   0 |    57/  907 batches | Loss:   6.38 | Tokens:   452 | Learning Rate: 1.6e-05 | Time: 0:08:14.368654 |\n",
      "| Epoch   0 |    58/  907 batches | Loss:   6.42 | Tokens:   461 | Learning Rate: 1.6e-05 | Time: 0:08:22.362604 |\n",
      "| Epoch   0 |    59/  907 batches | Loss:   6.33 | Tokens:   432 | Learning Rate: 1.6e-05 | Time: 0:08:30.868576 |\n",
      "| Epoch   0 |    60/  907 batches | Loss:   6.40 | Tokens:   462 | Learning Rate: 1.6e-05 | Time: 0:08:38.797719 |\n",
      "| Epoch   0 |    61/  907 batches | Loss:   6.38 | Tokens:   439 | Learning Rate: 1.7e-05 | Time: 0:08:47.351634 |\n",
      "| Epoch   0 |    62/  907 batches | Loss:   6.26 | Tokens:   470 | Learning Rate: 1.7e-05 | Time: 0:08:55.594867 |\n",
      "| Epoch   0 |    63/  907 batches | Loss:   6.40 | Tokens:   454 | Learning Rate: 1.7e-05 | Time: 0:09:04.122398 |\n",
      "| Epoch   0 |    64/  907 batches | Loss:   6.24 | Tokens:   444 | Learning Rate: 1.7e-05 | Time: 0:09:12.673705 |\n",
      "| Epoch   0 |    65/  907 batches | Loss:   6.34 | Tokens:   418 | Learning Rate: 1.8e-05 | Time: 0:09:21.671290 |\n",
      "| Epoch   0 |    66/  907 batches | Loss:   6.26 | Tokens:   464 | Learning Rate: 1.8e-05 | Time: 0:09:31.733989 |\n",
      "| Epoch   0 |    67/  907 batches | Loss:   6.24 | Tokens:   462 | Learning Rate: 1.8e-05 | Time: 0:09:41.918874 |\n",
      "| Epoch   0 |    68/  907 batches | Loss:   6.26 | Tokens:   410 | Learning Rate: 1.9e-05 | Time: 0:09:51.542007 |\n",
      "| Epoch   0 |    69/  907 batches | Loss:   6.23 | Tokens:   444 | Learning Rate: 1.9e-05 | Time: 0:10:00.628504 |\n",
      "| Epoch   0 |    70/  907 batches | Loss:   6.31 | Tokens:   463 | Learning Rate: 1.9e-05 | Time: 0:10:08.905785 |\n",
      "| Epoch   0 |    71/  907 batches | Loss:   6.27 | Tokens:   476 | Learning Rate: 1.9e-05 | Time: 0:10:17.514286 |\n",
      "| Epoch   0 |    72/  907 batches | Loss:   6.24 | Tokens:   538 | Learning Rate: 2.0e-05 | Time: 0:10:27.672083 |\n",
      "| Epoch   0 |    73/  907 batches | Loss:   6.25 | Tokens:   498 | Learning Rate: 2.0e-05 | Time: 0:10:42.187362 |\n",
      "| Epoch   0 |    74/  907 batches | Loss:   6.23 | Tokens:   452 | Learning Rate: 2.0e-05 | Time: 0:10:54.564683 |\n",
      "| Epoch   0 |    75/  907 batches | Loss:   6.15 | Tokens:   447 | Learning Rate: 2.0e-05 | Time: 0:11:05.342670 |\n",
      "| Epoch   0 |    76/  907 batches | Loss:   6.12 | Tokens:   428 | Learning Rate: 2.1e-05 | Time: 0:11:15.926296 |\n",
      "| Epoch   0 |    77/  907 batches | Loss:   6.14 | Tokens:   454 | Learning Rate: 2.1e-05 | Time: 0:11:26.109441 |\n",
      "| Epoch   0 |    78/  907 batches | Loss:   6.14 | Tokens:   446 | Learning Rate: 2.1e-05 | Time: 0:11:36.229012 |\n",
      "| Epoch   0 |    79/  907 batches | Loss:   6.09 | Tokens:   457 | Learning Rate: 2.2e-05 | Time: 0:11:46.485476 |\n",
      "| Epoch   0 |    80/  907 batches | Loss:   6.10 | Tokens:   453 | Learning Rate: 2.2e-05 | Time: 0:11:56.866591 |\n",
      "| Epoch   0 |    81/  907 batches | Loss:   6.01 | Tokens:   416 | Learning Rate: 2.2e-05 | Time: 0:12:07.333724 |\n",
      "| Epoch   0 |    82/  907 batches | Loss:   6.00 | Tokens:   454 | Learning Rate: 2.2e-05 | Time: 0:12:17.472408 |\n",
      "| Epoch   0 |    83/  907 batches | Loss:   6.08 | Tokens:   425 | Learning Rate: 2.3e-05 | Time: 0:12:27.614873 |\n",
      "| Epoch   0 |    84/  907 batches | Loss:   6.06 | Tokens:   435 | Learning Rate: 2.3e-05 | Time: 0:12:37.654219 |\n",
      "| Epoch   0 |    85/  907 batches | Loss:   6.02 | Tokens:   435 | Learning Rate: 2.3e-05 | Time: 0:12:48.286405 |\n",
      "| Epoch   0 |    86/  907 batches | Loss:   5.98 | Tokens:   436 | Learning Rate: 2.3e-05 | Time: 0:12:58.719661 |\n",
      "| Epoch   0 |    87/  907 batches | Loss:   5.94 | Tokens:   456 | Learning Rate: 2.4e-05 | Time: 0:13:08.675126 |\n",
      "| Epoch   0 |    88/  907 batches | Loss:   5.93 | Tokens:   409 | Learning Rate: 2.4e-05 | Time: 0:13:18.946302 |\n",
      "| Epoch   0 |    89/  907 batches | Loss:   5.95 | Tokens:   427 | Learning Rate: 2.4e-05 | Time: 0:13:29.069556 |\n",
      "| Epoch   0 |    90/  907 batches | Loss:   5.99 | Tokens:   458 | Learning Rate: 2.4e-05 | Time: 0:13:39.123823 |\n",
      "| Epoch   0 |    91/  907 batches | Loss:   5.90 | Tokens:   440 | Learning Rate: 2.5e-05 | Time: 0:13:49.316124 |\n",
      "| Epoch   0 |    92/  907 batches | Loss:   5.84 | Tokens:   432 | Learning Rate: 2.5e-05 | Time: 0:13:59.514526 |\n",
      "| Epoch   0 |    93/  907 batches | Loss:   5.87 | Tokens:   430 | Learning Rate: 2.5e-05 | Time: 0:14:09.598189 |\n",
      "| Epoch   0 |    94/  907 batches | Loss:   5.91 | Tokens:   454 | Learning Rate: 2.6e-05 | Time: 0:14:20.135710 |\n",
      "| Epoch   0 |    95/  907 batches | Loss:   5.86 | Tokens:   416 | Learning Rate: 2.6e-05 | Time: 0:14:30.288800 |\n",
      "| Epoch   0 |    96/  907 batches | Loss:   5.87 | Tokens:   420 | Learning Rate: 2.6e-05 | Time: 0:14:40.078629 |\n",
      "| Epoch   0 |    97/  907 batches | Loss:   5.87 | Tokens:   431 | Learning Rate: 2.6e-05 | Time: 0:14:50.028234 |\n",
      "| Epoch   0 |    98/  907 batches | Loss:   5.90 | Tokens:   433 | Learning Rate: 2.7e-05 | Time: 0:15:00.427253 |\n",
      "| Epoch   0 |    99/  907 batches | Loss:   5.69 | Tokens:   455 | Learning Rate: 2.7e-05 | Time: 0:15:10.737893 |\n",
      "| Epoch   0 |   100/  907 batches | Loss:   5.85 | Tokens:   448 | Learning Rate: 2.7e-05 | Time: 0:15:21.056568 |\n",
      "| Epoch   0 |   101/  907 batches | Loss:   5.62 | Tokens:   440 | Learning Rate: 2.7e-05 | Time: 0:15:31.425584 |\n",
      "| Epoch   0 |   102/  907 batches | Loss:   5.66 | Tokens:   449 | Learning Rate: 2.8e-05 | Time: 0:15:41.627518 |\n",
      "| Epoch   0 |   103/  907 batches | Loss:   5.65 | Tokens:   456 | Learning Rate: 2.8e-05 | Time: 0:15:52.702579 |\n",
      "| Epoch   0 |   104/  907 batches | Loss:   5.67 | Tokens:   452 | Learning Rate: 2.8e-05 | Time: 0:16:02.777832 |\n",
      "| Epoch   0 |   105/  907 batches | Loss:   5.66 | Tokens:   461 | Learning Rate: 2.9e-05 | Time: 0:16:12.521448 |\n",
      "| Epoch   0 |   106/  907 batches | Loss:   5.73 | Tokens:   468 | Learning Rate: 2.9e-05 | Time: 0:16:25.816506 |\n",
      "| Epoch   0 |   107/  907 batches | Loss:   5.69 | Tokens:   458 | Learning Rate: 2.9e-05 | Time: 0:16:37.789114 |\n",
      "| Epoch   0 |   108/  907 batches | Loss:   5.71 | Tokens:   472 | Learning Rate: 2.9e-05 | Time: 0:16:47.930797 |\n",
      "| Epoch   0 |   109/  907 batches | Loss:   5.59 | Tokens:   424 | Learning Rate: 3.0e-05 | Time: 0:16:59.019060 |\n",
      "| Epoch   0 |   110/  907 batches | Loss:   5.63 | Tokens:   466 | Learning Rate: 3.0e-05 | Time: 0:17:10.168391 |\n",
      "| Epoch   0 |   111/  907 batches | Loss:   5.49 | Tokens:   428 | Learning Rate: 3.0e-05 | Time: 0:17:20.992273 |\n",
      "| Epoch   0 |   112/  907 batches | Loss:   5.37 | Tokens:   417 | Learning Rate: 3.0e-05 | Time: 0:17:32.082623 |\n",
      "| Epoch   0 |   113/  907 batches | Loss:   5.38 | Tokens:   417 | Learning Rate: 3.1e-05 | Time: 0:17:45.071187 |\n",
      "| Epoch   0 |   114/  907 batches | Loss:   5.50 | Tokens:   444 | Learning Rate: 3.1e-05 | Time: 0:17:56.152573 |\n",
      "| Epoch   0 |   115/  907 batches | Loss:   5.49 | Tokens:   449 | Learning Rate: 3.1e-05 | Time: 0:18:06.022788 |\n",
      "| Epoch   0 |   116/  907 batches | Loss:   5.51 | Tokens:   461 | Learning Rate: 3.1e-05 | Time: 0:18:15.962781 |\n",
      "| Epoch   0 |   117/  907 batches | Loss:   5.38 | Tokens:   458 | Learning Rate: 3.2e-05 | Time: 0:18:25.681811 |\n",
      "| Epoch   0 |   118/  907 batches | Loss:   5.38 | Tokens:   413 | Learning Rate: 3.2e-05 | Time: 0:18:35.415460 |\n",
      "| Epoch   0 |   119/  907 batches | Loss:   5.28 | Tokens:   441 | Learning Rate: 3.2e-05 | Time: 0:18:45.076598 |\n",
      "| Epoch   0 |   120/  907 batches | Loss:   5.38 | Tokens:   458 | Learning Rate: 3.3e-05 | Time: 0:18:55.064911 |\n",
      "| Epoch   0 |   121/  907 batches | Loss:   5.41 | Tokens:   473 | Learning Rate: 3.3e-05 | Time: 0:19:04.833846 |\n",
      "| Epoch   0 |   122/  907 batches | Loss:   5.31 | Tokens:   438 | Learning Rate: 3.3e-05 | Time: 0:19:14.480251 |\n",
      "| Epoch   0 |   123/  907 batches | Loss:   5.40 | Tokens:   434 | Learning Rate: 3.3e-05 | Time: 0:19:24.248301 |\n",
      "| Epoch   0 |   124/  907 batches | Loss:   5.25 | Tokens:   428 | Learning Rate: 3.4e-05 | Time: 0:19:34.024176 |\n",
      "| Epoch   0 |   125/  907 batches | Loss:   5.38 | Tokens:   437 | Learning Rate: 3.4e-05 | Time: 0:19:43.677627 |\n",
      "| Epoch   0 |   126/  907 batches | Loss:   5.22 | Tokens:   406 | Learning Rate: 3.4e-05 | Time: 0:19:53.325001 |\n",
      "| Epoch   0 |   127/  907 batches | Loss:   5.14 | Tokens:   458 | Learning Rate: 3.4e-05 | Time: 0:20:03.152993 |\n",
      "| Epoch   0 |   128/  907 batches | Loss:   5.17 | Tokens:   434 | Learning Rate: 3.5e-05 | Time: 0:20:13.400704 |\n",
      "| Epoch   0 |   129/  907 batches | Loss:   5.26 | Tokens:   419 | Learning Rate: 3.5e-05 | Time: 0:20:24.825715 |\n",
      "| Epoch   0 |   130/  907 batches | Loss:   5.24 | Tokens:   454 | Learning Rate: 3.5e-05 | Time: 0:20:36.833271 |\n",
      "| Epoch   0 |   131/  907 batches | Loss:   5.08 | Tokens:   415 | Learning Rate: 3.6e-05 | Time: 0:20:53.075845 |\n",
      "| Epoch   0 |   132/  907 batches | Loss:   4.99 | Tokens:   413 | Learning Rate: 3.6e-05 | Time: 0:21:03.941098 |\n",
      "| Epoch   0 |   133/  907 batches | Loss:   5.09 | Tokens:   419 | Learning Rate: 3.6e-05 | Time: 0:21:12.551079 |\n",
      "| Epoch   0 |   134/  907 batches | Loss:   5.21 | Tokens:   412 | Learning Rate: 3.6e-05 | Time: 0:21:21.070304 |\n",
      "| Epoch   0 |   135/  907 batches | Loss:   5.09 | Tokens:   426 | Learning Rate: 3.7e-05 | Time: 0:21:29.082880 |\n",
      "| Epoch   0 |   136/  907 batches | Loss:   5.07 | Tokens:   440 | Learning Rate: 3.7e-05 | Time: 0:21:36.959821 |\n",
      "| Epoch   0 |   137/  907 batches | Loss:   4.94 | Tokens:   436 | Learning Rate: 3.7e-05 | Time: 0:21:45.185830 |\n",
      "| Epoch   0 |   138/  907 batches | Loss:   5.14 | Tokens:   450 | Learning Rate: 3.7e-05 | Time: 0:21:52.659846 |\n",
      "| Epoch   0 |   139/  907 batches | Loss:   4.90 | Tokens:   445 | Learning Rate: 3.8e-05 | Time: 0:22:00.203677 |\n",
      "| Epoch   0 |   140/  907 batches | Loss:   4.89 | Tokens:   429 | Learning Rate: 3.8e-05 | Time: 0:22:08.006815 |\n",
      "| Epoch   0 |   141/  907 batches | Loss:   5.03 | Tokens:   436 | Learning Rate: 3.8e-05 | Time: 0:22:15.629435 |\n",
      "| Epoch   0 |   142/  907 batches | Loss:   4.91 | Tokens:   437 | Learning Rate: 3.8e-05 | Time: 0:22:23.260034 |\n",
      "| Epoch   0 |   143/  907 batches | Loss:   5.00 | Tokens:   435 | Learning Rate: 3.9e-05 | Time: 0:22:30.811845 |\n",
      "| Epoch   0 |   144/  907 batches | Loss:   4.93 | Tokens:   425 | Learning Rate: 3.9e-05 | Time: 0:22:38.399558 |\n",
      "| Epoch   0 |   145/  907 batches | Loss:   4.87 | Tokens:   436 | Learning Rate: 3.9e-05 | Time: 0:22:46.425149 |\n",
      "| Epoch   0 |   146/  907 batches | Loss:   4.78 | Tokens:   404 | Learning Rate: 4.0e-05 | Time: 0:22:54.304083 |\n",
      "| Epoch   0 |   147/  907 batches | Loss:   4.81 | Tokens:   428 | Learning Rate: 4.0e-05 | Time: 0:23:02.000063 |\n",
      "| Epoch   0 |   148/  907 batches | Loss:   4.89 | Tokens:   438 | Learning Rate: 4.0e-05 | Time: 0:23:09.630663 |\n",
      "| Epoch   0 |   149/  907 batches | Loss:   4.87 | Tokens:   430 | Learning Rate: 4.0e-05 | Time: 0:23:17.148563 |\n",
      "| Epoch   0 |   150/  907 batches | Loss:   4.71 | Tokens:   434 | Learning Rate: 4.1e-05 | Time: 0:23:24.612607 |\n",
      "| Epoch   0 |   151/  907 batches | Loss:   4.99 | Tokens:   446 | Learning Rate: 4.1e-05 | Time: 0:23:31.995868 |\n",
      "| Epoch   0 |   152/  907 batches | Loss:   4.86 | Tokens:   478 | Learning Rate: 4.1e-05 | Time: 0:23:39.600536 |\n",
      "| Epoch   0 |   153/  907 batches | Loss:   4.76 | Tokens:   431 | Learning Rate: 4.1e-05 | Time: 0:23:47.243103 |\n",
      "| Epoch   0 |   154/  907 batches | Loss:   4.68 | Tokens:   457 | Learning Rate: 4.2e-05 | Time: 0:23:54.730086 |\n",
      "| Epoch   0 |   155/  907 batches | Loss:   4.78 | Tokens:   476 | Learning Rate: 4.2e-05 | Time: 0:24:02.269928 |\n",
      "| Epoch   0 |   156/  907 batches | Loss:   4.71 | Tokens:   444 | Learning Rate: 4.2e-05 | Time: 0:24:09.805782 |\n",
      "| Epoch   0 |   157/  907 batches | Loss:   4.77 | Tokens:   436 | Learning Rate: 4.2e-05 | Time: 0:24:17.349612 |\n",
      "| Epoch   0 |   158/  907 batches | Loss:   4.90 | Tokens:   475 | Learning Rate: 4.3e-05 | Time: 0:24:24.949293 |\n",
      "| Epoch   0 |   159/  907 batches | Loss:   4.94 | Tokens:   429 | Learning Rate: 4.3e-05 | Time: 0:24:32.404362 |\n",
      "| Epoch   0 |   160/  907 batches | Loss:   5.16 | Tokens:   437 | Learning Rate: 4.3e-05 | Time: 0:24:39.870401 |\n",
      "| Epoch   0 |   161/  907 batches | Loss:   4.87 | Tokens:   425 | Learning Rate: 4.4e-05 | Time: 0:24:47.772275 |\n",
      "| Epoch   0 |   162/  907 batches | Loss:   4.82 | Tokens:   448 | Learning Rate: 4.4e-05 | Time: 0:24:56.659514 |\n",
      "| Epoch   0 |   163/  907 batches | Loss:   4.73 | Tokens:   437 | Learning Rate: 4.4e-05 | Time: 0:25:05.078007 |\n",
      "| Epoch   0 |   164/  907 batches | Loss:   4.71 | Tokens:   477 | Learning Rate: 4.4e-05 | Time: 0:25:12.857209 |\n",
      "| Epoch   0 |   165/  907 batches | Loss:   4.72 | Tokens:   455 | Learning Rate: 4.5e-05 | Time: 0:25:20.791996 |\n",
      "| Epoch   0 |   166/  907 batches | Loss:   4.63 | Tokens:   412 | Learning Rate: 4.5e-05 | Time: 0:25:28.498391 |\n",
      "| Epoch   0 |   167/  907 batches | Loss:   4.67 | Tokens:   462 | Learning Rate: 4.5e-05 | Time: 0:25:35.972410 |\n",
      "| Epoch   0 |   168/  907 batches | Loss:   4.60 | Tokens:   429 | Learning Rate: 4.5e-05 | Time: 0:25:43.864310 |\n",
      "| Epoch   0 |   169/  907 batches | Loss:   4.72 | Tokens:   439 | Learning Rate: 4.6e-05 | Time: 0:25:52.849288 |\n",
      "| Epoch   0 |   170/  907 batches | Loss:   4.77 | Tokens:   495 | Learning Rate: 4.6e-05 | Time: 0:26:01.301690 |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3808\\2795856446.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     train_loss = train_epoch(epoch,\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3808\\1348658060.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(epoch, data_loader, model, loss_compute, optimizer, scheduler, padding_idx)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mntokens\u001b[0m \u001b[1;31m# mean loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 优化内存使用\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\miniconda3\\envs\\transformer\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\miniconda3\\envs\\transformer\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spacy_de, spacy_en = load_tokenizers()\n",
    "vocab_src, vocab_tgt = load_vocab(spacy_de, spacy_en)\n",
    "\n",
    "src_vocab_size = len(vocab_src)\n",
    "tgt_vocab_size = len(vocab_tgt)\n",
    "pad_idx = vocab_src['<blank>']\n",
    "d_model = 512\n",
    "init_lr = 1.0\n",
    "warmup = 3000\n",
    "num_epochs = 8\n",
    "batch_size = 32\n",
    "max_padding = 72\n",
    "\n",
    "train_dataloader, valid_dataloader = create_dataloaders(vocab_src, vocab_tgt, spacy_de, spacy_en, batch_size, max_padding)\n",
    "\n",
    "\n",
    "model = make_model(src_vocab_size, tgt_vocab_size, d_model)\n",
    "\n",
    "criterion = LabelSmoothingKL(vocab_size=tgt_vocab_size, padding_idx=pad_idx, smoothing=0.1)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=init_lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                           lr_lambda=lambda step: scheduler(step, d_model, factor=1, warmup=warmup))\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    train_loss = train_epoch(epoch,\n",
    "        train_dataloader,\n",
    "        model,\n",
    "        ComputeLoss(criterion),\n",
    "        optimizer,\n",
    "        lr_scheduler,\n",
    "        pad_idx\n",
    "    )\n",
    "    train_losses.append(train_loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = eval_epoch(epoch,\n",
    "        valid_dataloader,\n",
    "        model,\n",
    "        ComputeLoss(criterion),\n",
    "        pad_idx\n",
    "    )\n",
    "\n",
    "    val_losses.append(val_loss.item())\n",
    "    print('-' * 59)\n",
    "    print('| End of epoch {:3d} | Train Loss: {:8.3f} | Val Loss: {:8.3f} | time: {} |'\n",
    "          .format(epoch, train_loss, val_loss, timedelta(seconds=time.time()-start)))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='train_loss')\n",
    "plt.plot(val_losses, label='val_loss')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_py3.8",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "314.825px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
