{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T07:52:06.903790Z",
     "start_time": "2023-02-23T07:52:02.854054Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import spacy # 一个自然语言文本处理库\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "from torchtext.data.functional import to_map_style_dataset  # 将迭代器转化为 Dataset 类型，可直接索引\n",
    "\n",
    "\n",
    "from transformer_package import make_model, scheduler, LabelSmoothingKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T07:52:36.031152Z",
     "start_time": "2023-02-23T07:52:36.018154Z"
    }
   },
   "outputs": [],
   "source": [
    "class ComputeLoss:\n",
    "    def __init__(self, criterion):\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        x = F.log_softmax(x, dim=-1)  # if KL divergence is used\n",
    "        loss = (\n",
    "            self.criterion(    # x: [b, len, vocab_size] ---> [b*len, vocab_size];  y: [b, len] ---> [b*len, ]\n",
    "                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n",
    "            )\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T07:52:36.062064Z",
     "start_time": "2023-02-23T07:52:36.035108Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(epoch, data_loader, model, loss_compute, optimizer, scheduler, padding_idx):\n",
    "    \"\"\"Train a single epoch.\"\"\"\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    model.train()\n",
    "    for i, (src, tgt) in enumerate(data_loader):\n",
    "        # src, tgt shape: [batch_size, max_len]\n",
    "        tgt_y = copy.deepcopy(tgt[:, 1:])  # 真实的序列，用来构建loss，第一位往往是起始符\n",
    "        tgt_seq = copy.deepcopy(tgt[:, :-1])   # 输入decoder的序列，最后一位的token是用不到的。因为按照decoder的工作原理，最后一个token的生成是不会依赖到最后一个token的信息的。\n",
    "        ntokens = (tgt_y != padding_idx).data.sum()\n",
    "        \n",
    "        # get padding mask and sequence mask\n",
    "        src_mask = model.padding_mask(src, padding_idx)\n",
    "        tgt_mask = model.padding_mask(tgt_seq, padding_idx) & model.sequence_mask(tgt_seq.size(-1))\n",
    "        \n",
    "        # train\n",
    "        logit = model(src, tgt_seq, src_mask, tgt_mask)\n",
    "        loss = loss_compute(logit, tgt_y)\n",
    "        \n",
    "        \n",
    "        total_loss += loss\n",
    "        total_tokens += ntokens\n",
    "        \n",
    "        loss /= ntokens # mean loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True) # 优化内存使用\n",
    "        scheduler.step()\n",
    "\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        elapsed = time.time() - start\n",
    "        print(\n",
    "            (\n",
    "                \"| Epoch {:3d} | {:5d}/{:5d} batches | Loss: {:6.2f} \"\n",
    "                + \"| Tokens: {:5d} | Learning Rate: {:6.1e} | Time: {} |\"\n",
    "            ).format(epoch, i, len(data_loader), loss, ntokens, lr, timedelta(seconds=elapsed))\n",
    "        )\n",
    "        \n",
    "        del loss\n",
    "\n",
    "    return total_loss / total_tokens\n",
    "\n",
    "\n",
    "def eval_epoch(epoch, data_loader, model, loss_compute, padding_idx):\n",
    "    \"\"\"Eval a single epoch.\"\"\"\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (src, tgt) in enumerate(data_loader):\n",
    "            # src, tgt shape: [batch_size, max_len]\n",
    "            tgt_y = copy.deepcopy(tgt[:, 1:])  # 真实的序列，用来构建loss，第一位往往是起始符\n",
    "            tgt_seq = copy.deepcopy(tgt[:, :-1])   # 输入decoder的序列，最后一位的token是用不到的。因为按照decoder的工作原理，最后一个token的生成是不会依赖到最后一个token的信息的。\n",
    "            ntokens = (tgt_y != padding_idx).data.sum()\n",
    "            \n",
    "            # get padding mask and sequence mask\n",
    "            src_mask = model.padding_mask(src, padding_idx)\n",
    "            tgt_mask = model.padding_mask(tgt_seq, padding_idx) & model.sequence_mask(tgt_seq.size(-1))\n",
    "            \n",
    "            # train\n",
    "            logit = model(src, tgt_seq, src_mask, tgt_mask)\n",
    "            loss = loss_compute(logit, tgt_y)\n",
    "        \n",
    "\n",
    "            total_loss += loss\n",
    "            total_tokens += ntokens\n",
    "            \n",
    "            \n",
    "            loss /= ntokens # mean loss\n",
    "    \n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                (\n",
    "                    \"| Epoch {:3d} | {:5d}/{:5d} batches | Loss: {:6.2f} \"\n",
    "                    + \"| Tokens: {:5d} | Time: {} |\"\n",
    "                ).format(epoch, i, len(data_loader), loss, ntokens, timedelta(seconds=elapsed))\n",
    "            )\n",
    "            \n",
    "            del loss\n",
    "            \n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi30k German-English Translation task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T07:52:38.158345Z",
     "start_time": "2023-02-23T07:52:38.150334Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_tokenizers():\n",
    "\n",
    "    try:\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download de_core_news_sm\")  # 大概率要翻墙才行\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    try:\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    return spacy_de, spacy_en\n",
    "\n",
    "# spacy_de, spacy_en = load_tokenizers()\n",
    "# doc = spacy_en.tokenizer(\"This is a sentence.\")\n",
    "# print([(w.text, w.pos_) for w in doc])\n",
    "\n",
    "\n",
    "def tokenize(text, tokenizer):\n",
    "    return [tok.text for tok in tokenizer.tokenizer(text)]\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter, tokenizer, index):\n",
    "    for from_to_tuple in data_iter:\n",
    "        yield tokenizer(from_to_tuple[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T07:52:39.583589Z",
     "start_time": "2023-02-23T07:52:39.572618Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(spacy_de, spacy_en):\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(text, spacy_de)\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return tokenize(text, spacy_en)\n",
    "\n",
    "    print(\"Building German Vocabulary ...\")\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    vocab_src = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val + test, tokenize_de, index=0),  # 'de'\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],   # 分别代表起始符、终止符、padding字符、未知字符\n",
    "    )\n",
    "\n",
    "    print(\"Building English Vocabulary ...\")\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    vocab_tgt = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val + test, tokenize_en, index=1),  # 'en'\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    vocab_src.set_default_index(vocab_src[\"<unk>\"])  # This index will be returned when OOV token is queried.\n",
    "    vocab_tgt.set_default_index(vocab_tgt[\"<unk>\"])\n",
    "\n",
    "    return vocab_src, vocab_tgt\n",
    "\n",
    "\n",
    "\n",
    "def load_vocab(spacy_de, spacy_en):\n",
    "    if not os.path.exists(\"vocab.pt\"):\n",
    "        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)\n",
    "        torch.save((vocab_src, vocab_tgt), \"vocab.pt\")\n",
    "    else:\n",
    "        vocab_src, vocab_tgt = torch.load(\"vocab.pt\")\n",
    "    print(\"Finished.\\nVocabulary sizes:\")\n",
    "    print(len(vocab_src))\n",
    "    print(len(vocab_tgt))\n",
    "    return vocab_src, vocab_tgt\n",
    "\n",
    "\n",
    "\n",
    "# vocab_src, vocab_tgt = load_vocab(spacy_de, spacy_en)\n",
    "# print(vocab_src.get_stoi())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T07:52:41.112795Z",
     "start_time": "2023-02-23T07:52:41.088857Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch, src_pipline, tgt_pipline, src_vocab, tgt_vocab, max_padding=128, pad_id=2):  # <blank> token id\n",
    "    '''\n",
    "    负责在 DataLoad 提取一个 batch 的样本时，完成一系列预处理工作。\n",
    "    所以，我们将 collate_batch 函数通过参数 collate_fn 传入 DataLoader，\n",
    "    即可实现对变长数据的处理。\n",
    "    '''\n",
    "    bs_id = torch.tensor([0])  # <s> token id\n",
    "    eos_id = torch.tensor([1])  # </s> token id\n",
    "    src_list, tgt_list = [], []\n",
    "    \n",
    "    for (src, tgt) in batch:\n",
    "        # 为每一句话添加起始符和结束符\n",
    "        processed_src = torch.cat([bs_id, \n",
    "                                  torch.as_tensor(src_vocab(src_pipline(src)), dtype=torch.int64),\n",
    "                                  eos_id],\n",
    "                                  dim=0)\n",
    "        processed_tgt = torch.cat([bs_id, \n",
    "                                  torch.as_tensor(tgt_vocab(tgt_pipline(tgt)), dtype=torch.int64),\n",
    "                                  eos_id],\n",
    "                                  dim=0)\n",
    "        \n",
    "        # 给长度不足max_padding的句子打padding\n",
    "        processed_src = F.pad(processed_src, (0, max_padding - len(processed_src)), value=pad_id)  # # warning - overwrites values for negative values of padding - len\n",
    "        src_list.append(processed_src)\n",
    "        processed_tgt = F.pad(processed_tgt, (0, max_padding - len(processed_tgt)), value=pad_id)\n",
    "        tgt_list.append(processed_tgt)\n",
    "        \n",
    "       \n",
    "    src_batch = torch.stack(src_list)\n",
    "    tgt_batch = torch.stack(tgt_list)\n",
    "    \n",
    "    return (src_batch, tgt_batch)\n",
    "\n",
    "\n",
    "\n",
    "def create_dataloaders(vocab_src, vocab_tgt, spacy_de, spacy_en, batch_size=512, max_padding=128):\n",
    "    \n",
    "    def tokenize_de(text):    # src_pipline\n",
    "        return tokenize(text, spacy_de)\n",
    "\n",
    "    def tokenize_en(text):   # tgt_pipline\n",
    "        return tokenize(text, spacy_en)\n",
    "        \n",
    "    def collate_fn(batch):\n",
    "        return collate_batch(batch, \n",
    "                             tokenize_de, \n",
    "                             tokenize_en, \n",
    "                             vocab_src, \n",
    "                             vocab_tgt,\n",
    "                             max_padding=max_padding,\n",
    "                             pad_id=vocab_src.get_stoi()['<blank>'])\n",
    "    \n",
    "    train_iter, valid_iter, test_iter = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    \n",
    "    train_iter_map = to_map_style_dataset(train_iter)\n",
    "    valid_iter_map = to_map_style_dataset(valid_iter)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_iter_map,\n",
    "                                  batch_size,\n",
    "                                  collate_fn=collate_fn)\n",
    "\n",
    "    valid_dataloader = DataLoader(valid_iter_map,\n",
    "                                  batch_size,\n",
    "                                  collate_fn=collate_fn)\n",
    "\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "\n",
    "# train_dataloader, valid_dataloader = create_dataloaders(vocab_src, vocab_tgt, spacy_de, spacy_en, batch_size=512, max_padding=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-02-23T07:53:04.769Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "Vocabulary sizes:\n",
      "8185\n",
      "6291\n",
      "| Epoch   0 |     0/  227 batches | Loss:   7.61 | Tokens:  1792 | Learning Rate: 2.7e-07 | Time: 0:01:03.702847 |\n",
      "| Epoch   0 |     1/  227 batches | Loss:   7.61 | Tokens:  1795 | Learning Rate: 5.4e-07 | Time: 0:01:49.522915 |\n",
      "| Epoch   0 |     2/  227 batches | Loss:   7.60 | Tokens:  1791 | Learning Rate: 8.1e-07 | Time: 0:02:35.399620 |\n",
      "| Epoch   0 |     3/  227 batches | Loss:   7.59 | Tokens:  1813 | Learning Rate: 1.1e-06 | Time: 0:03:24.495143 |\n",
      "| Epoch   0 |     4/  227 batches | Loss:   7.58 | Tokens:  1807 | Learning Rate: 1.3e-06 | Time: 0:04:13.745857 |\n",
      "| Epoch   0 |     5/  227 batches | Loss:   7.57 | Tokens:  1824 | Learning Rate: 1.6e-06 | Time: 0:05:10.148724 |\n",
      "| Epoch   0 |     6/  227 batches | Loss:   7.55 | Tokens:  1794 | Learning Rate: 1.9e-06 | Time: 0:06:00.251952 |\n",
      "| Epoch   0 |     7/  227 batches | Loss:   7.52 | Tokens:  1796 | Learning Rate: 2.2e-06 | Time: 0:06:50.500950 |\n",
      "| Epoch   0 |     8/  227 batches | Loss:   7.51 | Tokens:  1748 | Learning Rate: 2.4e-06 | Time: 0:07:37.265487 |\n",
      "| Epoch   0 |     9/  227 batches | Loss:   7.49 | Tokens:  1834 | Learning Rate: 2.7e-06 | Time: 0:08:27.068622 |\n",
      "| Epoch   0 |    10/  227 batches | Loss:   7.47 | Tokens:  1797 | Learning Rate: 3.0e-06 | Time: 0:09:17.248589 |\n",
      "| Epoch   0 |    11/  227 batches | Loss:   7.43 | Tokens:  1745 | Learning Rate: 3.2e-06 | Time: 0:10:04.120960 |\n",
      "| Epoch   0 |    12/  227 batches | Loss:   7.39 | Tokens:  1776 | Learning Rate: 3.5e-06 | Time: 0:10:50.908056 |\n",
      "| Epoch   0 |    13/  227 batches | Loss:   7.35 | Tokens:  1722 | Learning Rate: 3.8e-06 | Time: 0:11:38.849498 |\n",
      "| Epoch   0 |    14/  227 batches | Loss:   7.32 | Tokens:  1782 | Learning Rate: 4.0e-06 | Time: 0:12:23.167915 |\n",
      "| Epoch   0 |    15/  227 batches | Loss:   7.29 | Tokens:  1825 | Learning Rate: 4.3e-06 | Time: 0:13:11.127578 |\n",
      "| Epoch   0 |    16/  227 batches | Loss:   7.24 | Tokens:  1788 | Learning Rate: 4.6e-06 | Time: 0:14:03.251421 |\n",
      "| Epoch   0 |    17/  227 batches | Loss:   7.23 | Tokens:  1793 | Learning Rate: 4.8e-06 | Time: 0:14:56.170200 |\n",
      "| Epoch   0 |    18/  227 batches | Loss:   7.21 | Tokens:  1935 | Learning Rate: 5.1e-06 | Time: 0:15:42.306257 |\n",
      "| Epoch   0 |    19/  227 batches | Loss:   7.16 | Tokens:  1785 | Learning Rate: 5.4e-06 | Time: 0:16:33.253467 |\n",
      "| Epoch   0 |    20/  227 batches | Loss:   7.10 | Tokens:  1748 | Learning Rate: 5.6e-06 | Time: 0:17:23.492388 |\n",
      "| Epoch   0 |    21/  227 batches | Loss:   7.09 | Tokens:  1762 | Learning Rate: 5.9e-06 | Time: 0:18:13.785911 |\n",
      "| Epoch   0 |    22/  227 batches | Loss:   7.07 | Tokens:  1734 | Learning Rate: 6.2e-06 | Time: 0:19:03.637094 |\n",
      "| Epoch   0 |    23/  227 batches | Loss:   7.04 | Tokens:  1732 | Learning Rate: 6.5e-06 | Time: 0:19:56.134580 |\n",
      "| Epoch   0 |    24/  227 batches | Loss:   7.01 | Tokens:  1739 | Learning Rate: 6.7e-06 | Time: 0:20:50.502633 |\n",
      "| Epoch   0 |    25/  227 batches | Loss:   6.96 | Tokens:  1793 | Learning Rate: 7.0e-06 | Time: 0:21:38.253710 |\n",
      "| Epoch   0 |    26/  227 batches | Loss:   6.94 | Tokens:  1839 | Learning Rate: 7.3e-06 | Time: 0:22:29.514845 |\n",
      "| Epoch   0 |    27/  227 batches | Loss:   6.94 | Tokens:  1790 | Learning Rate: 7.5e-06 | Time: 0:23:18.786474 |\n",
      "| Epoch   0 |    28/  227 batches | Loss:   6.90 | Tokens:  1727 | Learning Rate: 7.8e-06 | Time: 0:24:07.865430 |\n",
      "| Epoch   0 |    29/  227 batches | Loss:   6.88 | Tokens:  1773 | Learning Rate: 8.1e-06 | Time: 0:24:56.258765 |\n",
      "| Epoch   0 |    30/  227 batches | Loss:   6.86 | Tokens:  1803 | Learning Rate: 8.3e-06 | Time: 0:25:43.627831 |\n",
      "| Epoch   0 |    31/  227 batches | Loss:   6.84 | Tokens:  1729 | Learning Rate: 8.6e-06 | Time: 0:26:31.294459 |\n",
      "| Epoch   0 |    32/  227 batches | Loss:   6.80 | Tokens:  1722 | Learning Rate: 8.9e-06 | Time: 0:27:19.198628 |\n",
      "| Epoch   0 |    33/  227 batches | Loss:   6.78 | Tokens:  1670 | Learning Rate: 9.1e-06 | Time: 0:28:05.739702 |\n",
      "| Epoch   0 |    34/  227 batches | Loss:   6.76 | Tokens:  1771 | Learning Rate: 9.4e-06 | Time: 0:28:51.946079 |\n",
      "| Epoch   0 |    35/  227 batches | Loss:   6.72 | Tokens:  1737 | Learning Rate: 9.7e-06 | Time: 0:29:36.483989 |\n",
      "| Epoch   0 |    36/  227 batches | Loss:   6.69 | Tokens:  1693 | Learning Rate: 1.0e-05 | Time: 0:30:21.865927 |\n",
      "| Epoch   0 |    37/  227 batches | Loss:   6.71 | Tokens:  1748 | Learning Rate: 1.0e-05 | Time: 0:31:10.590946 |\n",
      "| Epoch   0 |    38/  227 batches | Loss:   6.65 | Tokens:  1842 | Learning Rate: 1.0e-05 | Time: 0:31:54.471464 |\n",
      "| Epoch   0 |    39/  227 batches | Loss:   6.69 | Tokens:  1784 | Learning Rate: 1.1e-05 | Time: 0:32:36.998073 |\n",
      "| Epoch   0 |    40/  227 batches | Loss:   6.70 | Tokens:  1747 | Learning Rate: 1.1e-05 | Time: 0:33:22.317811 |\n",
      "| Epoch   0 |    41/  227 batches | Loss:   6.64 | Tokens:  1806 | Learning Rate: 1.1e-05 | Time: 0:34:09.553079 |\n",
      "| Epoch   0 |    42/  227 batches | Loss:   6.61 | Tokens:  1811 | Learning Rate: 1.2e-05 | Time: 0:34:52.929099 |\n",
      "| Epoch   0 |    43/  227 batches | Loss:   6.59 | Tokens:  1823 | Learning Rate: 1.2e-05 | Time: 0:35:38.108536 |\n",
      "| Epoch   0 |    44/  227 batches | Loss:   6.56 | Tokens:  1729 | Learning Rate: 1.2e-05 | Time: 0:36:22.899040 |\n",
      "| Epoch   0 |    45/  227 batches | Loss:   6.53 | Tokens:  1668 | Learning Rate: 1.2e-05 | Time: 0:37:09.970058 |\n",
      "| Epoch   0 |    46/  227 batches | Loss:   6.54 | Tokens:  1776 | Learning Rate: 1.3e-05 | Time: 0:37:57.515410 |\n",
      "| Epoch   0 |    47/  227 batches | Loss:   6.57 | Tokens:  1692 | Learning Rate: 1.3e-05 | Time: 0:38:45.039755 |\n",
      "| Epoch   0 |    48/  227 batches | Loss:   6.46 | Tokens:  1746 | Learning Rate: 1.3e-05 | Time: 0:39:29.979532 |\n",
      "| Epoch   0 |    49/  227 batches | Loss:   6.49 | Tokens:  1684 | Learning Rate: 1.3e-05 | Time: 0:40:13.150393 |\n",
      "| Epoch   0 |    50/  227 batches | Loss:   6.49 | Tokens:  1793 | Learning Rate: 1.4e-05 | Time: 0:40:58.535593 |\n",
      "| Epoch   0 |    51/  227 batches | Loss:   6.46 | Tokens:  1800 | Learning Rate: 1.4e-05 | Time: 0:41:45.055059 |\n",
      "| Epoch   0 |    52/  227 batches | Loss:   6.46 | Tokens:  1794 | Learning Rate: 1.4e-05 | Time: 0:42:32.166754 |\n",
      "| Epoch   0 |    53/  227 batches | Loss:   6.45 | Tokens:  1767 | Learning Rate: 1.5e-05 | Time: 0:43:20.006079 |\n",
      "| Epoch   0 |    54/  227 batches | Loss:   6.42 | Tokens:  1792 | Learning Rate: 1.5e-05 | Time: 0:44:05.664043 |\n",
      "| Epoch   0 |    55/  227 batches | Loss:   6.41 | Tokens:  1734 | Learning Rate: 1.5e-05 | Time: 0:44:54.835288 |\n",
      "| Epoch   0 |    56/  227 batches | Loss:   6.44 | Tokens:  1780 | Learning Rate: 1.5e-05 | Time: 0:45:42.715132 |\n",
      "| Epoch   0 |    57/  227 batches | Loss:   6.44 | Tokens:  1833 | Learning Rate: 1.6e-05 | Time: 0:46:25.869162 |\n",
      "| Epoch   0 |    58/  227 batches | Loss:   6.41 | Tokens:  1793 | Learning Rate: 1.6e-05 | Time: 0:47:10.833073 |\n",
      "| Epoch   0 |    59/  227 batches | Loss:   6.37 | Tokens:  1720 | Learning Rate: 1.6e-05 | Time: 0:47:55.201675 |\n",
      "| Epoch   0 |    60/  227 batches | Loss:   6.37 | Tokens:  1798 | Learning Rate: 1.6e-05 | Time: 0:48:43.579288 |\n",
      "| Epoch   0 |    61/  227 batches | Loss:   6.35 | Tokens:  1709 | Learning Rate: 1.7e-05 | Time: 0:49:28.851244 |\n",
      "| Epoch   0 |    62/  227 batches | Loss:   6.40 | Tokens:  1852 | Learning Rate: 1.7e-05 | Time: 0:50:16.435799 |\n",
      "| Epoch   0 |    63/  227 batches | Loss:   6.37 | Tokens:  1783 | Learning Rate: 1.7e-05 | Time: 0:51:04.852617 |\n",
      "| Epoch   0 |    64/  227 batches | Loss:   6.36 | Tokens:  1821 | Learning Rate: 1.7e-05 | Time: 0:51:48.894825 |\n",
      "| Epoch   0 |    65/  227 batches | Loss:   6.29 | Tokens:  1746 | Learning Rate: 1.8e-05 | Time: 0:52:32.461097 |\n",
      "| Epoch   0 |    66/  227 batches | Loss:   6.32 | Tokens:  1725 | Learning Rate: 1.8e-05 | Time: 0:53:18.964944 |\n",
      "| Epoch   0 |    67/  227 batches | Loss:   6.29 | Tokens:  1720 | Learning Rate: 1.8e-05 | Time: 0:54:08.100454 |\n",
      "| Epoch   0 |    68/  227 batches | Loss:   6.27 | Tokens:  1700 | Learning Rate: 1.9e-05 | Time: 0:54:55.575718 |\n",
      "| Epoch   0 |    69/  227 batches | Loss:   6.25 | Tokens:  1758 | Learning Rate: 1.9e-05 | Time: 0:55:42.186554 |\n",
      "| Epoch   0 |    70/  227 batches | Loss:   6.32 | Tokens:  1781 | Learning Rate: 1.9e-05 | Time: 0:56:26.132079 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   0 |    71/  227 batches | Loss:   6.29 | Tokens:  1755 | Learning Rate: 1.9e-05 | Time: 0:57:12.522978 |\n",
      "| Epoch   0 |    72/  227 batches | Loss:   6.23 | Tokens:  1710 | Learning Rate: 2.0e-05 | Time: 0:57:58.643895 |\n",
      "| Epoch   0 |    73/  227 batches | Loss:   6.26 | Tokens:  1669 | Learning Rate: 2.0e-05 | Time: 0:58:44.575481 |\n",
      "| Epoch   0 |    74/  227 batches | Loss:   6.25 | Tokens:  1764 | Learning Rate: 2.0e-05 | Time: 0:59:29.325651 |\n",
      "| Epoch   0 |    75/  227 batches | Loss:   6.20 | Tokens:  1747 | Learning Rate: 2.0e-05 | Time: 1:00:17.824932 |\n",
      "| Epoch   0 |    76/  227 batches | Loss:   6.21 | Tokens:  1781 | Learning Rate: 2.1e-05 | Time: 1:01:03.923181 |\n",
      "| Epoch   0 |    77/  227 batches | Loss:   6.15 | Tokens:  1721 | Learning Rate: 2.1e-05 | Time: 1:01:48.339299 |\n",
      "| Epoch   0 |    78/  227 batches | Loss:   6.18 | Tokens:  1723 | Learning Rate: 2.1e-05 | Time: 1:02:33.121294 |\n",
      "| Epoch   0 |    79/  227 batches | Loss:   6.20 | Tokens:  1710 | Learning Rate: 2.2e-05 | Time: 1:03:20.933832 |\n",
      "| Epoch   0 |    80/  227 batches | Loss:   6.18 | Tokens:  1721 | Learning Rate: 2.2e-05 | Time: 1:04:07.592284 |\n",
      "| Epoch   0 |    81/  227 batches | Loss:   6.13 | Tokens:  1737 | Learning Rate: 2.2e-05 | Time: 1:04:53.933637 |\n",
      "| Epoch   0 |    82/  227 batches | Loss:   6.10 | Tokens:  1690 | Learning Rate: 2.2e-05 | Time: 1:05:41.335656 |\n",
      "| Epoch   0 |    83/  227 batches | Loss:   6.15 | Tokens:  1668 | Learning Rate: 2.3e-05 | Time: 1:06:25.524146 |\n",
      "| Epoch   0 |    84/  227 batches | Loss:   6.09 | Tokens:  1690 | Learning Rate: 2.3e-05 | Time: 1:07:09.402470 |\n",
      "| Epoch   0 |    85/  227 batches | Loss:   6.08 | Tokens:  1712 | Learning Rate: 2.3e-05 | Time: 1:07:54.522573 |\n",
      "| Epoch   0 |    86/  227 batches | Loss:   6.09 | Tokens:  1714 | Learning Rate: 2.3e-05 | Time: 1:08:38.160157 |\n",
      "| Epoch   0 |    87/  227 batches | Loss:   6.05 | Tokens:  1705 | Learning Rate: 2.4e-05 | Time: 1:09:23.136213 |\n",
      "| Epoch   0 |    88/  227 batches | Loss:   6.06 | Tokens:  1684 | Learning Rate: 2.4e-05 | Time: 1:10:07.392711 |\n",
      "| Epoch   0 |    89/  227 batches | Loss:   5.99 | Tokens:  1671 | Learning Rate: 2.4e-05 | Time: 1:11:15.317562 |\n",
      "| Epoch   0 |    90/  227 batches | Loss:   6.09 | Tokens:  1713 | Learning Rate: 2.4e-05 | Time: 1:12:14.086425 |\n",
      "| Epoch   0 |    91/  227 batches | Loss:   6.00 | Tokens:  1691 | Learning Rate: 2.5e-05 | Time: 1:13:08.016781 |\n",
      "| Epoch   0 |    92/  227 batches | Loss:   6.05 | Tokens:  1741 | Learning Rate: 2.5e-05 | Time: 1:14:09.116475 |\n",
      "| Epoch   0 |    93/  227 batches | Loss:   5.97 | Tokens:  1663 | Learning Rate: 2.5e-05 | Time: 1:15:00.867831 |\n",
      "| Epoch   0 |    94/  227 batches | Loss:   5.95 | Tokens:  1686 | Learning Rate: 2.6e-05 | Time: 1:15:52.668407 |\n",
      "| Epoch   0 |    95/  227 batches | Loss:   5.98 | Tokens:  1729 | Learning Rate: 2.6e-05 | Time: 1:16:39.727315 |\n",
      "| Epoch   0 |    96/  227 batches | Loss:   5.92 | Tokens:  1765 | Learning Rate: 2.6e-05 | Time: 1:17:28.129247 |\n",
      "| Epoch   0 |    97/  227 batches | Loss:   5.93 | Tokens:  1682 | Learning Rate: 2.6e-05 | Time: 1:18:15.670424 |\n",
      "| Epoch   0 |    98/  227 batches | Loss:   5.87 | Tokens:  1728 | Learning Rate: 2.7e-05 | Time: 1:19:08.435351 |\n",
      "| Epoch   0 |    99/  227 batches | Loss:   5.88 | Tokens:  1701 | Learning Rate: 2.7e-05 | Time: 1:19:57.435699 |\n",
      "| Epoch   0 |   100/  227 batches | Loss:   5.87 | Tokens:  1694 | Learning Rate: 2.7e-05 | Time: 1:20:47.412107 |\n",
      "| Epoch   0 |   101/  227 batches | Loss:   5.78 | Tokens:  1716 | Learning Rate: 2.7e-05 | Time: 1:21:32.985116 |\n",
      "| Epoch   0 |   102/  227 batches | Loss:   5.80 | Tokens:  1758 | Learning Rate: 2.8e-05 | Time: 1:22:21.012471 |\n",
      "| Epoch   0 |   103/  227 batches | Loss:   5.85 | Tokens:  1728 | Learning Rate: 2.8e-05 | Time: 1:23:09.343489 |\n",
      "| Epoch   0 |   104/  227 batches | Loss:   5.81 | Tokens:  1717 | Learning Rate: 2.8e-05 | Time: 1:24:00.939153 |\n",
      "| Epoch   0 |   105/  227 batches | Loss:   5.79 | Tokens:  1740 | Learning Rate: 2.9e-05 | Time: 1:24:48.326042 |\n",
      "| Epoch   0 |   106/  227 batches | Loss:   5.72 | Tokens:  1695 | Learning Rate: 2.9e-05 | Time: 1:25:38.251301 |\n",
      "| Epoch   0 |   107/  227 batches | Loss:   5.76 | Tokens:  1766 | Learning Rate: 2.9e-05 | Time: 1:26:25.142879 |\n",
      "| Epoch   0 |   108/  227 batches | Loss:   5.78 | Tokens:  1750 | Learning Rate: 2.9e-05 | Time: 1:27:18.584472 |\n",
      "| Epoch   0 |   109/  227 batches | Loss:   5.68 | Tokens:  1725 | Learning Rate: 3.0e-05 | Time: 1:28:05.793219 |\n",
      "| Epoch   0 |   110/  227 batches | Loss:   5.68 | Tokens:  1700 | Learning Rate: 3.0e-05 | Time: 1:28:55.299016 |\n",
      "| Epoch   0 |   111/  227 batches | Loss:   5.64 | Tokens:  1678 | Learning Rate: 3.0e-05 | Time: 1:29:45.429301 |\n",
      "| Epoch   0 |   112/  227 batches | Loss:   5.64 | Tokens:  1741 | Learning Rate: 3.0e-05 | Time: 1:30:33.575065 |\n",
      "| Epoch   0 |   113/  227 batches | Loss:   5.59 | Tokens:  1659 | Learning Rate: 3.1e-05 | Time: 1:31:19.860996 |\n",
      "| Epoch   0 |   114/  227 batches | Loss:   5.63 | Tokens:  1703 | Learning Rate: 3.1e-05 | Time: 1:32:07.530476 |\n",
      "| Epoch   0 |   115/  227 batches | Loss:   5.58 | Tokens:  1812 | Learning Rate: 3.1e-05 | Time: 1:32:50.941809 |\n",
      "| Epoch   0 |   116/  227 batches | Loss:   5.50 | Tokens:  1765 | Learning Rate: 3.1e-05 | Time: 1:33:34.620003 |\n",
      "| Epoch   0 |   117/  227 batches | Loss:   5.56 | Tokens:  1705 | Learning Rate: 3.2e-05 | Time: 1:34:21.145216 |\n",
      "| Epoch   0 |   118/  227 batches | Loss:   5.51 | Tokens:  1787 | Learning Rate: 3.2e-05 | Time: 1:35:08.585944 |\n",
      "| Epoch   0 |   119/  227 batches | Loss:   5.58 | Tokens:  1861 | Learning Rate: 3.2e-05 | Time: 1:35:54.733556 |\n",
      "| Epoch   0 |   120/  227 batches | Loss:   5.55 | Tokens:  1808 | Learning Rate: 3.3e-05 | Time: 1:36:41.812349 |\n",
      "| Epoch   0 |   121/  227 batches | Loss:   5.53 | Tokens:  1817 | Learning Rate: 3.3e-05 | Time: 1:37:25.821332 |\n",
      "| Epoch   0 |   122/  227 batches | Loss:   5.48 | Tokens:  1773 | Learning Rate: 3.3e-05 | Time: 1:38:14.712192 |\n",
      "| Epoch   0 |   123/  227 batches | Loss:   5.43 | Tokens:  1815 | Learning Rate: 3.3e-05 | Time: 1:38:59.166002 |\n",
      "| Epoch   0 |   124/  227 batches | Loss:   5.46 | Tokens:  1835 | Learning Rate: 3.4e-05 | Time: 1:39:44.931607 |\n",
      "| Epoch   0 |   125/  227 batches | Loss:   5.47 | Tokens:  1915 | Learning Rate: 3.4e-05 | Time: 1:40:36.474471 |\n",
      "| Epoch   0 |   126/  227 batches | Loss:   5.35 | Tokens:  1790 | Learning Rate: 3.4e-05 | Time: 1:41:23.512531 |\n",
      "| Epoch   0 |   127/  227 batches | Loss:   5.40 | Tokens:  1831 | Learning Rate: 3.4e-05 | Time: 1:42:09.171848 |\n",
      "| Epoch   0 |   128/  227 batches | Loss:   5.35 | Tokens:  1776 | Learning Rate: 3.5e-05 | Time: 1:42:54.229470 |\n",
      "| Epoch   0 |   129/  227 batches | Loss:   5.40 | Tokens:  1890 | Learning Rate: 3.5e-05 | Time: 1:43:40.038275 |\n",
      "| Epoch   0 |   130/  227 batches | Loss:   5.31 | Tokens:  1800 | Learning Rate: 3.5e-05 | Time: 1:44:30.853082 |\n",
      "| Epoch   0 |   131/  227 batches | Loss:   5.31 | Tokens:  1787 | Learning Rate: 3.6e-05 | Time: 1:45:22.058790 |\n",
      "| Epoch   0 |   132/  227 batches | Loss:   5.25 | Tokens:  1809 | Learning Rate: 3.6e-05 | Time: 1:46:10.074757 |\n",
      "| Epoch   0 |   133/  227 batches | Loss:   5.28 | Tokens:  1821 | Learning Rate: 3.6e-05 | Time: 1:46:58.152524 |\n",
      "| Epoch   0 |   134/  227 batches | Loss:   5.30 | Tokens:  1887 | Learning Rate: 3.6e-05 | Time: 1:47:44.513657 |\n",
      "| Epoch   0 |   135/  227 batches | Loss:   5.25 | Tokens:  1865 | Learning Rate: 3.7e-05 | Time: 1:48:28.395260 |\n",
      "| Epoch   0 |   136/  227 batches | Loss:   5.27 | Tokens:  1843 | Learning Rate: 3.7e-05 | Time: 1:49:14.502496 |\n",
      "| Epoch   0 |   137/  227 batches | Loss:   5.20 | Tokens:  1822 | Learning Rate: 3.7e-05 | Time: 1:49:58.206641 |\n",
      "| Epoch   0 |   138/  227 batches | Loss:   5.13 | Tokens:  1836 | Learning Rate: 3.7e-05 | Time: 1:50:45.451055 |\n",
      "| Epoch   0 |   139/  227 batches | Loss:   5.13 | Tokens:  1857 | Learning Rate: 3.8e-05 | Time: 1:51:30.622117 |\n",
      "| Epoch   0 |   140/  227 batches | Loss:   5.09 | Tokens:  1798 | Learning Rate: 3.8e-05 | Time: 1:52:16.301174 |\n",
      "| Epoch   0 |   141/  227 batches | Loss:   5.07 | Tokens:  1829 | Learning Rate: 3.8e-05 | Time: 1:53:04.270065 |\n",
      "| Epoch   0 |   142/  227 batches | Loss:   5.02 | Tokens:  1882 | Learning Rate: 3.8e-05 | Time: 1:53:49.660517 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   0 |   143/  227 batches | Loss:   5.02 | Tokens:  1860 | Learning Rate: 3.9e-05 | Time: 1:54:38.563361 |\n",
      "| Epoch   0 |   144/  227 batches | Loss:   5.01 | Tokens:  1867 | Learning Rate: 3.9e-05 | Time: 1:55:21.806534 |\n",
      "| Epoch   0 |   145/  227 batches | Loss:   5.06 | Tokens:  1832 | Learning Rate: 3.9e-05 | Time: 1:56:07.363344 |\n",
      "| Epoch   0 |   146/  227 batches | Loss:   5.03 | Tokens:  1888 | Learning Rate: 4.0e-05 | Time: 1:56:55.639392 |\n",
      "| Epoch   0 |   147/  227 batches | Loss:   4.96 | Tokens:  1829 | Learning Rate: 4.0e-05 | Time: 1:57:39.127458 |\n",
      "| Epoch   0 |   148/  227 batches | Loss:   4.91 | Tokens:  1764 | Learning Rate: 4.0e-05 | Time: 1:58:23.846070 |\n",
      "| Epoch   0 |   149/  227 batches | Loss:   4.91 | Tokens:  1790 | Learning Rate: 4.0e-05 | Time: 1:59:08.282601 |\n",
      "| Epoch   0 |   150/  227 batches | Loss:   4.97 | Tokens:  1889 | Learning Rate: 4.1e-05 | Time: 1:59:50.764932 |\n",
      "| Epoch   0 |   151/  227 batches | Loss:   4.89 | Tokens:  1870 | Learning Rate: 4.1e-05 | Time: 2:00:37.341454 |\n",
      "| Epoch   0 |   152/  227 batches | Loss:   4.96 | Tokens:  1898 | Learning Rate: 4.1e-05 | Time: 2:01:21.392494 |\n",
      "| Epoch   0 |   153/  227 batches | Loss:   4.90 | Tokens:  1892 | Learning Rate: 4.1e-05 | Time: 2:02:10.298561 |\n",
      "| Epoch   0 |   154/  227 batches | Loss:   4.85 | Tokens:  1804 | Learning Rate: 4.2e-05 | Time: 2:02:58.037199 |\n",
      "| Epoch   0 |   155/  227 batches | Loss:   4.88 | Tokens:  1884 | Learning Rate: 4.2e-05 | Time: 2:03:47.195220 |\n",
      "| Epoch   0 |   156/  227 batches | Loss:   4.88 | Tokens:  1799 | Learning Rate: 4.2e-05 | Time: 2:04:33.094600 |\n",
      "| Epoch   0 |   157/  227 batches | Loss:   4.85 | Tokens:  1827 | Learning Rate: 4.2e-05 | Time: 2:05:20.038406 |\n",
      "| Epoch   0 |   158/  227 batches | Loss:   4.75 | Tokens:  1810 | Learning Rate: 4.3e-05 | Time: 2:06:05.597204 |\n",
      "| Epoch   0 |   159/  227 batches | Loss:   4.86 | Tokens:  1859 | Learning Rate: 4.3e-05 | Time: 2:06:50.557661 |\n",
      "| Epoch   0 |   160/  227 batches | Loss:   4.79 | Tokens:  1835 | Learning Rate: 4.3e-05 | Time: 2:07:36.411676 |\n",
      "| Epoch   0 |   161/  227 batches | Loss:   4.73 | Tokens:  1820 | Learning Rate: 4.4e-05 | Time: 2:08:19.673493 |\n",
      "| Epoch   0 |   162/  227 batches | Loss:   4.72 | Tokens:  1789 | Learning Rate: 4.4e-05 | Time: 2:09:08.449209 |\n",
      "| Epoch   0 |   163/  227 batches | Loss:   4.82 | Tokens:  1895 | Learning Rate: 4.4e-05 | Time: 2:09:53.019095 |\n",
      "| Epoch   0 |   164/  227 batches | Loss:   4.76 | Tokens:  1853 | Learning Rate: 4.4e-05 | Time: 2:10:40.677908 |\n",
      "| Epoch   0 |   165/  227 batches | Loss:   4.69 | Tokens:  1799 | Learning Rate: 4.5e-05 | Time: 2:11:24.829564 |\n",
      "| Epoch   0 |   166/  227 batches | Loss:   4.68 | Tokens:  1819 | Learning Rate: 4.5e-05 | Time: 2:12:12.927687 |\n",
      "| Epoch   0 |   167/  227 batches | Loss:   4.60 | Tokens:  1806 | Learning Rate: 4.5e-05 | Time: 2:12:59.867312 |\n",
      "| Epoch   0 |   168/  227 batches | Loss:   4.75 | Tokens:  1901 | Learning Rate: 4.5e-05 | Time: 2:13:46.279699 |\n",
      "| Epoch   0 |   169/  227 batches | Loss:   4.59 | Tokens:  1805 | Learning Rate: 4.6e-05 | Time: 2:14:30.959183 |\n",
      "| Epoch   0 |   170/  227 batches | Loss:   4.59 | Tokens:  1851 | Learning Rate: 4.6e-05 | Time: 2:15:16.262378 |\n",
      "| Epoch   0 |   171/  227 batches | Loss:   4.62 | Tokens:  1858 | Learning Rate: 4.6e-05 | Time: 2:16:03.899209 |\n",
      "| Epoch   0 |   172/  227 batches | Loss:   4.67 | Tokens:  1884 | Learning Rate: 4.7e-05 | Time: 2:16:50.843464 |\n",
      "| Epoch   0 |   173/  227 batches | Loss:   4.61 | Tokens:  1852 | Learning Rate: 4.7e-05 | Time: 2:17:34.364470 |\n",
      "| Epoch   0 |   174/  227 batches | Loss:   4.61 | Tokens:  1837 | Learning Rate: 4.7e-05 | Time: 2:18:20.919475 |\n",
      "| Epoch   0 |   175/  227 batches | Loss:   4.56 | Tokens:  1800 | Learning Rate: 4.7e-05 | Time: 2:19:06.287254 |\n",
      "| Epoch   0 |   176/  227 batches | Loss:   4.54 | Tokens:  1810 | Learning Rate: 4.8e-05 | Time: 2:19:49.903866 |\n",
      "| Epoch   0 |   177/  227 batches | Loss:   4.56 | Tokens:  1848 | Learning Rate: 4.8e-05 | Time: 2:20:35.308632 |\n",
      "| Epoch   0 |   178/  227 batches | Loss:   4.59 | Tokens:  1800 | Learning Rate: 4.8e-05 | Time: 2:21:20.475455 |\n",
      "| Epoch   0 |   179/  227 batches | Loss:   4.46 | Tokens:  1850 | Learning Rate: 4.8e-05 | Time: 2:22:07.237279 |\n",
      "| Epoch   0 |   180/  227 batches | Loss:   4.52 | Tokens:  1816 | Learning Rate: 4.9e-05 | Time: 2:22:54.404896 |\n",
      "| Epoch   0 |   181/  227 batches | Loss:   4.50 | Tokens:  1889 | Learning Rate: 4.9e-05 | Time: 2:23:43.441409 |\n",
      "| Epoch   0 |   182/  227 batches | Loss:   4.58 | Tokens:  1778 | Learning Rate: 4.9e-05 | Time: 2:24:30.891840 |\n",
      "| Epoch   0 |   183/  227 batches | Loss:   4.47 | Tokens:  1814 | Learning Rate: 4.9e-05 | Time: 2:25:21.333883 |\n",
      "| Epoch   0 |   184/  227 batches | Loss:   4.60 | Tokens:  1888 | Learning Rate: 5.0e-05 | Time: 2:26:06.561442 |\n",
      "| Epoch   0 |   185/  227 batches | Loss:   4.64 | Tokens:  1855 | Learning Rate: 5.0e-05 | Time: 2:26:52.741602 |\n",
      "| Epoch   0 |   186/  227 batches | Loss:   4.55 | Tokens:  1862 | Learning Rate: 5.0e-05 | Time: 2:27:36.985032 |\n",
      "| Epoch   0 |   187/  227 batches | Loss:   4.55 | Tokens:  1846 | Learning Rate: 5.1e-05 | Time: 2:28:23.515010 |\n",
      "| Epoch   0 |   188/  227 batches | Loss:   4.61 | Tokens:  1900 | Learning Rate: 5.1e-05 | Time: 2:29:13.504562 |\n",
      "| Epoch   0 |   189/  227 batches | Loss:   4.59 | Tokens:  1854 | Learning Rate: 5.1e-05 | Time: 2:29:57.181824 |\n",
      "| Epoch   0 |   190/  227 batches | Loss:   4.59 | Tokens:  1919 | Learning Rate: 5.1e-05 | Time: 2:30:43.715645 |\n",
      "| Epoch   0 |   191/  227 batches | Loss:   4.55 | Tokens:  1894 | Learning Rate: 5.2e-05 | Time: 2:31:29.066164 |\n",
      "| Epoch   0 |   192/  227 batches | Loss:   4.65 | Tokens:  2015 | Learning Rate: 5.2e-05 | Time: 2:32:14.919220 |\n",
      "| Epoch   0 |   193/  227 batches | Loss:   4.50 | Tokens:  1861 | Learning Rate: 5.2e-05 | Time: 2:33:02.489854 |\n",
      "| Epoch   0 |   194/  227 batches | Loss:   4.55 | Tokens:  1987 | Learning Rate: 5.2e-05 | Time: 2:33:49.701418 |\n",
      "| Epoch   0 |   195/  227 batches | Loss:   4.55 | Tokens:  1965 | Learning Rate: 5.3e-05 | Time: 2:34:36.584720 |\n",
      "| Epoch   0 |   196/  227 batches | Loss:   4.57 | Tokens:  2014 | Learning Rate: 5.3e-05 | Time: 2:35:20.847950 |\n",
      "| Epoch   0 |   197/  227 batches | Loss:   4.53 | Tokens:  1950 | Learning Rate: 5.3e-05 | Time: 2:36:04.027813 |\n",
      "| Epoch   0 |   198/  227 batches | Loss:   4.57 | Tokens:  1901 | Learning Rate: 5.4e-05 | Time: 2:36:50.374837 |\n",
      "| Epoch   0 |   199/  227 batches | Loss:   4.59 | Tokens:  2041 | Learning Rate: 5.4e-05 | Time: 2:37:33.708286 |\n",
      "| Epoch   0 |   200/  227 batches | Loss:   4.55 | Tokens:  1937 | Learning Rate: 5.4e-05 | Time: 2:38:21.503785 |\n",
      "| Epoch   0 |   201/  227 batches | Loss:   4.62 | Tokens:  1934 | Learning Rate: 5.4e-05 | Time: 2:39:07.591345 |\n",
      "| Epoch   0 |   202/  227 batches | Loss:   4.55 | Tokens:  1939 | Learning Rate: 5.5e-05 | Time: 2:39:50.359110 |\n",
      "| Epoch   0 |   203/  227 batches | Loss:   4.68 | Tokens:  2015 | Learning Rate: 5.5e-05 | Time: 2:40:31.852035 |\n",
      "| Epoch   0 |   204/  227 batches | Loss:   4.48 | Tokens:  1894 | Learning Rate: 5.5e-05 | Time: 2:41:18.789645 |\n",
      "| Epoch   0 |   205/  227 batches | Loss:   4.54 | Tokens:  1985 | Learning Rate: 5.5e-05 | Time: 2:42:10.239888 |\n",
      "| Epoch   0 |   206/  227 batches | Loss:   4.50 | Tokens:  1928 | Learning Rate: 5.6e-05 | Time: 2:43:00.120976 |\n",
      "| Epoch   0 |   207/  227 batches | Loss:   4.54 | Tokens:  1855 | Learning Rate: 5.6e-05 | Time: 2:43:47.566818 |\n",
      "| Epoch   0 |   208/  227 batches | Loss:   4.49 | Tokens:  1938 | Learning Rate: 5.6e-05 | Time: 2:44:32.239573 |\n",
      "| Epoch   0 |   209/  227 batches | Loss:   4.58 | Tokens:  2011 | Learning Rate: 5.6e-05 | Time: 2:45:28.712687 |\n",
      "| Epoch   0 |   210/  227 batches | Loss:   4.49 | Tokens:  1921 | Learning Rate: 5.7e-05 | Time: 2:46:17.880557 |\n",
      "| Epoch   0 |   211/  227 batches | Loss:   4.62 | Tokens:  1959 | Learning Rate: 5.7e-05 | Time: 2:47:03.219360 |\n",
      "| Epoch   0 |   212/  227 batches | Loss:   4.51 | Tokens:  1962 | Learning Rate: 5.7e-05 | Time: 2:47:48.966023 |\n",
      "| Epoch   0 |   213/  227 batches | Loss:   4.58 | Tokens:  1922 | Learning Rate: 5.8e-05 | Time: 2:48:33.455827 |\n",
      "| Epoch   0 |   214/  227 batches | Loss:   4.44 | Tokens:  1906 | Learning Rate: 5.8e-05 | Time: 2:49:20.278599 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   0 |   215/  227 batches | Loss:   4.44 | Tokens:  1911 | Learning Rate: 5.8e-05 | Time: 2:50:04.846523 |\n",
      "| Epoch   0 |   216/  227 batches | Loss:   4.33 | Tokens:  1840 | Learning Rate: 5.8e-05 | Time: 2:50:48.618488 |\n",
      "| Epoch   0 |   217/  227 batches | Loss:   4.53 | Tokens:  1967 | Learning Rate: 5.9e-05 | Time: 2:51:35.303857 |\n",
      "| Epoch   0 |   218/  227 batches | Loss:   4.52 | Tokens:  1920 | Learning Rate: 5.9e-05 | Time: 2:52:25.899691 |\n",
      "| Epoch   0 |   219/  227 batches | Loss:   4.50 | Tokens:  1933 | Learning Rate: 5.9e-05 | Time: 2:53:16.116080 |\n",
      "| Epoch   0 |   220/  227 batches | Loss:   4.57 | Tokens:  2004 | Learning Rate: 5.9e-05 | Time: 2:54:04.743613 |\n",
      "| Epoch   0 |   221/  227 batches | Loss:   4.43 | Tokens:  1910 | Learning Rate: 6.0e-05 | Time: 2:54:54.010412 |\n",
      "| Epoch   0 |   222/  227 batches | Loss:   4.38 | Tokens:  1899 | Learning Rate: 6.0e-05 | Time: 2:55:39.853488 |\n",
      "| Epoch   0 |   223/  227 batches | Loss:   4.31 | Tokens:  1864 | Learning Rate: 6.0e-05 | Time: 2:56:28.452201 |\n",
      "| Epoch   0 |   224/  227 batches | Loss:   4.27 | Tokens:  1752 | Learning Rate: 6.1e-05 | Time: 2:57:25.770510 |\n",
      "| Epoch   0 |   225/  227 batches | Loss:   4.23 | Tokens:  1765 | Learning Rate: 6.1e-05 | Time: 2:58:17.080118 |\n",
      "| Epoch   0 |   226/  227 batches | Loss:   4.34 | Tokens:   981 | Learning Rate: 6.1e-05 | Time: 2:58:38.382973 |\n",
      "| Epoch   0 |     0/    8 batches | Loss:   4.27 | Tokens:  1835 | Time: 0:00:13.363285 |\n",
      "| Epoch   0 |     1/    8 batches | Loss:   4.17 | Tokens:  1756 | Time: 0:00:25.873304 |\n",
      "| Epoch   0 |     2/    8 batches | Loss:   4.23 | Tokens:  1796 | Time: 0:00:38.222547 |\n",
      "| Epoch   0 |     3/    8 batches | Loss:   4.16 | Tokens:  1690 | Time: 0:00:50.576387 |\n",
      "| Epoch   0 |     4/    8 batches | Loss:   4.28 | Tokens:  1840 | Time: 0:01:04.073287 |\n",
      "| Epoch   0 |     5/    8 batches | Loss:   4.19 | Tokens:  1882 | Time: 0:01:16.493066 |\n",
      "| Epoch   0 |     6/    8 batches | Loss:   4.17 | Tokens:  1877 | Time: 0:01:28.876941 |\n",
      "| Epoch   0 |     7/    8 batches | Loss:   4.35 | Tokens:  1765 | Time: 0:01:40.355239 |\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   0 | Train Loss:    5.647 | Val Loss:    4.228 | time: 3:00:18.755165 |\n",
      "-----------------------------------------------------------\n",
      "| Epoch   1 |     0/  227 batches | Loss:   4.22 | Tokens:  1792 | Learning Rate: 6.1e-05 | Time: 0:00:46.409145 |\n",
      "| Epoch   1 |     1/  227 batches | Loss:   4.32 | Tokens:  1795 | Learning Rate: 6.2e-05 | Time: 0:01:32.276463 |\n",
      "| Epoch   1 |     2/  227 batches | Loss:   4.18 | Tokens:  1791 | Learning Rate: 6.2e-05 | Time: 0:02:16.601437 |\n",
      "| Epoch   1 |     3/  227 batches | Loss:   4.35 | Tokens:  1813 | Learning Rate: 6.2e-05 | Time: 0:03:03.655664 |\n",
      "| Epoch   1 |     4/  227 batches | Loss:   4.26 | Tokens:  1807 | Learning Rate: 6.2e-05 | Time: 0:03:54.648757 |\n",
      "| Epoch   1 |     5/  227 batches | Loss:   4.18 | Tokens:  1824 | Learning Rate: 6.3e-05 | Time: 0:04:41.003980 |\n",
      "| Epoch   1 |     6/  227 batches | Loss:   4.30 | Tokens:  1794 | Learning Rate: 6.3e-05 | Time: 0:05:25.429430 |\n",
      "| Epoch   1 |     7/  227 batches | Loss:   4.15 | Tokens:  1796 | Learning Rate: 6.3e-05 | Time: 0:06:10.784172 |\n",
      "| Epoch   1 |     8/  227 batches | Loss:   4.10 | Tokens:  1748 | Learning Rate: 6.3e-05 | Time: 0:06:57.452204 |\n",
      "| Epoch   1 |     9/  227 batches | Loss:   4.16 | Tokens:  1834 | Learning Rate: 6.4e-05 | Time: 0:07:42.980431 |\n",
      "| Epoch   1 |    10/  227 batches | Loss:   4.28 | Tokens:  1797 | Learning Rate: 6.4e-05 | Time: 0:08:32.504670 |\n",
      "| Epoch   1 |    11/  227 batches | Loss:   4.23 | Tokens:  1745 | Learning Rate: 6.4e-05 | Time: 0:09:18.233533 |\n",
      "| Epoch   1 |    12/  227 batches | Loss:   4.26 | Tokens:  1776 | Learning Rate: 6.5e-05 | Time: 0:10:01.993815 |\n",
      "| Epoch   1 |    13/  227 batches | Loss:   4.12 | Tokens:  1722 | Learning Rate: 6.5e-05 | Time: 0:10:54.914883 |\n",
      "| Epoch   1 |    14/  227 batches | Loss:   4.16 | Tokens:  1782 | Learning Rate: 6.5e-05 | Time: 0:11:40.814106 |\n",
      "| Epoch   1 |    15/  227 batches | Loss:   4.23 | Tokens:  1825 | Learning Rate: 6.5e-05 | Time: 0:12:27.477034 |\n",
      "| Epoch   1 |    16/  227 batches | Loss:   4.11 | Tokens:  1788 | Learning Rate: 6.6e-05 | Time: 0:13:13.980342 |\n",
      "| Epoch   1 |    17/  227 batches | Loss:   4.20 | Tokens:  1793 | Learning Rate: 6.6e-05 | Time: 0:13:59.634632 |\n",
      "| Epoch   1 |    18/  227 batches | Loss:   4.25 | Tokens:  1935 | Learning Rate: 6.6e-05 | Time: 0:14:45.656669 |\n",
      "| Epoch   1 |    19/  227 batches | Loss:   4.18 | Tokens:  1785 | Learning Rate: 6.6e-05 | Time: 0:15:31.864367 |\n",
      "| Epoch   1 |    20/  227 batches | Loss:   4.15 | Tokens:  1748 | Learning Rate: 6.7e-05 | Time: 0:16:17.926884 |\n",
      "| Epoch   1 |    21/  227 batches | Loss:   4.16 | Tokens:  1762 | Learning Rate: 6.7e-05 | Time: 0:17:04.654742 |\n",
      "| Epoch   1 |    22/  227 batches | Loss:   4.15 | Tokens:  1734 | Learning Rate: 6.7e-05 | Time: 0:17:49.791557 |\n",
      "| Epoch   1 |    23/  227 batches | Loss:   4.15 | Tokens:  1732 | Learning Rate: 6.8e-05 | Time: 0:18:38.206948 |\n",
      "| Epoch   1 |    24/  227 batches | Loss:   4.21 | Tokens:  1739 | Learning Rate: 6.8e-05 | Time: 0:19:27.295703 |\n",
      "| Epoch   1 |    25/  227 batches | Loss:   4.06 | Tokens:  1793 | Learning Rate: 6.8e-05 | Time: 0:20:24.455164 |\n",
      "| Epoch   1 |    26/  227 batches | Loss:   4.18 | Tokens:  1839 | Learning Rate: 6.8e-05 | Time: 0:21:09.462769 |\n",
      "| Epoch   1 |    27/  227 batches | Loss:   4.18 | Tokens:  1790 | Learning Rate: 6.9e-05 | Time: 0:21:55.953982 |\n",
      "| Epoch   1 |    28/  227 batches | Loss:   4.03 | Tokens:  1727 | Learning Rate: 6.9e-05 | Time: 0:22:45.546245 |\n",
      "| Epoch   1 |    29/  227 batches | Loss:   4.10 | Tokens:  1773 | Learning Rate: 6.9e-05 | Time: 0:23:34.550803 |\n",
      "| Epoch   1 |    30/  227 batches | Loss:   4.18 | Tokens:  1803 | Learning Rate: 6.9e-05 | Time: 0:24:23.008819 |\n",
      "| Epoch   1 |    31/  227 batches | Loss:   4.13 | Tokens:  1729 | Learning Rate: 7.0e-05 | Time: 0:25:10.153800 |\n",
      "| Epoch   1 |    32/  227 batches | Loss:   4.12 | Tokens:  1722 | Learning Rate: 7.0e-05 | Time: 0:25:58.773249 |\n",
      "| Epoch   1 |    33/  227 batches | Loss:   4.09 | Tokens:  1670 | Learning Rate: 7.0e-05 | Time: 0:26:46.164502 |\n",
      "| Epoch   1 |    34/  227 batches | Loss:   4.05 | Tokens:  1771 | Learning Rate: 7.0e-05 | Time: 0:27:32.148369 |\n",
      "| Epoch   1 |    35/  227 batches | Loss:   4.09 | Tokens:  1737 | Learning Rate: 7.1e-05 | Time: 0:28:19.266210 |\n",
      "| Epoch   1 |    36/  227 batches | Loss:   3.97 | Tokens:  1693 | Learning Rate: 7.1e-05 | Time: 0:29:07.609144 |\n",
      "| Epoch   1 |    37/  227 batches | Loss:   4.06 | Tokens:  1748 | Learning Rate: 7.1e-05 | Time: 0:29:54.421314 |\n",
      "| Epoch   1 |    38/  227 batches | Loss:   4.03 | Tokens:  1842 | Learning Rate: 7.2e-05 | Time: 0:30:39.475725 |\n",
      "| Epoch   1 |    39/  227 batches | Loss:   4.11 | Tokens:  1784 | Learning Rate: 7.2e-05 | Time: 0:31:25.879801 |\n",
      "| Epoch   1 |    40/  227 batches | Loss:   4.22 | Tokens:  1747 | Learning Rate: 7.2e-05 | Time: 0:32:12.102527 |\n",
      "| Epoch   1 |    41/  227 batches | Loss:   4.03 | Tokens:  1806 | Learning Rate: 7.2e-05 | Time: 0:32:59.423068 |\n",
      "| Epoch   1 |    42/  227 batches | Loss:   4.09 | Tokens:  1811 | Learning Rate: 7.3e-05 | Time: 0:33:46.021216 |\n",
      "| Epoch   1 |    43/  227 batches | Loss:   4.06 | Tokens:  1823 | Learning Rate: 7.3e-05 | Time: 0:34:30.908556 |\n",
      "| Epoch   1 |    44/  227 batches | Loss:   4.04 | Tokens:  1729 | Learning Rate: 7.3e-05 | Time: 0:35:14.175185 |\n",
      "| Epoch   1 |    45/  227 batches | Loss:   3.94 | Tokens:  1668 | Learning Rate: 7.3e-05 | Time: 0:36:02.358927 |\n",
      "| Epoch   1 |    46/  227 batches | Loss:   4.01 | Tokens:  1776 | Learning Rate: 7.4e-05 | Time: 0:36:48.424124 |\n",
      "| Epoch   1 |    47/  227 batches | Loss:   4.20 | Tokens:  1692 | Learning Rate: 7.4e-05 | Time: 0:37:36.862919 |\n",
      "| Epoch   1 |    48/  227 batches | Loss:   3.96 | Tokens:  1746 | Learning Rate: 7.4e-05 | Time: 0:38:21.034760 |\n",
      "| Epoch   1 |    49/  227 batches | Loss:   3.95 | Tokens:  1684 | Learning Rate: 7.5e-05 | Time: 0:39:06.131811 |\n",
      "| Epoch   1 |    50/  227 batches | Loss:   4.01 | Tokens:  1793 | Learning Rate: 7.5e-05 | Time: 0:39:53.398333 |\n",
      "| Epoch   1 |    51/  227 batches | Loss:   3.94 | Tokens:  1800 | Learning Rate: 7.5e-05 | Time: 0:40:40.544049 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   1 |    52/  227 batches | Loss:   4.10 | Tokens:  1794 | Learning Rate: 7.5e-05 | Time: 0:41:27.650749 |\n",
      "| Epoch   1 |    53/  227 batches | Loss:   4.02 | Tokens:  1767 | Learning Rate: 7.6e-05 | Time: 0:42:13.001234 |\n",
      "| Epoch   1 |    54/  227 batches | Loss:   4.01 | Tokens:  1792 | Learning Rate: 7.6e-05 | Time: 0:43:01.840516 |\n",
      "| Epoch   1 |    55/  227 batches | Loss:   4.02 | Tokens:  1734 | Learning Rate: 7.6e-05 | Time: 0:43:49.752062 |\n",
      "| Epoch   1 |    56/  227 batches | Loss:   3.99 | Tokens:  1780 | Learning Rate: 7.6e-05 | Time: 0:44:38.497504 |\n",
      "| Epoch   1 |    57/  227 batches | Loss:   4.02 | Tokens:  1833 | Learning Rate: 7.7e-05 | Time: 0:45:32.900157 |\n",
      "| Epoch   1 |    58/  227 batches | Loss:   3.97 | Tokens:  1793 | Learning Rate: 7.7e-05 | Time: 0:46:19.623019 |\n",
      "| Epoch   1 |    59/  227 batches | Loss:   3.93 | Tokens:  1720 | Learning Rate: 7.7e-05 | Time: 0:47:05.898271 |\n",
      "| Epoch   1 |    60/  227 batches | Loss:   3.97 | Tokens:  1798 | Learning Rate: 7.7e-05 | Time: 0:47:52.703636 |\n",
      "| Epoch   1 |    61/  227 batches | Loss:   3.89 | Tokens:  1709 | Learning Rate: 7.8e-05 | Time: 0:48:40.281321 |\n",
      "| Epoch   1 |    62/  227 batches | Loss:   4.05 | Tokens:  1852 | Learning Rate: 7.8e-05 | Time: 0:49:26.627321 |\n",
      "| Epoch   1 |    63/  227 batches | Loss:   3.97 | Tokens:  1783 | Learning Rate: 7.8e-05 | Time: 0:50:14.343523 |\n",
      "| Epoch   1 |    64/  227 batches | Loss:   3.96 | Tokens:  1821 | Learning Rate: 7.9e-05 | Time: 0:51:09.113027 |\n",
      "| Epoch   1 |    65/  227 batches | Loss:   3.81 | Tokens:  1746 | Learning Rate: 7.9e-05 | Time: 0:51:56.292511 |\n",
      "| Epoch   1 |    66/  227 batches | Loss:   3.90 | Tokens:  1725 | Learning Rate: 7.9e-05 | Time: 0:52:46.352887 |\n",
      "| Epoch   1 |    67/  227 batches | Loss:   3.89 | Tokens:  1720 | Learning Rate: 7.9e-05 | Time: 0:53:35.533340 |\n",
      "| Epoch   1 |    68/  227 batches | Loss:   3.94 | Tokens:  1700 | Learning Rate: 8.0e-05 | Time: 0:54:27.590098 |\n",
      "| Epoch   1 |    69/  227 batches | Loss:   3.95 | Tokens:  1758 | Learning Rate: 8.0e-05 | Time: 0:55:23.644945 |\n",
      "| Epoch   1 |    70/  227 batches | Loss:   4.03 | Tokens:  1781 | Learning Rate: 8.0e-05 | Time: 0:56:12.870363 |\n",
      "| Epoch   1 |    71/  227 batches | Loss:   3.99 | Tokens:  1755 | Learning Rate: 8.0e-05 | Time: 0:57:02.238460 |\n",
      "| Epoch   1 |    72/  227 batches | Loss:   3.96 | Tokens:  1710 | Learning Rate: 8.1e-05 | Time: 0:57:52.630671 |\n",
      "| Epoch   1 |    73/  227 batches | Loss:   3.97 | Tokens:  1669 | Learning Rate: 8.1e-05 | Time: 0:58:39.513268 |\n",
      "| Epoch   1 |    74/  227 batches | Loss:   3.99 | Tokens:  1764 | Learning Rate: 8.1e-05 | Time: 0:59:29.830680 |\n",
      "| Epoch   1 |    75/  227 batches | Loss:   3.81 | Tokens:  1747 | Learning Rate: 8.1e-05 | Time: 1:00:14.990150 |\n",
      "| Epoch   1 |    76/  227 batches | Loss:   3.91 | Tokens:  1781 | Learning Rate: 8.2e-05 | Time: 1:01:07.736065 |\n",
      "| Epoch   1 |    77/  227 batches | Loss:   3.78 | Tokens:  1721 | Learning Rate: 8.2e-05 | Time: 1:01:57.875135 |\n",
      "| Epoch   1 |    78/  227 batches | Loss:   3.87 | Tokens:  1723 | Learning Rate: 8.2e-05 | Time: 1:02:46.195885 |\n",
      "| Epoch   1 |    79/  227 batches | Loss:   3.96 | Tokens:  1710 | Learning Rate: 8.3e-05 | Time: 1:03:36.043552 |\n",
      "| Epoch   1 |    80/  227 batches | Loss:   3.88 | Tokens:  1721 | Learning Rate: 8.3e-05 | Time: 1:04:26.150527 |\n",
      "| Epoch   1 |    81/  227 batches | Loss:   3.78 | Tokens:  1737 | Learning Rate: 8.3e-05 | Time: 1:05:14.794482 |\n",
      "| Epoch   1 |    82/  227 batches | Loss:   3.77 | Tokens:  1690 | Learning Rate: 8.3e-05 | Time: 1:06:04.666958 |\n",
      "| Epoch   1 |    83/  227 batches | Loss:   3.85 | Tokens:  1668 | Learning Rate: 8.4e-05 | Time: 1:06:54.409905 |\n",
      "| Epoch   1 |    84/  227 batches | Loss:   3.85 | Tokens:  1690 | Learning Rate: 8.4e-05 | Time: 1:07:43.120614 |\n",
      "| Epoch   1 |    85/  227 batches | Loss:   3.77 | Tokens:  1712 | Learning Rate: 8.4e-05 | Time: 1:08:29.615249 |\n",
      "| Epoch   1 |    86/  227 batches | Loss:   3.89 | Tokens:  1714 | Learning Rate: 8.4e-05 | Time: 1:09:16.775106 |\n",
      "| Epoch   1 |    87/  227 batches | Loss:   3.75 | Tokens:  1705 | Learning Rate: 8.5e-05 | Time: 1:10:05.801970 |\n",
      "| Epoch   1 |    88/  227 batches | Loss:   3.86 | Tokens:  1684 | Learning Rate: 8.5e-05 | Time: 1:11:09.231972 |\n",
      "| Epoch   1 |    89/  227 batches | Loss:   3.68 | Tokens:  1671 | Learning Rate: 8.5e-05 | Time: 1:12:05.181436 |\n",
      "| Epoch   1 |    90/  227 batches | Loss:   3.95 | Tokens:  1713 | Learning Rate: 8.6e-05 | Time: 1:13:02.124790 |\n",
      "| Epoch   1 |    91/  227 batches | Loss:   3.79 | Tokens:  1691 | Learning Rate: 8.6e-05 | Time: 1:14:01.702976 |\n",
      "| Epoch   1 |    92/  227 batches | Loss:   3.93 | Tokens:  1741 | Learning Rate: 8.6e-05 | Time: 1:14:57.463637 |\n",
      "| Epoch   1 |    93/  227 batches | Loss:   3.72 | Tokens:  1663 | Learning Rate: 8.6e-05 | Time: 1:15:53.448472 |\n",
      "| Epoch   1 |    94/  227 batches | Loss:   3.70 | Tokens:  1686 | Learning Rate: 8.7e-05 | Time: 1:16:46.976588 |\n",
      "| Epoch   1 |    95/  227 batches | Loss:   3.76 | Tokens:  1729 | Learning Rate: 8.7e-05 | Time: 1:17:42.874124 |\n",
      "| Epoch   1 |    96/  227 batches | Loss:   3.76 | Tokens:  1765 | Learning Rate: 8.7e-05 | Time: 1:18:34.862821 |\n",
      "| Epoch   1 |    97/  227 batches | Loss:   3.80 | Tokens:  1682 | Learning Rate: 8.7e-05 | Time: 1:19:25.982986 |\n",
      "| Epoch   1 |    98/  227 batches | Loss:   3.69 | Tokens:  1728 | Learning Rate: 8.8e-05 | Time: 1:20:24.501519 |\n",
      "| Epoch   1 |    99/  227 batches | Loss:   3.78 | Tokens:  1701 | Learning Rate: 8.8e-05 | Time: 1:21:14.313029 |\n",
      "| Epoch   1 |   100/  227 batches | Loss:   3.83 | Tokens:  1694 | Learning Rate: 8.8e-05 | Time: 1:22:07.087448 |\n",
      "| Epoch   1 |   101/  227 batches | Loss:   3.67 | Tokens:  1716 | Learning Rate: 8.8e-05 | Time: 1:23:03.856241 |\n",
      "| Epoch   1 |   102/  227 batches | Loss:   3.67 | Tokens:  1758 | Learning Rate: 8.9e-05 | Time: 1:24:01.904650 |\n",
      "| Epoch   1 |   103/  227 batches | Loss:   3.83 | Tokens:  1728 | Learning Rate: 8.9e-05 | Time: 1:24:53.529811 |\n",
      "| Epoch   1 |   104/  227 batches | Loss:   3.78 | Tokens:  1717 | Learning Rate: 8.9e-05 | Time: 1:25:45.069418 |\n",
      "| Epoch   1 |   105/  227 batches | Loss:   3.79 | Tokens:  1740 | Learning Rate: 9.0e-05 | Time: 1:26:35.373864 |\n",
      "| Epoch   1 |   106/  227 batches | Loss:   3.62 | Tokens:  1695 | Learning Rate: 9.0e-05 | Time: 1:27:27.518836 |\n",
      "| Epoch   1 |   107/  227 batches | Loss:   3.73 | Tokens:  1766 | Learning Rate: 9.0e-05 | Time: 1:28:16.782067 |\n",
      "| Epoch   1 |   108/  227 batches | Loss:   3.82 | Tokens:  1750 | Learning Rate: 9.0e-05 | Time: 1:29:04.312476 |\n",
      "| Epoch   1 |   109/  227 batches | Loss:   3.68 | Tokens:  1725 | Learning Rate: 9.1e-05 | Time: 1:29:56.288449 |\n",
      "| Epoch   1 |   110/  227 batches | Loss:   3.68 | Tokens:  1700 | Learning Rate: 9.1e-05 | Time: 1:30:46.908053 |\n",
      "| Epoch   1 |   111/  227 batches | Loss:   3.69 | Tokens:  1678 | Learning Rate: 9.1e-05 | Time: 1:31:35.210114 |\n",
      "| Epoch   1 |   112/  227 batches | Loss:   3.71 | Tokens:  1741 | Learning Rate: 9.1e-05 | Time: 1:32:21.941116 |\n",
      "| Epoch   1 |   113/  227 batches | Loss:   3.64 | Tokens:  1659 | Learning Rate: 9.2e-05 | Time: 1:33:08.563410 |\n",
      "| Epoch   1 |   114/  227 batches | Loss:   3.84 | Tokens:  1703 | Learning Rate: 9.2e-05 | Time: 1:34:01.291666 |\n",
      "| Epoch   1 |   115/  227 batches | Loss:   3.70 | Tokens:  1812 | Learning Rate: 9.2e-05 | Time: 1:34:55.698163 |\n",
      "| Epoch   1 |   116/  227 batches | Loss:   3.63 | Tokens:  1765 | Learning Rate: 9.3e-05 | Time: 1:35:48.562761 |\n",
      "| Epoch   1 |   117/  227 batches | Loss:   3.74 | Tokens:  1705 | Learning Rate: 9.3e-05 | Time: 1:36:36.035780 |\n",
      "| Epoch   1 |   118/  227 batches | Loss:   3.73 | Tokens:  1787 | Learning Rate: 9.3e-05 | Time: 1:37:23.350223 |\n",
      "| Epoch   1 |   119/  227 batches | Loss:   3.79 | Tokens:  1861 | Learning Rate: 9.3e-05 | Time: 1:38:11.205952 |\n",
      "| Epoch   1 |   120/  227 batches | Loss:   3.84 | Tokens:  1808 | Learning Rate: 9.4e-05 | Time: 1:38:59.268564 |\n",
      "| Epoch   1 |   121/  227 batches | Loss:   3.81 | Tokens:  1817 | Learning Rate: 9.4e-05 | Time: 1:39:47.985255 |\n",
      "| Epoch   1 |   122/  227 batches | Loss:   3.79 | Tokens:  1773 | Learning Rate: 9.4e-05 | Time: 1:40:34.731217 |\n",
      "| Epoch   1 |   123/  227 batches | Loss:   3.72 | Tokens:  1815 | Learning Rate: 9.4e-05 | Time: 1:41:28.680912 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   1 |   124/  227 batches | Loss:   3.80 | Tokens:  1835 | Learning Rate: 9.5e-05 | Time: 1:42:29.223696 |\n",
      "| Epoch   1 |   125/  227 batches | Loss:   3.80 | Tokens:  1915 | Learning Rate: 9.5e-05 | Time: 1:43:21.192478 |\n",
      "| Epoch   1 |   126/  227 batches | Loss:   3.68 | Tokens:  1790 | Learning Rate: 9.5e-05 | Time: 1:44:13.072019 |\n",
      "| Epoch   1 |   127/  227 batches | Loss:   3.81 | Tokens:  1831 | Learning Rate: 9.5e-05 | Time: 1:45:07.947999 |\n",
      "| Epoch   1 |   128/  227 batches | Loss:   3.75 | Tokens:  1776 | Learning Rate: 9.6e-05 | Time: 1:46:04.544400 |\n",
      "| Epoch   1 |   129/  227 batches | Loss:   3.86 | Tokens:  1890 | Learning Rate: 9.6e-05 | Time: 1:46:56.144266 |\n",
      "| Epoch   1 |   130/  227 batches | Loss:   3.72 | Tokens:  1800 | Learning Rate: 9.6e-05 | Time: 1:47:52.231985 |\n",
      "| Epoch   1 |   131/  227 batches | Loss:   3.78 | Tokens:  1787 | Learning Rate: 9.7e-05 | Time: 1:48:42.702585 |\n",
      "| Epoch   1 |   132/  227 batches | Loss:   3.62 | Tokens:  1809 | Learning Rate: 9.7e-05 | Time: 1:49:34.487359 |\n",
      "| Epoch   1 |   133/  227 batches | Loss:   3.76 | Tokens:  1821 | Learning Rate: 9.7e-05 | Time: 1:50:26.638203 |\n",
      "| Epoch   1 |   134/  227 batches | Loss:   3.77 | Tokens:  1887 | Learning Rate: 9.7e-05 | Time: 1:51:17.579403 |\n",
      "| Epoch   1 |   135/  227 batches | Loss:   3.81 | Tokens:  1865 | Learning Rate: 9.8e-05 | Time: 1:52:07.255744 |\n",
      "| Epoch   1 |   136/  227 batches | Loss:   3.77 | Tokens:  1843 | Learning Rate: 9.8e-05 | Time: 1:53:00.665269 |\n",
      "| Epoch   1 |   137/  227 batches | Loss:   3.69 | Tokens:  1822 | Learning Rate: 9.8e-05 | Time: 1:53:54.177799 |\n",
      "| Epoch   1 |   138/  227 batches | Loss:   3.62 | Tokens:  1836 | Learning Rate: 9.8e-05 | Time: 1:54:46.524108 |\n",
      "| Epoch   1 |   139/  227 batches | Loss:   3.67 | Tokens:  1857 | Learning Rate: 9.9e-05 | Time: 1:55:38.899582 |\n",
      "| Epoch   1 |   140/  227 batches | Loss:   3.64 | Tokens:  1798 | Learning Rate: 9.9e-05 | Time: 1:56:30.725462 |\n",
      "| Epoch   1 |   141/  227 batches | Loss:   3.56 | Tokens:  1829 | Learning Rate: 9.9e-05 | Time: 1:57:21.152875 |\n",
      "| Epoch   1 |   142/  227 batches | Loss:   3.53 | Tokens:  1882 | Learning Rate: 1.0e-04 | Time: 1:58:13.144529 |\n",
      "| Epoch   1 |   143/  227 batches | Loss:   3.60 | Tokens:  1860 | Learning Rate: 1.0e-04 | Time: 1:59:04.610629 |\n",
      "| Epoch   1 |   144/  227 batches | Loss:   3.61 | Tokens:  1867 | Learning Rate: 1.0e-04 | Time: 1:59:57.533828 |\n",
      "| Epoch   1 |   145/  227 batches | Loss:   3.72 | Tokens:  1832 | Learning Rate: 1.0e-04 | Time: 2:00:49.832455 |\n",
      "| Epoch   1 |   146/  227 batches | Loss:   3.63 | Tokens:  1888 | Learning Rate: 1.0e-04 | Time: 2:01:40.922798 |\n",
      "| Epoch   1 |   147/  227 batches | Loss:   3.60 | Tokens:  1829 | Learning Rate: 1.0e-04 | Time: 2:02:28.000874 |\n",
      "| Epoch   1 |   148/  227 batches | Loss:   3.48 | Tokens:  1764 | Learning Rate: 1.0e-04 | Time: 2:03:22.095274 |\n",
      "| Epoch   1 |   149/  227 batches | Loss:   3.56 | Tokens:  1790 | Learning Rate: 1.0e-04 | Time: 2:04:08.703605 |\n",
      "| Epoch   1 |   150/  227 batches | Loss:   3.62 | Tokens:  1889 | Learning Rate: 1.0e-04 | Time: 2:04:58.654667 |\n",
      "| Epoch   1 |   151/  227 batches | Loss:   3.56 | Tokens:  1870 | Learning Rate: 1.0e-04 | Time: 2:05:44.706711 |\n",
      "| Epoch   1 |   152/  227 batches | Loss:   3.70 | Tokens:  1898 | Learning Rate: 1.0e-04 | Time: 2:06:34.854576 |\n",
      "| Epoch   1 |   153/  227 batches | Loss:   3.63 | Tokens:  1892 | Learning Rate: 1.0e-04 | Time: 2:07:25.036730 |\n",
      "| Epoch   1 |   154/  227 batches | Loss:   3.56 | Tokens:  1804 | Learning Rate: 1.0e-04 | Time: 2:08:19.455499 |\n",
      "| Epoch   1 |   155/  227 batches | Loss:   3.56 | Tokens:  1884 | Learning Rate: 1.0e-04 | Time: 2:09:12.526214 |\n",
      "| Epoch   1 |   156/  227 batches | Loss:   3.61 | Tokens:  1799 | Learning Rate: 1.0e-04 | Time: 2:10:03.558687 |\n",
      "| Epoch   1 |   157/  227 batches | Loss:   3.63 | Tokens:  1827 | Learning Rate: 1.0e-04 | Time: 2:10:55.517375 |\n",
      "| Epoch   1 |   158/  227 batches | Loss:   3.47 | Tokens:  1810 | Learning Rate: 1.0e-04 | Time: 2:11:47.871148 |\n",
      "| Epoch   1 |   159/  227 batches | Loss:   3.67 | Tokens:  1859 | Learning Rate: 1.0e-04 | Time: 2:12:38.812125 |\n",
      "| Epoch   1 |   160/  227 batches | Loss:   3.57 | Tokens:  1835 | Learning Rate: 1.0e-04 | Time: 2:13:28.964051 |\n",
      "| Epoch   1 |   161/  227 batches | Loss:   3.55 | Tokens:  1820 | Learning Rate: 1.0e-04 | Time: 2:14:21.908827 |\n",
      "| Epoch   1 |   162/  227 batches | Loss:   3.55 | Tokens:  1789 | Learning Rate: 1.0e-04 | Time: 2:15:12.220710 |\n",
      "| Epoch   1 |   163/  227 batches | Loss:   3.64 | Tokens:  1895 | Learning Rate: 1.1e-04 | Time: 2:16:03.202474 |\n",
      "| Epoch   1 |   164/  227 batches | Loss:   3.60 | Tokens:  1853 | Learning Rate: 1.1e-04 | Time: 2:16:53.227972 |\n",
      "| Epoch   1 |   165/  227 batches | Loss:   3.49 | Tokens:  1799 | Learning Rate: 1.1e-04 | Time: 2:17:45.867337 |\n",
      "| Epoch   1 |   166/  227 batches | Loss:   3.51 | Tokens:  1819 | Learning Rate: 1.1e-04 | Time: 2:18:36.564572 |\n",
      "| Epoch   1 |   167/  227 batches | Loss:   3.43 | Tokens:  1806 | Learning Rate: 1.1e-04 | Time: 2:19:29.492145 |\n",
      "| Epoch   1 |   168/  227 batches | Loss:   3.62 | Tokens:  1901 | Learning Rate: 1.1e-04 | Time: 2:20:21.308174 |\n",
      "| Epoch   1 |   169/  227 batches | Loss:   3.39 | Tokens:  1805 | Learning Rate: 1.1e-04 | Time: 2:21:15.499119 |\n",
      "| Epoch   1 |   170/  227 batches | Loss:   3.42 | Tokens:  1851 | Learning Rate: 1.1e-04 | Time: 2:22:06.687016 |\n",
      "| Epoch   1 |   171/  227 batches | Loss:   3.57 | Tokens:  1858 | Learning Rate: 1.1e-04 | Time: 2:22:59.281021 |\n",
      "| Epoch   1 |   172/  227 batches | Loss:   3.58 | Tokens:  1884 | Learning Rate: 1.1e-04 | Time: 2:23:54.424692 |\n",
      "| Epoch   1 |   173/  227 batches | Loss:   3.53 | Tokens:  1852 | Learning Rate: 1.1e-04 | Time: 2:24:44.765867 |\n",
      "| Epoch   1 |   174/  227 batches | Loss:   3.51 | Tokens:  1837 | Learning Rate: 1.1e-04 | Time: 2:25:35.587498 |\n",
      "| Epoch   1 |   175/  227 batches | Loss:   3.46 | Tokens:  1800 | Learning Rate: 1.1e-04 | Time: 2:26:26.330162 |\n",
      "| Epoch   1 |   176/  227 batches | Loss:   3.36 | Tokens:  1810 | Learning Rate: 1.1e-04 | Time: 2:27:18.945476 |\n",
      "| Epoch   1 |   177/  227 batches | Loss:   3.48 | Tokens:  1848 | Learning Rate: 1.1e-04 | Time: 2:28:10.825256 |\n",
      "| Epoch   1 |   178/  227 batches | Loss:   3.51 | Tokens:  1800 | Learning Rate: 1.1e-04 | Time: 2:29:01.802712 |\n",
      "| Epoch   1 |   179/  227 batches | Loss:   3.34 | Tokens:  1850 | Learning Rate: 1.1e-04 | Time: 2:29:51.282071 |\n",
      "| Epoch   1 |   180/  227 batches | Loss:   3.40 | Tokens:  1816 | Learning Rate: 1.1e-04 | Time: 2:30:40.952931 |\n",
      "| Epoch   1 |   181/  227 batches | Loss:   3.41 | Tokens:  1889 | Learning Rate: 1.1e-04 | Time: 2:31:31.121623 |\n",
      "| Epoch   1 |   182/  227 batches | Loss:   3.54 | Tokens:  1778 | Learning Rate: 1.1e-04 | Time: 2:32:24.243016 |\n",
      "| Epoch   1 |   183/  227 batches | Loss:   3.37 | Tokens:  1814 | Learning Rate: 1.1e-04 | Time: 2:33:13.385533 |\n",
      "| Epoch   1 |   184/  227 batches | Loss:   3.62 | Tokens:  1888 | Learning Rate: 1.1e-04 | Time: 2:34:06.545062 |\n",
      "| Epoch   1 |   185/  227 batches | Loss:   3.67 | Tokens:  1855 | Learning Rate: 1.1e-04 | Time: 2:34:57.539305 |\n",
      "| Epoch   1 |   186/  227 batches | Loss:   3.52 | Tokens:  1862 | Learning Rate: 1.1e-04 | Time: 2:35:46.973081 |\n",
      "| Epoch   1 |   187/  227 batches | Loss:   3.56 | Tokens:  1846 | Learning Rate: 1.1e-04 | Time: 2:36:37.752256 |\n",
      "| Epoch   1 |   188/  227 batches | Loss:   3.64 | Tokens:  1900 | Learning Rate: 1.1e-04 | Time: 2:37:26.970604 |\n",
      "| Epoch   1 |   189/  227 batches | Loss:   3.61 | Tokens:  1854 | Learning Rate: 1.1e-04 | Time: 2:38:13.723710 |\n",
      "| Epoch   1 |   190/  227 batches | Loss:   3.58 | Tokens:  1919 | Learning Rate: 1.1e-04 | Time: 2:39:01.028180 |\n",
      "| Epoch   1 |   191/  227 batches | Loss:   3.60 | Tokens:  1894 | Learning Rate: 1.1e-04 | Time: 2:39:49.939054 |\n",
      "| Epoch   1 |   192/  227 batches | Loss:   3.69 | Tokens:  2015 | Learning Rate: 1.1e-04 | Time: 2:40:37.842210 |\n",
      "| Epoch   1 |   193/  227 batches | Loss:   3.61 | Tokens:  1861 | Learning Rate: 1.1e-04 | Time: 2:41:26.886028 |\n",
      "| Epoch   1 |   194/  227 batches | Loss:   3.59 | Tokens:  1987 | Learning Rate: 1.1e-04 | Time: 2:42:12.237153 |\n",
      "| Epoch   1 |   195/  227 batches | Loss:   3.65 | Tokens:  1965 | Learning Rate: 1.1e-04 | Time: 2:43:00.783302 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   1 |   196/  227 batches | Loss:   3.65 | Tokens:  2014 | Learning Rate: 1.1e-04 | Time: 2:43:46.575538 |\n",
      "| Epoch   1 |   197/  227 batches | Loss:   3.61 | Tokens:  1950 | Learning Rate: 1.1e-04 | Time: 2:44:34.883324 |\n",
      "| Epoch   1 |   198/  227 batches | Loss:   3.73 | Tokens:  1901 | Learning Rate: 1.1e-04 | Time: 2:45:24.653200 |\n",
      "| Epoch   1 |   199/  227 batches | Loss:   3.70 | Tokens:  2041 | Learning Rate: 1.1e-04 | Time: 2:46:08.933757 |\n",
      "| Epoch   1 |   200/  227 batches | Loss:   3.73 | Tokens:  1937 | Learning Rate: 1.2e-04 | Time: 2:46:57.235560 |\n",
      "| Epoch   1 |   201/  227 batches | Loss:   3.83 | Tokens:  1934 | Learning Rate: 1.2e-04 | Time: 2:47:44.926133 |\n",
      "| Epoch   1 |   202/  227 batches | Loss:   3.75 | Tokens:  1939 | Learning Rate: 1.2e-04 | Time: 2:48:32.102944 |\n",
      "| Epoch   1 |   203/  227 batches | Loss:   3.92 | Tokens:  2015 | Learning Rate: 1.2e-04 | Time: 2:49:21.270428 |\n",
      "| Epoch   1 |   204/  227 batches | Loss:   3.62 | Tokens:  1894 | Learning Rate: 1.2e-04 | Time: 2:50:06.418665 |\n",
      "| Epoch   1 |   205/  227 batches | Loss:   3.70 | Tokens:  1985 | Learning Rate: 1.2e-04 | Time: 2:50:54.954842 |\n",
      "| Epoch   1 |   206/  227 batches | Loss:   3.67 | Tokens:  1928 | Learning Rate: 1.2e-04 | Time: 2:51:42.899599 |\n",
      "| Epoch   1 |   207/  227 batches | Loss:   3.75 | Tokens:  1855 | Learning Rate: 1.2e-04 | Time: 2:52:29.607663 |\n",
      "| Epoch   1 |   208/  227 batches | Loss:   3.73 | Tokens:  1938 | Learning Rate: 1.2e-04 | Time: 2:53:15.935743 |\n",
      "| Epoch   1 |   209/  227 batches | Loss:   3.78 | Tokens:  2011 | Learning Rate: 1.2e-04 | Time: 2:54:05.357661 |\n",
      "| Epoch   1 |   210/  227 batches | Loss:   3.72 | Tokens:  1921 | Learning Rate: 1.2e-04 | Time: 2:54:54.366823 |\n",
      "| Epoch   1 |   211/  227 batches | Loss:   3.86 | Tokens:  1959 | Learning Rate: 1.2e-04 | Time: 2:55:44.847583 |\n",
      "| Epoch   1 |   212/  227 batches | Loss:   3.70 | Tokens:  1962 | Learning Rate: 1.2e-04 | Time: 2:56:32.718538 |\n",
      "| Epoch   1 |   213/  227 batches | Loss:   3.77 | Tokens:  1922 | Learning Rate: 1.2e-04 | Time: 2:57:21.632923 |\n",
      "| Epoch   1 |   214/  227 batches | Loss:   3.60 | Tokens:  1906 | Learning Rate: 1.2e-04 | Time: 2:58:09.326266 |\n",
      "| Epoch   1 |   215/  227 batches | Loss:   3.58 | Tokens:  1911 | Learning Rate: 1.2e-04 | Time: 2:58:58.013039 |\n",
      "| Epoch   1 |   216/  227 batches | Loss:   3.51 | Tokens:  1840 | Learning Rate: 1.2e-04 | Time: 2:59:45.064186 |\n",
      "| Epoch   1 |   217/  227 batches | Loss:   3.75 | Tokens:  1967 | Learning Rate: 1.2e-04 | Time: 3:00:31.833088 |\n",
      "| Epoch   1 |   218/  227 batches | Loss:   3.73 | Tokens:  1920 | Learning Rate: 1.2e-04 | Time: 3:01:15.404938 |\n",
      "| Epoch   1 |   219/  227 batches | Loss:   3.70 | Tokens:  1933 | Learning Rate: 1.2e-04 | Time: 3:02:03.331743 |\n",
      "| Epoch   1 |   220/  227 batches | Loss:   3.81 | Tokens:  2004 | Learning Rate: 1.2e-04 | Time: 3:02:51.337337 |\n",
      "| Epoch   1 |   221/  227 batches | Loss:   3.57 | Tokens:  1910 | Learning Rate: 1.2e-04 | Time: 3:03:44.445283 |\n",
      "| Epoch   1 |   222/  227 batches | Loss:   3.54 | Tokens:  1899 | Learning Rate: 1.2e-04 | Time: 3:04:31.054613 |\n",
      "| Epoch   1 |   223/  227 batches | Loss:   3.45 | Tokens:  1864 | Learning Rate: 1.2e-04 | Time: 3:05:15.298271 |\n",
      "| Epoch   1 |   224/  227 batches | Loss:   3.43 | Tokens:  1752 | Learning Rate: 1.2e-04 | Time: 3:06:04.925526 |\n",
      "| Epoch   1 |   225/  227 batches | Loss:   3.30 | Tokens:  1765 | Learning Rate: 1.2e-04 | Time: 3:06:54.671466 |\n",
      "| Epoch   1 |   226/  227 batches | Loss:   3.48 | Tokens:   981 | Learning Rate: 1.2e-04 | Time: 3:07:16.731460 |\n",
      "| Epoch   1 |     0/    8 batches | Loss:   3.38 | Tokens:  1835 | Time: 0:00:14.176082 |\n",
      "| Epoch   1 |     1/    8 batches | Loss:   3.23 | Tokens:  1756 | Time: 0:00:27.280031 |\n",
      "| Epoch   1 |     2/    8 batches | Loss:   3.30 | Tokens:  1796 | Time: 0:00:40.623340 |\n",
      "| Epoch   1 |     3/    8 batches | Loss:   3.21 | Tokens:  1690 | Time: 0:00:54.213989 |\n",
      "| Epoch   1 |     4/    8 batches | Loss:   3.42 | Tokens:  1840 | Time: 0:01:08.095857 |\n",
      "| Epoch   1 |     5/    8 batches | Loss:   3.25 | Tokens:  1882 | Time: 0:01:21.637635 |\n",
      "| Epoch   1 |     6/    8 batches | Loss:   3.32 | Tokens:  1877 | Time: 0:01:34.563062 |\n",
      "| Epoch   1 |     7/    8 batches | Loss:   3.52 | Tokens:  1765 | Time: 0:01:46.840223 |\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   1 | Train Loss:    3.803 | Val Loss:    3.330 | time: 3:09:05.226259 |\n",
      "-----------------------------------------------------------\n",
      "| Epoch   2 |     0/  227 batches | Loss:   3.28 | Tokens:  1792 | Learning Rate: 1.2e-04 | Time: 0:00:47.721354 |\n",
      "| Epoch   2 |     1/  227 batches | Loss:   3.38 | Tokens:  1795 | Learning Rate: 1.2e-04 | Time: 0:01:36.595626 |\n",
      "| Epoch   2 |     2/  227 batches | Loss:   3.31 | Tokens:  1791 | Learning Rate: 1.2e-04 | Time: 0:02:23.767449 |\n",
      "| Epoch   2 |     3/  227 batches | Loss:   3.44 | Tokens:  1813 | Learning Rate: 1.2e-04 | Time: 0:03:10.367803 |\n",
      "| Epoch   2 |     4/  227 batches | Loss:   3.43 | Tokens:  1807 | Learning Rate: 1.2e-04 | Time: 0:04:01.453147 |\n",
      "| Epoch   2 |     5/  227 batches | Loss:   3.27 | Tokens:  1824 | Learning Rate: 1.2e-04 | Time: 0:04:51.142944 |\n",
      "| Epoch   2 |     6/  227 batches | Loss:   3.42 | Tokens:  1794 | Learning Rate: 1.2e-04 | Time: 0:05:38.396547 |\n",
      "| Epoch   2 |     7/  227 batches | Loss:   3.24 | Tokens:  1796 | Learning Rate: 1.2e-04 | Time: 0:06:26.278473 |\n",
      "| Epoch   2 |     8/  227 batches | Loss:   3.19 | Tokens:  1748 | Learning Rate: 1.2e-04 | Time: 0:07:13.762463 |\n",
      "| Epoch   2 |     9/  227 batches | Loss:   3.22 | Tokens:  1834 | Learning Rate: 1.2e-04 | Time: 0:08:02.967849 |\n",
      "| Epoch   2 |    10/  227 batches | Loss:   3.43 | Tokens:  1797 | Learning Rate: 1.3e-04 | Time: 0:08:51.060210 |\n",
      "| Epoch   2 |    11/  227 batches | Loss:   3.31 | Tokens:  1745 | Learning Rate: 1.3e-04 | Time: 0:09:41.717712 |\n",
      "| Epoch   2 |    12/  227 batches | Loss:   3.37 | Tokens:  1776 | Learning Rate: 1.3e-04 | Time: 0:10:32.127876 |\n",
      "| Epoch   2 |    13/  227 batches | Loss:   3.24 | Tokens:  1722 | Learning Rate: 1.3e-04 | Time: 0:11:19.043383 |\n",
      "| Epoch   2 |    14/  227 batches | Loss:   3.29 | Tokens:  1782 | Learning Rate: 1.3e-04 | Time: 0:12:06.106509 |\n",
      "| Epoch   2 |    15/  227 batches | Loss:   3.35 | Tokens:  1825 | Learning Rate: 1.3e-04 | Time: 0:12:54.288630 |\n",
      "| Epoch   2 |    16/  227 batches | Loss:   3.20 | Tokens:  1788 | Learning Rate: 1.3e-04 | Time: 0:23:38.653117 |\n",
      "| Epoch   2 |    17/  227 batches | Loss:   3.28 | Tokens:  1793 | Learning Rate: 1.3e-04 | Time: 0:24:34.659052 |\n",
      "| Epoch   2 |    18/  227 batches | Loss:   3.39 | Tokens:  1935 | Learning Rate: 1.3e-04 | Time: 0:25:26.221703 |\n",
      "| Epoch   2 |    19/  227 batches | Loss:   3.32 | Tokens:  1785 | Learning Rate: 1.3e-04 | Time: 0:26:16.352611 |\n",
      "| Epoch   2 |    20/  227 batches | Loss:   3.29 | Tokens:  1748 | Learning Rate: 1.3e-04 | Time: 0:27:03.377827 |\n",
      "| Epoch   2 |    21/  227 batches | Loss:   3.28 | Tokens:  1762 | Learning Rate: 1.3e-04 | Time: 0:27:52.818583 |\n",
      "| Epoch   2 |    22/  227 batches | Loss:   3.29 | Tokens:  1734 | Learning Rate: 1.3e-04 | Time: 0:28:36.451873 |\n",
      "| Epoch   2 |    23/  227 batches | Loss:   3.25 | Tokens:  1732 | Learning Rate: 1.3e-04 | Time: 0:29:24.785590 |\n",
      "| Epoch   2 |    24/  227 batches | Loss:   3.27 | Tokens:  1739 | Learning Rate: 1.3e-04 | Time: 0:30:11.634279 |\n",
      "| Epoch   2 |    25/  227 batches | Loss:   3.16 | Tokens:  1793 | Learning Rate: 1.3e-04 | Time: 0:30:55.973679 |\n",
      "| Epoch   2 |    26/  227 batches | Loss:   3.29 | Tokens:  1839 | Learning Rate: 1.3e-04 | Time: 0:31:41.243593 |\n",
      "| Epoch   2 |    27/  227 batches | Loss:   3.28 | Tokens:  1790 | Learning Rate: 1.3e-04 | Time: 0:32:30.175707 |\n",
      "| Epoch   2 |    28/  227 batches | Loss:   3.11 | Tokens:  1727 | Learning Rate: 1.3e-04 | Time: 0:33:15.825603 |\n",
      "| Epoch   2 |    29/  227 batches | Loss:   3.23 | Tokens:  1773 | Learning Rate: 1.3e-04 | Time: 0:34:03.751412 |\n",
      "| Epoch   2 |    30/  227 batches | Loss:   3.32 | Tokens:  1803 | Learning Rate: 1.3e-04 | Time: 0:34:51.614386 |\n",
      "| Epoch   2 |    31/  227 batches | Loss:   3.25 | Tokens:  1729 | Learning Rate: 1.3e-04 | Time: 0:35:35.183552 |\n",
      "| Epoch   2 |    32/  227 batches | Loss:   3.24 | Tokens:  1722 | Learning Rate: 1.3e-04 | Time: 0:36:20.748673 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   2 |    33/  227 batches | Loss:   3.16 | Tokens:  1670 | Learning Rate: 1.3e-04 | Time: 0:37:05.142927 |\n",
      "| Epoch   2 |    34/  227 batches | Loss:   3.14 | Tokens:  1771 | Learning Rate: 1.3e-04 | Time: 0:37:50.819362 |\n",
      "| Epoch   2 |    35/  227 batches | Loss:   3.28 | Tokens:  1737 | Learning Rate: 1.3e-04 | Time: 0:38:35.562682 |\n",
      "| Epoch   2 |    36/  227 batches | Loss:   3.06 | Tokens:  1693 | Learning Rate: 1.3e-04 | Time: 0:39:23.465552 |\n",
      "| Epoch   2 |    37/  227 batches | Loss:   3.22 | Tokens:  1748 | Learning Rate: 1.3e-04 | Time: 0:40:10.093829 |\n",
      "| Epoch   2 |    38/  227 batches | Loss:   3.12 | Tokens:  1842 | Learning Rate: 1.3e-04 | Time: 0:40:55.311882 |\n",
      "| Epoch   2 |    39/  227 batches | Loss:   3.22 | Tokens:  1784 | Learning Rate: 1.3e-04 | Time: 0:41:40.684777 |\n",
      "| Epoch   2 |    40/  227 batches | Loss:   3.37 | Tokens:  1747 | Learning Rate: 1.3e-04 | Time: 0:42:25.861506 |\n",
      "| Epoch   2 |    41/  227 batches | Loss:   3.14 | Tokens:  1806 | Learning Rate: 1.3e-04 | Time: 0:43:13.595824 |\n",
      "| Epoch   2 |    42/  227 batches | Loss:   3.20 | Tokens:  1811 | Learning Rate: 1.3e-04 | Time: 0:43:59.914830 |\n",
      "| Epoch   2 |    43/  227 batches | Loss:   3.20 | Tokens:  1823 | Learning Rate: 1.3e-04 | Time: 0:44:44.965329 |\n",
      "| Epoch   2 |    44/  227 batches | Loss:   3.21 | Tokens:  1729 | Learning Rate: 1.3e-04 | Time: 0:45:30.147476 |\n",
      "| Epoch   2 |    45/  227 batches | Loss:   3.01 | Tokens:  1668 | Learning Rate: 1.3e-04 | Time: 0:46:15.889338 |\n",
      "| Epoch   2 |    46/  227 batches | Loss:   3.19 | Tokens:  1776 | Learning Rate: 1.3e-04 | Time: 0:47:01.400639 |\n",
      "| Epoch   2 |    47/  227 batches | Loss:   3.32 | Tokens:  1692 | Learning Rate: 1.4e-04 | Time: 0:47:49.445129 |\n",
      "| Epoch   2 |    48/  227 batches | Loss:   3.07 | Tokens:  1746 | Learning Rate: 1.4e-04 | Time: 0:48:34.215343 |\n",
      "| Epoch   2 |    49/  227 batches | Loss:   3.13 | Tokens:  1684 | Learning Rate: 1.4e-04 | Time: 0:49:18.399171 |\n",
      "| Epoch   2 |    50/  227 batches | Loss:   3.15 | Tokens:  1793 | Learning Rate: 1.4e-04 | Time: 0:50:03.407783 |\n",
      "| Epoch   2 |    51/  227 batches | Loss:   3.06 | Tokens:  1800 | Learning Rate: 1.4e-04 | Time: 0:50:48.976336 |\n",
      "| Epoch   2 |    52/  227 batches | Loss:   3.30 | Tokens:  1794 | Learning Rate: 1.4e-04 | Time: 0:51:33.487276 |\n",
      "| Epoch   2 |    53/  227 batches | Loss:   3.14 | Tokens:  1767 | Learning Rate: 1.4e-04 | Time: 0:52:21.548722 |\n",
      "| Epoch   2 |    54/  227 batches | Loss:   3.22 | Tokens:  1792 | Learning Rate: 1.4e-04 | Time: 0:53:12.404693 |\n",
      "| Epoch   2 |    55/  227 batches | Loss:   3.16 | Tokens:  1734 | Learning Rate: 1.4e-04 | Time: 0:53:57.720481 |\n",
      "| Epoch   2 |    56/  227 batches | Loss:   3.08 | Tokens:  1780 | Learning Rate: 1.4e-04 | Time: 0:54:43.574829 |\n",
      "| Epoch   2 |    57/  227 batches | Loss:   3.15 | Tokens:  1833 | Learning Rate: 1.4e-04 | Time: 0:55:29.205388 |\n",
      "| Epoch   2 |    58/  227 batches | Loss:   3.08 | Tokens:  1793 | Learning Rate: 1.4e-04 | Time: 0:56:14.522175 |\n",
      "| Epoch   2 |    59/  227 batches | Loss:   3.10 | Tokens:  1720 | Learning Rate: 1.4e-04 | Time: 0:57:02.302372 |\n",
      "| Epoch   2 |    60/  227 batches | Loss:   3.16 | Tokens:  1798 | Learning Rate: 1.4e-04 | Time: 0:57:49.028390 |\n",
      "| Epoch   2 |    61/  227 batches | Loss:   3.06 | Tokens:  1709 | Learning Rate: 1.4e-04 | Time: 0:58:34.493215 |\n",
      "| Epoch   2 |    62/  227 batches | Loss:   3.19 | Tokens:  1852 | Learning Rate: 1.4e-04 | Time: 0:59:18.468241 |\n",
      "| Epoch   2 |    63/  227 batches | Loss:   3.16 | Tokens:  1783 | Learning Rate: 1.4e-04 | Time: 1:00:06.959536 |\n",
      "| Epoch   2 |    64/  227 batches | Loss:   3.13 | Tokens:  1821 | Learning Rate: 1.4e-04 | Time: 1:00:50.349475 |\n",
      "| Epoch   2 |    65/  227 batches | Loss:   2.98 | Tokens:  1746 | Learning Rate: 1.4e-04 | Time: 1:01:35.070947 |\n",
      "| Epoch   2 |    66/  227 batches | Loss:   3.05 | Tokens:  1725 | Learning Rate: 1.4e-04 | Time: 1:02:20.218750 |\n",
      "| Epoch   2 |    67/  227 batches | Loss:   3.10 | Tokens:  1720 | Learning Rate: 1.4e-04 | Time: 1:03:06.817109 |\n",
      "| Epoch   2 |    68/  227 batches | Loss:   3.17 | Tokens:  1700 | Learning Rate: 1.4e-04 | Time: 1:03:52.799116 |\n",
      "| Epoch   2 |    69/  227 batches | Loss:   3.17 | Tokens:  1758 | Learning Rate: 1.4e-04 | Time: 1:04:37.741117 |\n",
      "| Epoch   2 |    70/  227 batches | Loss:   3.22 | Tokens:  1781 | Learning Rate: 1.4e-04 | Time: 1:05:20.539682 |\n",
      "| Epoch   2 |    71/  227 batches | Loss:   3.23 | Tokens:  1755 | Learning Rate: 1.4e-04 | Time: 1:06:06.304271 |\n",
      "| Epoch   2 |    72/  227 batches | Loss:   3.17 | Tokens:  1710 | Learning Rate: 1.4e-04 | Time: 1:06:51.803569 |\n",
      "| Epoch   2 |    73/  227 batches | Loss:   3.22 | Tokens:  1669 | Learning Rate: 1.4e-04 | Time: 1:07:37.120730 |\n",
      "| Epoch   2 |    74/  227 batches | Loss:   3.19 | Tokens:  1764 | Learning Rate: 1.4e-04 | Time: 1:08:22.426546 |\n",
      "| Epoch   2 |    75/  227 batches | Loss:   2.99 | Tokens:  1747 | Learning Rate: 1.4e-04 | Time: 1:09:09.263973 |\n",
      "| Epoch   2 |    76/  227 batches | Loss:   3.12 | Tokens:  1781 | Learning Rate: 1.4e-04 | Time: 1:09:53.010959 |\n",
      "| Epoch   2 |    77/  227 batches | Loss:   2.96 | Tokens:  1721 | Learning Rate: 1.4e-04 | Time: 1:10:36.267257 |\n",
      "| Epoch   2 |    78/  227 batches | Loss:   3.16 | Tokens:  1723 | Learning Rate: 1.4e-04 | Time: 1:11:20.440102 |\n",
      "| Epoch   2 |    79/  227 batches | Loss:   3.19 | Tokens:  1710 | Learning Rate: 1.4e-04 | Time: 1:12:05.836676 |\n",
      "| Epoch   2 |    80/  227 batches | Loss:   3.12 | Tokens:  1721 | Learning Rate: 1.4e-04 | Time: 1:12:50.573472 |\n",
      "| Epoch   2 |    81/  227 batches | Loss:   2.99 | Tokens:  1737 | Learning Rate: 1.4e-04 | Time: 1:13:36.629283 |\n",
      "| Epoch   2 |    82/  227 batches | Loss:   3.01 | Tokens:  1690 | Learning Rate: 1.4e-04 | Time: 1:14:22.544468 |\n",
      "| Epoch   2 |    83/  227 batches | Loss:   3.10 | Tokens:  1668 | Learning Rate: 1.4e-04 | Time: 1:15:06.520189 |\n",
      "| Epoch   2 |    84/  227 batches | Loss:   3.18 | Tokens:  1690 | Learning Rate: 1.4e-04 | Time: 1:15:49.954012 |\n",
      "| Epoch   2 |    85/  227 batches | Loss:   3.03 | Tokens:  1712 | Learning Rate: 1.5e-04 | Time: 1:16:39.524274 |\n",
      "| Epoch   2 |    86/  227 batches | Loss:   3.08 | Tokens:  1714 | Learning Rate: 1.5e-04 | Time: 1:17:25.258942 |\n",
      "| Epoch   2 |    87/  227 batches | Loss:   3.00 | Tokens:  1705 | Learning Rate: 1.5e-04 | Time: 1:18:11.145205 |\n",
      "| Epoch   2 |    88/  227 batches | Loss:   3.11 | Tokens:  1684 | Learning Rate: 1.5e-04 | Time: 1:18:57.756532 |\n",
      "| Epoch   2 |    89/  227 batches | Loss:   2.93 | Tokens:  1671 | Learning Rate: 1.5e-04 | Time: 1:19:42.758208 |\n",
      "| Epoch   2 |    90/  227 batches | Loss:   3.23 | Tokens:  1713 | Learning Rate: 1.5e-04 | Time: 1:20:27.689028 |\n",
      "| Epoch   2 |    91/  227 batches | Loss:   3.05 | Tokens:  1691 | Learning Rate: 1.5e-04 | Time: 1:21:12.179304 |\n",
      "| Epoch   2 |    92/  227 batches | Loss:   3.21 | Tokens:  1741 | Learning Rate: 1.5e-04 | Time: 1:21:57.420294 |\n",
      "| Epoch   2 |    93/  227 batches | Loss:   2.96 | Tokens:  1663 | Learning Rate: 1.5e-04 | Time: 1:22:47.140337 |\n",
      "| Epoch   2 |    94/  227 batches | Loss:   2.98 | Tokens:  1686 | Learning Rate: 1.5e-04 | Time: 1:23:33.615027 |\n",
      "| Epoch   2 |    95/  227 batches | Loss:   3.04 | Tokens:  1729 | Learning Rate: 1.5e-04 | Time: 1:24:21.824077 |\n",
      "| Epoch   2 |    96/  227 batches | Loss:   3.00 | Tokens:  1765 | Learning Rate: 1.5e-04 | Time: 1:25:06.657158 |\n",
      "| Epoch   2 |    97/  227 batches | Loss:   3.05 | Tokens:  1682 | Learning Rate: 1.5e-04 | Time: 1:25:52.315361 |\n",
      "| Epoch   2 |    98/  227 batches | Loss:   2.94 | Tokens:  1728 | Learning Rate: 1.5e-04 | Time: 1:26:36.544058 |\n",
      "| Epoch   2 |    99/  227 batches | Loss:   3.06 | Tokens:  1701 | Learning Rate: 1.5e-04 | Time: 1:27:23.870468 |\n",
      "| Epoch   2 |   100/  227 batches | Loss:   3.08 | Tokens:  1694 | Learning Rate: 1.5e-04 | Time: 1:28:09.375751 |\n",
      "| Epoch   2 |   101/  227 batches | Loss:   2.98 | Tokens:  1716 | Learning Rate: 1.5e-04 | Time: 1:28:56.638730 |\n",
      "| Epoch   2 |   102/  227 batches | Loss:   2.95 | Tokens:  1758 | Learning Rate: 1.5e-04 | Time: 1:29:43.892337 |\n",
      "| Epoch   2 |   103/  227 batches | Loss:   3.17 | Tokens:  1728 | Learning Rate: 1.5e-04 | Time: 1:30:26.335809 |\n",
      "| Epoch   2 |   104/  227 batches | Loss:   3.01 | Tokens:  1717 | Learning Rate: 1.5e-04 | Time: 1:31:12.467415 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   2 |   105/  227 batches | Loss:   3.06 | Tokens:  1740 | Learning Rate: 1.5e-04 | Time: 1:31:57.850025 |\n",
      "| Epoch   2 |   106/  227 batches | Loss:   2.88 | Tokens:  1695 | Learning Rate: 1.5e-04 | Time: 1:32:43.753243 |\n",
      "| Epoch   2 |   107/  227 batches | Loss:   3.00 | Tokens:  1766 | Learning Rate: 1.5e-04 | Time: 1:33:31.979247 |\n",
      "| Epoch   2 |   108/  227 batches | Loss:   3.08 | Tokens:  1750 | Learning Rate: 1.5e-04 | Time: 1:34:19.004465 |\n",
      "| Epoch   2 |   109/  227 batches | Loss:   2.98 | Tokens:  1725 | Learning Rate: 1.5e-04 | Time: 1:35:03.579237 |\n",
      "| Epoch   2 |   110/  227 batches | Loss:   2.98 | Tokens:  1700 | Learning Rate: 1.5e-04 | Time: 1:35:48.570527 |\n",
      "| Epoch   2 |   111/  227 batches | Loss:   2.93 | Tokens:  1678 | Learning Rate: 1.5e-04 | Time: 1:36:33.997020 |\n",
      "| Epoch   2 |   112/  227 batches | Loss:   3.03 | Tokens:  1741 | Learning Rate: 1.5e-04 | Time: 1:37:19.456103 |\n",
      "| Epoch   2 |   113/  227 batches | Loss:   2.92 | Tokens:  1659 | Learning Rate: 1.5e-04 | Time: 1:38:07.140558 |\n",
      "| Epoch   2 |   114/  227 batches | Loss:   3.13 | Tokens:  1703 | Learning Rate: 1.5e-04 | Time: 1:38:51.524838 |\n",
      "| Epoch   2 |   115/  227 batches | Loss:   3.00 | Tokens:  1812 | Learning Rate: 1.5e-04 | Time: 1:39:35.820326 |\n",
      "| Epoch   2 |   116/  227 batches | Loss:   2.94 | Tokens:  1765 | Learning Rate: 1.5e-04 | Time: 1:40:20.656631 |\n",
      "| Epoch   2 |   117/  227 batches | Loss:   3.07 | Tokens:  1705 | Learning Rate: 1.5e-04 | Time: 1:41:04.450490 |\n",
      "| Epoch   2 |   118/  227 batches | Loss:   3.10 | Tokens:  1787 | Learning Rate: 1.5e-04 | Time: 1:41:49.981415 |\n",
      "| Epoch   2 |   119/  227 batches | Loss:   3.07 | Tokens:  1861 | Learning Rate: 1.5e-04 | Time: 1:42:35.538314 |\n",
      "| Epoch   2 |   120/  227 batches | Loss:   3.18 | Tokens:  1808 | Learning Rate: 1.5e-04 | Time: 1:43:23.013565 |\n",
      "| Epoch   2 |   121/  227 batches | Loss:   3.16 | Tokens:  1817 | Learning Rate: 1.5e-04 | Time: 1:44:09.602949 |\n",
      "| Epoch   2 |   122/  227 batches | Loss:   3.11 | Tokens:  1773 | Learning Rate: 1.6e-04 | Time: 1:44:55.695660 |\n",
      "| Epoch   2 |   123/  227 batches | Loss:   3.03 | Tokens:  1815 | Learning Rate: 1.6e-04 | Time: 1:45:42.569710 |\n",
      "| Epoch   2 |   124/  227 batches | Loss:   3.15 | Tokens:  1835 | Learning Rate: 1.6e-04 | Time: 1:46:25.393741 |\n",
      "| Epoch   2 |   125/  227 batches | Loss:   3.16 | Tokens:  1915 | Learning Rate: 1.6e-04 | Time: 1:47:11.077548 |\n",
      "| Epoch   2 |   126/  227 batches | Loss:   3.02 | Tokens:  1790 | Learning Rate: 1.6e-04 | Time: 1:47:56.247305 |\n",
      "| Epoch   2 |   127/  227 batches | Loss:   3.19 | Tokens:  1831 | Learning Rate: 1.6e-04 | Time: 1:48:41.517550 |\n",
      "| Epoch   2 |   128/  227 batches | Loss:   3.13 | Tokens:  1776 | Learning Rate: 1.6e-04 | Time: 1:49:26.957515 |\n",
      "| Epoch   2 |   129/  227 batches | Loss:   3.24 | Tokens:  1890 | Learning Rate: 1.6e-04 | Time: 1:50:14.151281 |\n",
      "| Epoch   2 |   130/  227 batches | Loss:   3.00 | Tokens:  1800 | Learning Rate: 1.6e-04 | Time: 1:50:57.366223 |\n",
      "| Epoch   2 |   131/  227 batches | Loss:   3.17 | Tokens:  1787 | Learning Rate: 1.6e-04 | Time: 1:51:42.356881 |\n",
      "| Epoch   2 |   132/  227 batches | Loss:   2.97 | Tokens:  1809 | Learning Rate: 1.6e-04 | Time: 1:52:28.787689 |\n",
      "| Epoch   2 |   133/  227 batches | Loss:   3.11 | Tokens:  1821 | Learning Rate: 1.6e-04 | Time: 1:53:15.646351 |\n",
      "| Epoch   2 |   134/  227 batches | Loss:   3.15 | Tokens:  1887 | Learning Rate: 1.6e-04 | Time: 1:54:02.560864 |\n",
      "| Epoch   2 |   135/  227 batches | Loss:   3.18 | Tokens:  1865 | Learning Rate: 1.6e-04 | Time: 1:54:49.569125 |\n",
      "| Epoch   2 |   136/  227 batches | Loss:   3.14 | Tokens:  1843 | Learning Rate: 1.6e-04 | Time: 1:55:35.344719 |\n",
      "| Epoch   2 |   137/  227 batches | Loss:   3.06 | Tokens:  1822 | Learning Rate: 1.6e-04 | Time: 1:56:22.878574 |\n",
      "| Epoch   2 |   138/  227 batches | Loss:   3.02 | Tokens:  1836 | Learning Rate: 1.6e-04 | Time: 1:57:07.547893 |\n",
      "| Epoch   2 |   139/  227 batches | Loss:   3.04 | Tokens:  1857 | Learning Rate: 1.6e-04 | Time: 1:57:52.158569 |\n",
      "| Epoch   2 |   140/  227 batches | Loss:   2.99 | Tokens:  1798 | Learning Rate: 1.6e-04 | Time: 1:58:39.298478 |\n",
      "| Epoch   2 |   141/  227 batches | Loss:   2.91 | Tokens:  1829 | Learning Rate: 1.6e-04 | Time: 1:59:25.425097 |\n",
      "| Epoch   2 |   142/  227 batches | Loss:   2.89 | Tokens:  1882 | Learning Rate: 1.6e-04 | Time: 2:00:14.676360 |\n",
      "| Epoch   2 |   143/  227 batches | Loss:   2.98 | Tokens:  1860 | Learning Rate: 1.6e-04 | Time: 2:00:59.814624 |\n",
      "| Epoch   2 |   144/  227 batches | Loss:   2.93 | Tokens:  1867 | Learning Rate: 1.6e-04 | Time: 2:01:46.515634 |\n",
      "| Epoch   2 |   145/  227 batches | Loss:   3.09 | Tokens:  1832 | Learning Rate: 1.6e-04 | Time: 2:02:32.356020 |\n",
      "| Epoch   2 |   146/  227 batches | Loss:   2.95 | Tokens:  1888 | Learning Rate: 1.6e-04 | Time: 2:03:17.589871 |\n",
      "| Epoch   2 |   147/  227 batches | Loss:   3.02 | Tokens:  1829 | Learning Rate: 1.6e-04 | Time: 2:04:04.761089 |\n",
      "| Epoch   2 |   148/  227 batches | Loss:   2.85 | Tokens:  1764 | Learning Rate: 1.6e-04 | Time: 2:04:48.781342 |\n",
      "| Epoch   2 |   149/  227 batches | Loss:   2.96 | Tokens:  1790 | Learning Rate: 1.6e-04 | Time: 2:05:34.681568 |\n",
      "| Epoch   2 |   150/  227 batches | Loss:   3.07 | Tokens:  1889 | Learning Rate: 1.6e-04 | Time: 2:06:19.522626 |\n",
      "| Epoch   2 |   151/  227 batches | Loss:   2.95 | Tokens:  1870 | Learning Rate: 1.6e-04 | Time: 2:07:01.740701 |\n",
      "| Epoch   2 |   152/  227 batches | Loss:   3.13 | Tokens:  1898 | Learning Rate: 1.6e-04 | Time: 2:07:47.000927 |\n",
      "| Epoch   2 |   153/  227 batches | Loss:   3.05 | Tokens:  1892 | Learning Rate: 1.6e-04 | Time: 2:08:34.693865 |\n",
      "| Epoch   2 |   154/  227 batches | Loss:   2.96 | Tokens:  1804 | Learning Rate: 1.6e-04 | Time: 2:09:20.713846 |\n",
      "| Epoch   2 |   155/  227 batches | Loss:   2.97 | Tokens:  1884 | Learning Rate: 1.6e-04 | Time: 2:10:07.749036 |\n",
      "| Epoch   2 |   156/  227 batches | Loss:   3.04 | Tokens:  1799 | Learning Rate: 1.6e-04 | Time: 2:10:52.208878 |\n",
      "| Epoch   2 |   157/  227 batches | Loss:   3.05 | Tokens:  1827 | Learning Rate: 1.6e-04 | Time: 2:11:37.551595 |\n",
      "| Epoch   2 |   158/  227 batches | Loss:   2.89 | Tokens:  1810 | Learning Rate: 1.6e-04 | Time: 2:12:23.060110 |\n",
      "| Epoch   2 |   159/  227 batches | Loss:   3.07 | Tokens:  1859 | Learning Rate: 1.7e-04 | Time: 2:13:10.691707 |\n",
      "| Epoch   2 |   160/  227 batches | Loss:   2.98 | Tokens:  1835 | Learning Rate: 1.7e-04 | Time: 2:13:59.239848 |\n",
      "| Epoch   2 |   161/  227 batches | Loss:   2.97 | Tokens:  1820 | Learning Rate: 1.7e-04 | Time: 2:14:45.819075 |\n",
      "| Epoch   2 |   162/  227 batches | Loss:   2.97 | Tokens:  1789 | Learning Rate: 1.7e-04 | Time: 2:15:31.584327 |\n",
      "| Epoch   2 |   163/  227 batches | Loss:   3.08 | Tokens:  1895 | Learning Rate: 1.7e-04 | Time: 2:16:20.419732 |\n",
      "| Epoch   2 |   164/  227 batches | Loss:   3.00 | Tokens:  1853 | Learning Rate: 1.7e-04 | Time: 2:17:05.067280 |\n",
      "| Epoch   2 |   165/  227 batches | Loss:   2.93 | Tokens:  1799 | Learning Rate: 1.7e-04 | Time: 2:17:46.851717 |\n",
      "| Epoch   2 |   166/  227 batches | Loss:   2.93 | Tokens:  1819 | Learning Rate: 1.7e-04 | Time: 2:18:34.134755 |\n",
      "| Epoch   2 |   167/  227 batches | Loss:   2.81 | Tokens:  1806 | Learning Rate: 1.7e-04 | Time: 2:19:19.722815 |\n",
      "| Epoch   2 |   168/  227 batches | Loss:   3.05 | Tokens:  1901 | Learning Rate: 1.7e-04 | Time: 2:20:05.654055 |\n",
      "| Epoch   2 |   169/  227 batches | Loss:   2.83 | Tokens:  1805 | Learning Rate: 1.7e-04 | Time: 2:20:50.006510 |\n",
      "| Epoch   2 |   170/  227 batches | Loss:   2.84 | Tokens:  1851 | Learning Rate: 1.7e-04 | Time: 2:21:35.829940 |\n",
      "| Epoch   2 |   171/  227 batches | Loss:   3.04 | Tokens:  1858 | Learning Rate: 1.7e-04 | Time: 2:22:22.201814 |\n",
      "| Epoch   2 |   172/  227 batches | Loss:   3.01 | Tokens:  1884 | Learning Rate: 1.7e-04 | Time: 2:23:05.601728 |\n",
      "| Epoch   2 |   173/  227 batches | Loss:   2.92 | Tokens:  1852 | Learning Rate: 1.7e-04 | Time: 2:23:51.622631 |\n",
      "| Epoch   2 |   174/  227 batches | Loss:   2.94 | Tokens:  1837 | Learning Rate: 1.7e-04 | Time: 2:24:38.464340 |\n",
      "| Epoch   2 |   175/  227 batches | Loss:   2.84 | Tokens:  1800 | Learning Rate: 1.7e-04 | Time: 2:25:24.953460 |\n",
      "| Epoch   2 |   176/  227 batches | Loss:   2.78 | Tokens:  1810 | Learning Rate: 1.7e-04 | Time: 2:26:10.518056 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   2 |   177/  227 batches | Loss:   2.92 | Tokens:  1848 | Learning Rate: 1.7e-04 | Time: 2:26:55.260376 |\n",
      "| Epoch   2 |   178/  227 batches | Loss:   3.01 | Tokens:  1800 | Learning Rate: 1.7e-04 | Time: 2:27:41.042916 |\n",
      "| Epoch   2 |   179/  227 batches | Loss:   2.79 | Tokens:  1850 | Learning Rate: 1.7e-04 | Time: 2:28:23.346765 |\n",
      "| Epoch   2 |   180/  227 batches | Loss:   2.87 | Tokens:  1816 | Learning Rate: 1.7e-04 | Time: 2:29:09.629965 |\n",
      "| Epoch   2 |   181/  227 batches | Loss:   2.83 | Tokens:  1889 | Learning Rate: 1.7e-04 | Time: 2:29:52.875020 |\n",
      "| Epoch   2 |   182/  227 batches | Loss:   2.98 | Tokens:  1778 | Learning Rate: 1.7e-04 | Time: 2:30:36.982042 |\n",
      "| Epoch   2 |   183/  227 batches | Loss:   2.80 | Tokens:  1814 | Learning Rate: 1.7e-04 | Time: 2:31:24.277908 |\n",
      "| Epoch   2 |   184/  227 batches | Loss:   3.05 | Tokens:  1888 | Learning Rate: 1.7e-04 | Time: 2:32:09.854997 |\n",
      "| Epoch   2 |   185/  227 batches | Loss:   3.12 | Tokens:  1855 | Learning Rate: 1.7e-04 | Time: 2:32:55.423112 |\n",
      "| Epoch   2 |   186/  227 batches | Loss:   2.97 | Tokens:  1862 | Learning Rate: 1.7e-04 | Time: 2:33:38.286768 |\n",
      "| Epoch   2 |   187/  227 batches | Loss:   3.01 | Tokens:  1846 | Learning Rate: 1.7e-04 | Time: 2:34:23.390125 |\n",
      "| Epoch   2 |   188/  227 batches | Loss:   3.11 | Tokens:  1900 | Learning Rate: 1.7e-04 | Time: 2:35:08.866485 |\n",
      "| Epoch   2 |   189/  227 batches | Loss:   3.07 | Tokens:  1854 | Learning Rate: 1.7e-04 | Time: 2:36:01.997996 |\n",
      "| Epoch   2 |   190/  227 batches | Loss:   3.05 | Tokens:  1919 | Learning Rate: 1.7e-04 | Time: 2:36:47.631933 |\n",
      "| Epoch   2 |   191/  227 batches | Loss:   3.08 | Tokens:  1894 | Learning Rate: 1.7e-04 | Time: 2:37:31.152260 |\n",
      "| Epoch   2 |   192/  227 batches | Loss:   3.17 | Tokens:  2015 | Learning Rate: 1.7e-04 | Time: 2:38:16.081045 |\n",
      "| Epoch   2 |   193/  227 batches | Loss:   3.10 | Tokens:  1861 | Learning Rate: 1.7e-04 | Time: 2:38:59.499908 |\n",
      "| Epoch   2 |   194/  227 batches | Loss:   3.08 | Tokens:  1987 | Learning Rate: 1.7e-04 | Time: 2:39:46.614883 |\n",
      "| Epoch   2 |   195/  227 batches | Loss:   3.16 | Tokens:  1965 | Learning Rate: 1.7e-04 | Time: 2:40:30.264129 |\n",
      "| Epoch   2 |   196/  227 batches | Loss:   3.17 | Tokens:  2014 | Learning Rate: 1.8e-04 | Time: 2:41:13.534317 |\n",
      "| Epoch   2 |   197/  227 batches | Loss:   3.11 | Tokens:  1950 | Learning Rate: 1.8e-04 | Time: 2:42:00.039011 |\n",
      "| Epoch   2 |   198/  227 batches | Loss:   3.21 | Tokens:  1901 | Learning Rate: 1.8e-04 | Time: 2:42:47.634048 |\n",
      "| Epoch   2 |   199/  227 batches | Loss:   3.22 | Tokens:  2041 | Learning Rate: 1.8e-04 | Time: 2:43:32.813288 |\n",
      "| Epoch   2 |   200/  227 batches | Loss:   3.21 | Tokens:  1937 | Learning Rate: 1.8e-04 | Time: 2:44:15.934945 |\n",
      "| Epoch   2 |   201/  227 batches | Loss:   3.29 | Tokens:  1934 | Learning Rate: 1.8e-04 | Time: 2:45:01.784308 |\n",
      "| Epoch   2 |   202/  227 batches | Loss:   3.21 | Tokens:  1939 | Learning Rate: 1.8e-04 | Time: 2:45:46.454219 |\n",
      "| Epoch   2 |   203/  227 batches | Loss:   3.45 | Tokens:  2015 | Learning Rate: 1.8e-04 | Time: 2:46:31.452671 |\n",
      "| Epoch   2 |   204/  227 batches | Loss:   3.13 | Tokens:  1894 | Learning Rate: 1.8e-04 | Time: 2:47:17.480555 |\n",
      "| Epoch   2 |   205/  227 batches | Loss:   3.16 | Tokens:  1985 | Learning Rate: 1.8e-04 | Time: 2:48:03.719118 |\n",
      "| Epoch   2 |   206/  227 batches | Loss:   3.23 | Tokens:  1928 | Learning Rate: 1.8e-04 | Time: 2:48:49.865684 |\n",
      "| Epoch   2 |   207/  227 batches | Loss:   3.27 | Tokens:  1855 | Learning Rate: 1.8e-04 | Time: 2:49:32.826678 |\n",
      "| Epoch   2 |   208/  227 batches | Loss:   3.26 | Tokens:  1938 | Learning Rate: 1.8e-04 | Time: 2:50:17.346595 |\n",
      "| Epoch   2 |   209/  227 batches | Loss:   3.31 | Tokens:  2011 | Learning Rate: 1.8e-04 | Time: 2:51:01.551357 |\n",
      "| Epoch   2 |   210/  227 batches | Loss:   3.25 | Tokens:  1921 | Learning Rate: 1.8e-04 | Time: 2:51:46.544231 |\n",
      "| Epoch   2 |   211/  227 batches | Loss:   3.37 | Tokens:  1959 | Learning Rate: 1.8e-04 | Time: 2:52:32.371652 |\n",
      "| Epoch   2 |   212/  227 batches | Loss:   3.17 | Tokens:  1962 | Learning Rate: 1.8e-04 | Time: 2:53:19.848697 |\n",
      "| Epoch   2 |   213/  227 batches | Loss:   3.29 | Tokens:  1922 | Learning Rate: 1.8e-04 | Time: 2:54:05.439925 |\n",
      "| Epoch   2 |   214/  227 batches | Loss:   3.10 | Tokens:  1906 | Learning Rate: 1.8e-04 | Time: 2:54:51.045937 |\n",
      "| Epoch   2 |   215/  227 batches | Loss:   3.08 | Tokens:  1911 | Learning Rate: 1.8e-04 | Time: 2:55:36.100425 |\n",
      "| Epoch   2 |   216/  227 batches | Loss:   3.01 | Tokens:  1840 | Learning Rate: 1.8e-04 | Time: 2:56:20.698972 |\n",
      "| Epoch   2 |   217/  227 batches | Loss:   3.29 | Tokens:  1967 | Learning Rate: 1.8e-04 | Time: 2:57:07.391081 |\n",
      "| Epoch   2 |   218/  227 batches | Loss:   3.25 | Tokens:  1920 | Learning Rate: 1.8e-04 | Time: 2:57:55.822536 |\n",
      "| Epoch   2 |   219/  227 batches | Loss:   3.27 | Tokens:  1933 | Learning Rate: 1.8e-04 | Time: 2:58:41.629011 |\n",
      "| Epoch   2 |   220/  227 batches | Loss:   3.34 | Tokens:  2004 | Learning Rate: 1.8e-04 | Time: 2:59:26.727964 |\n",
      "| Epoch   2 |   221/  227 batches | Loss:   3.08 | Tokens:  1910 | Learning Rate: 1.8e-04 | Time: 3:00:11.750792 |\n",
      "| Epoch   2 |   222/  227 batches | Loss:   3.02 | Tokens:  1899 | Learning Rate: 1.8e-04 | Time: 3:00:55.578620 |\n",
      "| Epoch   2 |   223/  227 batches | Loss:   3.00 | Tokens:  1864 | Learning Rate: 1.8e-04 | Time: 3:01:40.392750 |\n",
      "| Epoch   2 |   224/  227 batches | Loss:   2.92 | Tokens:  1752 | Learning Rate: 1.8e-04 | Time: 3:02:25.918977 |\n",
      "| Epoch   2 |   225/  227 batches | Loss:   2.82 | Tokens:  1765 | Learning Rate: 1.8e-04 | Time: 3:03:10.650328 |\n",
      "| Epoch   2 |   226/  227 batches | Loss:   2.98 | Tokens:   981 | Learning Rate: 1.8e-04 | Time: 3:03:31.648165 |\n",
      "| Epoch   2 |     0/    8 batches | Loss:   2.95 | Tokens:  1835 | Time: 0:00:12.959337 |\n",
      "| Epoch   2 |     1/    8 batches | Loss:   2.76 | Tokens:  1756 | Time: 0:00:26.368469 |\n",
      "| Epoch   2 |     2/    8 batches | Loss:   2.83 | Tokens:  1796 | Time: 0:00:39.755662 |\n",
      "| Epoch   2 |     3/    8 batches | Loss:   2.75 | Tokens:  1690 | Time: 0:00:52.958347 |\n",
      "| Epoch   2 |     4/    8 batches | Loss:   2.93 | Tokens:  1840 | Time: 0:01:05.794013 |\n",
      "| Epoch   2 |     5/    8 batches | Loss:   2.78 | Tokens:  1882 | Time: 0:01:18.889984 |\n",
      "| Epoch   2 |     6/    8 batches | Loss:   2.87 | Tokens:  1877 | Time: 0:01:32.275182 |\n",
      "| Epoch   2 |     7/    8 batches | Loss:   3.08 | Tokens:  1765 | Time: 0:01:44.618167 |\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   2 | Train Loss:    3.110 | Val Loss:    2.868 | time: 3:05:18.068855 |\n",
      "-----------------------------------------------------------\n",
      "| Epoch   3 |     0/  227 batches | Loss:   2.80 | Tokens:  1792 | Learning Rate: 1.8e-04 | Time: 0:00:42.931168 |\n",
      "| Epoch   3 |     1/  227 batches | Loss:   2.90 | Tokens:  1795 | Learning Rate: 1.8e-04 | Time: 0:01:28.601327 |\n",
      "| Epoch   3 |     2/  227 batches | Loss:   2.80 | Tokens:  1791 | Learning Rate: 1.8e-04 | Time: 0:02:16.154134 |\n",
      "| Epoch   3 |     3/  227 batches | Loss:   2.94 | Tokens:  1813 | Learning Rate: 1.8e-04 | Time: 0:03:00.551378 |\n",
      "| Epoch   3 |     4/  227 batches | Loss:   2.90 | Tokens:  1807 | Learning Rate: 1.8e-04 | Time: 0:03:45.691637 |\n",
      "| Epoch   3 |     5/  227 batches | Loss:   2.77 | Tokens:  1824 | Learning Rate: 1.8e-04 | Time: 0:04:32.494738 |\n",
      "| Epoch   3 |     6/  227 batches | Loss:   2.96 | Tokens:  1794 | Learning Rate: 1.9e-04 | Time: 0:05:17.396635 |\n",
      "| Epoch   3 |     7/  227 batches | Loss:   2.79 | Tokens:  1796 | Learning Rate: 1.9e-04 | Time: 0:06:00.174212 |\n",
      "| Epoch   3 |     8/  227 batches | Loss:   2.75 | Tokens:  1748 | Learning Rate: 1.9e-04 | Time: 0:06:46.646907 |\n",
      "| Epoch   3 |     9/  227 batches | Loss:   2.73 | Tokens:  1834 | Learning Rate: 1.9e-04 | Time: 0:07:30.520556 |\n",
      "| Epoch   3 |    10/  227 batches | Loss:   2.95 | Tokens:  1797 | Learning Rate: 1.9e-04 | Time: 0:08:15.055435 |\n",
      "| Epoch   3 |    11/  227 batches | Loss:   2.85 | Tokens:  1745 | Learning Rate: 1.9e-04 | Time: 0:08:58.982936 |\n",
      "| Epoch   3 |    12/  227 batches | Loss:   2.86 | Tokens:  1776 | Learning Rate: 1.9e-04 | Time: 0:09:42.994944 |\n",
      "| Epoch   3 |    13/  227 batches | Loss:   2.78 | Tokens:  1722 | Learning Rate: 1.9e-04 | Time: 0:10:30.161782 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   3 |    14/  227 batches | Loss:   2.87 | Tokens:  1782 | Learning Rate: 1.9e-04 | Time: 0:11:11.657789 |\n",
      "| Epoch   3 |    15/  227 batches | Loss:   2.85 | Tokens:  1825 | Learning Rate: 1.9e-04 | Time: 0:11:55.000855 |\n",
      "| Epoch   3 |    16/  227 batches | Loss:   2.75 | Tokens:  1788 | Learning Rate: 1.9e-04 | Time: 0:12:40.571961 |\n",
      "| Epoch   3 |    17/  227 batches | Loss:   2.82 | Tokens:  1793 | Learning Rate: 1.9e-04 | Time: 0:13:27.777149 |\n",
      "| Epoch   3 |    18/  227 batches | Loss:   2.94 | Tokens:  1935 | Learning Rate: 1.9e-04 | Time: 0:14:11.391490 |\n",
      "| Epoch   3 |    19/  227 batches | Loss:   2.81 | Tokens:  1785 | Learning Rate: 1.9e-04 | Time: 0:14:55.296054 |\n",
      "| Epoch   3 |    20/  227 batches | Loss:   2.88 | Tokens:  1748 | Learning Rate: 1.9e-04 | Time: 0:15:42.419009 |\n",
      "| Epoch   3 |    21/  227 batches | Loss:   2.81 | Tokens:  1762 | Learning Rate: 1.9e-04 | Time: 0:16:23.383437 |\n",
      "| Epoch   3 |    22/  227 batches | Loss:   2.84 | Tokens:  1734 | Learning Rate: 1.9e-04 | Time: 0:17:06.875104 |\n",
      "| Epoch   3 |    23/  227 batches | Loss:   2.82 | Tokens:  1732 | Learning Rate: 1.9e-04 | Time: 0:17:52.071214 |\n",
      "| Epoch   3 |    24/  227 batches | Loss:   2.82 | Tokens:  1739 | Learning Rate: 1.9e-04 | Time: 0:18:36.979419 |\n",
      "| Epoch   3 |    25/  227 batches | Loss:   2.70 | Tokens:  1793 | Learning Rate: 1.9e-04 | Time: 0:19:24.264939 |\n",
      "| Epoch   3 |    26/  227 batches | Loss:   2.84 | Tokens:  1839 | Learning Rate: 1.9e-04 | Time: 0:20:08.100688 |\n",
      "| Epoch   3 |    27/  227 batches | Loss:   2.86 | Tokens:  1790 | Learning Rate: 1.9e-04 | Time: 0:20:53.919134 |\n",
      "| Epoch   3 |    28/  227 batches | Loss:   2.62 | Tokens:  1727 | Learning Rate: 1.9e-04 | Time: 0:21:35.275512 |\n",
      "| Epoch   3 |    29/  227 batches | Loss:   2.77 | Tokens:  1773 | Learning Rate: 1.9e-04 | Time: 0:22:19.142177 |\n",
      "| Epoch   3 |    30/  227 batches | Loss:   2.87 | Tokens:  1803 | Learning Rate: 1.9e-04 | Time: 0:23:05.891133 |\n",
      "| Epoch   3 |    31/  227 batches | Loss:   2.82 | Tokens:  1729 | Learning Rate: 1.9e-04 | Time: 0:23:52.090561 |\n",
      "| Epoch   3 |    32/  227 batches | Loss:   2.82 | Tokens:  1722 | Learning Rate: 1.9e-04 | Time: 0:24:35.671985 |\n",
      "| Epoch   3 |    33/  227 batches | Loss:   2.75 | Tokens:  1670 | Learning Rate: 1.9e-04 | Time: 0:25:19.001089 |\n",
      "| Epoch   3 |    34/  227 batches | Loss:   2.67 | Tokens:  1771 | Learning Rate: 1.9e-04 | Time: 0:26:04.497396 |\n",
      "| Epoch   3 |    35/  227 batches | Loss:   2.82 | Tokens:  1737 | Learning Rate: 1.9e-04 | Time: 0:26:47.814865 |\n",
      "| Epoch   3 |    36/  227 batches | Loss:   2.61 | Tokens:  1693 | Learning Rate: 1.9e-04 | Time: 0:27:31.902939 |\n",
      "| Epoch   3 |    37/  227 batches | Loss:   2.78 | Tokens:  1748 | Learning Rate: 1.9e-04 | Time: 0:28:15.126324 |\n",
      "| Epoch   3 |    38/  227 batches | Loss:   2.71 | Tokens:  1842 | Learning Rate: 1.9e-04 | Time: 0:29:00.160865 |\n",
      "| Epoch   3 |    39/  227 batches | Loss:   2.79 | Tokens:  1784 | Learning Rate: 1.9e-04 | Time: 0:29:44.153595 |\n",
      "| Epoch   3 |    40/  227 batches | Loss:   2.87 | Tokens:  1747 | Learning Rate: 1.9e-04 | Time: 0:30:29.346714 |\n",
      "| Epoch   3 |    41/  227 batches | Loss:   2.68 | Tokens:  1806 | Learning Rate: 1.9e-04 | Time: 0:31:15.947065 |\n",
      "| Epoch   3 |    42/  227 batches | Loss:   2.78 | Tokens:  1811 | Learning Rate: 1.9e-04 | Time: 0:32:01.036469 |\n",
      "| Epoch   3 |    43/  227 batches | Loss:   2.79 | Tokens:  1823 | Learning Rate: 1.9e-04 | Time: 0:32:44.223949 |\n",
      "| Epoch   3 |    44/  227 batches | Loss:   2.79 | Tokens:  1729 | Learning Rate: 2.0e-04 | Time: 0:33:30.771443 |\n",
      "| Epoch   3 |    45/  227 batches | Loss:   2.58 | Tokens:  1668 | Learning Rate: 2.0e-04 | Time: 0:34:14.948672 |\n",
      "| Epoch   3 |    46/  227 batches | Loss:   2.73 | Tokens:  1776 | Learning Rate: 2.0e-04 | Time: 0:34:58.758490 |\n",
      "| Epoch   3 |    47/  227 batches | Loss:   2.86 | Tokens:  1692 | Learning Rate: 2.0e-04 | Time: 0:35:44.142898 |\n",
      "| Epoch   3 |    48/  227 batches | Loss:   2.63 | Tokens:  1746 | Learning Rate: 2.0e-04 | Time: 0:36:30.025172 |\n",
      "| Epoch   3 |    49/  227 batches | Loss:   2.71 | Tokens:  1684 | Learning Rate: 2.0e-04 | Time: 0:37:14.766500 |\n",
      "| Epoch   3 |    50/  227 batches | Loss:   2.73 | Tokens:  1793 | Learning Rate: 2.0e-04 | Time: 0:38:01.138462 |\n",
      "| Epoch   3 |    51/  227 batches | Loss:   2.64 | Tokens:  1800 | Learning Rate: 2.0e-04 | Time: 0:38:47.516411 |\n",
      "| Epoch   3 |    52/  227 batches | Loss:   2.90 | Tokens:  1794 | Learning Rate: 2.0e-04 | Time: 0:39:33.628071 |\n",
      "| Epoch   3 |    53/  227 batches | Loss:   2.71 | Tokens:  1767 | Learning Rate: 2.0e-04 | Time: 0:40:20.050899 |\n",
      "| Epoch   3 |    54/  227 batches | Loss:   2.81 | Tokens:  1792 | Learning Rate: 2.0e-04 | Time: 0:41:05.012634 |\n",
      "| Epoch   3 |    55/  227 batches | Loss:   2.76 | Tokens:  1734 | Learning Rate: 2.0e-04 | Time: 0:41:50.795672 |\n",
      "| Epoch   3 |    56/  227 batches | Loss:   2.67 | Tokens:  1780 | Learning Rate: 2.0e-04 | Time: 0:42:35.580880 |\n",
      "| Epoch   3 |    57/  227 batches | Loss:   2.74 | Tokens:  1833 | Learning Rate: 2.0e-04 | Time: 0:43:22.070529 |\n",
      "| Epoch   3 |    58/  227 batches | Loss:   2.65 | Tokens:  1793 | Learning Rate: 2.0e-04 | Time: 0:44:06.163589 |\n",
      "| Epoch   3 |    59/  227 batches | Loss:   2.68 | Tokens:  1720 | Learning Rate: 2.0e-04 | Time: 0:44:52.382962 |\n",
      "| Epoch   3 |    60/  227 batches | Loss:   2.76 | Tokens:  1798 | Learning Rate: 2.0e-04 | Time: 0:45:36.565619 |\n",
      "| Epoch   3 |    61/  227 batches | Loss:   2.59 | Tokens:  1709 | Learning Rate: 2.0e-04 | Time: 0:46:21.931275 |\n",
      "| Epoch   3 |    62/  227 batches | Loss:   2.77 | Tokens:  1852 | Learning Rate: 2.0e-04 | Time: 0:47:08.350113 |\n",
      "| Epoch   3 |    63/  227 batches | Loss:   2.74 | Tokens:  1783 | Learning Rate: 2.0e-04 | Time: 0:47:55.131981 |\n",
      "| Epoch   3 |    64/  227 batches | Loss:   2.70 | Tokens:  1821 | Learning Rate: 2.0e-04 | Time: 0:48:43.659181 |\n",
      "| Epoch   3 |    65/  227 batches | Loss:   2.56 | Tokens:  1746 | Learning Rate: 2.0e-04 | Time: 0:49:30.295437 |\n",
      "| Epoch   3 |    66/  227 batches | Loss:   2.61 | Tokens:  1725 | Learning Rate: 2.0e-04 | Time: 0:50:13.823010 |\n",
      "| Epoch   3 |    67/  227 batches | Loss:   2.65 | Tokens:  1720 | Learning Rate: 2.0e-04 | Time: 0:50:57.299630 |\n",
      "| Epoch   3 |    68/  227 batches | Loss:   2.74 | Tokens:  1700 | Learning Rate: 2.0e-04 | Time: 0:51:43.644665 |\n",
      "| Epoch   3 |    69/  227 batches | Loss:   2.75 | Tokens:  1758 | Learning Rate: 2.0e-04 | Time: 0:52:27.087780 |\n",
      "| Epoch   3 |    70/  227 batches | Loss:   2.79 | Tokens:  1781 | Learning Rate: 2.0e-04 | Time: 0:53:12.274911 |\n",
      "| Epoch   3 |    71/  227 batches | Loss:   2.81 | Tokens:  1755 | Learning Rate: 2.0e-04 | Time: 0:54:00.014218 |\n",
      "| Epoch   3 |    72/  227 batches | Loss:   2.73 | Tokens:  1710 | Learning Rate: 2.0e-04 | Time: 0:54:48.438691 |\n",
      "| Epoch   3 |    73/  227 batches | Loss:   2.80 | Tokens:  1669 | Learning Rate: 2.0e-04 | Time: 0:55:34.966240 |\n",
      "| Epoch   3 |    74/  227 batches | Loss:   2.81 | Tokens:  1764 | Learning Rate: 2.0e-04 | Time: 0:56:19.978842 |\n",
      "| Epoch   3 |    75/  227 batches | Loss:   2.60 | Tokens:  1747 | Learning Rate: 2.0e-04 | Time: 0:57:05.926939 |\n",
      "| Epoch   3 |    76/  227 batches | Loss:   2.69 | Tokens:  1781 | Learning Rate: 2.0e-04 | Time: 0:57:50.194356 |\n",
      "| Epoch   3 |    77/  227 batches | Loss:   2.58 | Tokens:  1721 | Learning Rate: 2.0e-04 | Time: 0:58:36.768777 |\n",
      "| Epoch   3 |    78/  227 batches | Loss:   2.75 | Tokens:  1723 | Learning Rate: 2.0e-04 | Time: 0:59:25.033304 |\n",
      "| Epoch   3 |    79/  227 batches | Loss:   2.78 | Tokens:  1710 | Learning Rate: 2.0e-04 | Time: 1:00:08.424395 |\n",
      "| Epoch   3 |    80/  227 batches | Loss:   2.71 | Tokens:  1721 | Learning Rate: 2.0e-04 | Time: 1:00:52.746570 |\n",
      "| Epoch   3 |    81/  227 batches | Loss:   2.61 | Tokens:  1737 | Learning Rate: 2.1e-04 | Time: 1:01:41.430349 |\n",
      "| Epoch   3 |    82/  227 batches | Loss:   2.61 | Tokens:  1690 | Learning Rate: 2.1e-04 | Time: 1:02:26.589558 |\n",
      "| Epoch   3 |    83/  227 batches | Loss:   2.73 | Tokens:  1668 | Learning Rate: 2.1e-04 | Time: 1:03:09.983643 |\n",
      "| Epoch   3 |    84/  227 batches | Loss:   2.76 | Tokens:  1690 | Learning Rate: 2.1e-04 | Time: 1:03:53.693594 |\n",
      "| Epoch   3 |    85/  227 batches | Loss:   2.62 | Tokens:  1712 | Learning Rate: 2.1e-04 | Time: 1:04:41.229445 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   3 |    86/  227 batches | Loss:   2.71 | Tokens:  1714 | Learning Rate: 2.1e-04 | Time: 1:05:25.284606 |\n",
      "| Epoch   3 |    87/  227 batches | Loss:   2.60 | Tokens:  1705 | Learning Rate: 2.1e-04 | Time: 1:06:10.676192 |\n",
      "| Epoch   3 |    88/  227 batches | Loss:   2.70 | Tokens:  1684 | Learning Rate: 2.1e-04 | Time: 1:06:56.464717 |\n",
      "| Epoch   3 |    89/  227 batches | Loss:   2.55 | Tokens:  1671 | Learning Rate: 2.1e-04 | Time: 1:07:38.974014 |\n",
      "| Epoch   3 |    90/  227 batches | Loss:   2.85 | Tokens:  1713 | Learning Rate: 2.1e-04 | Time: 1:08:22.211361 |\n",
      "| Epoch   3 |    91/  227 batches | Loss:   2.64 | Tokens:  1691 | Learning Rate: 2.1e-04 | Time: 1:09:10.931047 |\n",
      "| Epoch   3 |    92/  227 batches | Loss:   2.78 | Tokens:  1741 | Learning Rate: 2.1e-04 | Time: 1:09:55.163733 |\n",
      "| Epoch   3 |    93/  227 batches | Loss:   2.57 | Tokens:  1663 | Learning Rate: 2.1e-04 | Time: 1:10:40.005788 |\n",
      "| Epoch   3 |    94/  227 batches | Loss:   2.60 | Tokens:  1686 | Learning Rate: 2.1e-04 | Time: 1:11:26.814258 |\n",
      "| Epoch   3 |    95/  227 batches | Loss:   2.64 | Tokens:  1729 | Learning Rate: 2.1e-04 | Time: 1:12:10.705856 |\n",
      "| Epoch   3 |    96/  227 batches | Loss:   2.59 | Tokens:  1765 | Learning Rate: 2.1e-04 | Time: 1:12:57.266463 |\n",
      "| Epoch   3 |    97/  227 batches | Loss:   2.66 | Tokens:  1682 | Learning Rate: 2.1e-04 | Time: 1:13:42.993156 |\n",
      "| Epoch   3 |    98/  227 batches | Loss:   2.52 | Tokens:  1728 | Learning Rate: 2.1e-04 | Time: 1:14:27.967855 |\n",
      "| Epoch   3 |    99/  227 batches | Loss:   2.68 | Tokens:  1701 | Learning Rate: 2.1e-04 | Time: 1:15:13.738707 |\n",
      "| Epoch   3 |   100/  227 batches | Loss:   2.63 | Tokens:  1694 | Learning Rate: 2.1e-04 | Time: 1:15:57.431835 |\n",
      "| Epoch   3 |   101/  227 batches | Loss:   2.58 | Tokens:  1716 | Learning Rate: 2.1e-04 | Time: 1:16:43.902534 |\n",
      "| Epoch   3 |   102/  227 batches | Loss:   2.59 | Tokens:  1758 | Learning Rate: 2.1e-04 | Time: 1:17:30.605613 |\n",
      "| Epoch   3 |   103/  227 batches | Loss:   2.78 | Tokens:  1728 | Learning Rate: 2.1e-04 | Time: 1:18:16.214402 |\n",
      "| Epoch   3 |   104/  227 batches | Loss:   2.64 | Tokens:  1717 | Learning Rate: 2.1e-04 | Time: 1:19:02.086703 |\n",
      "| Epoch   3 |   105/  227 batches | Loss:   2.67 | Tokens:  1740 | Learning Rate: 2.1e-04 | Time: 1:19:46.009219 |\n",
      "| Epoch   3 |   106/  227 batches | Loss:   2.50 | Tokens:  1695 | Learning Rate: 2.1e-04 | Time: 1:20:33.211960 |\n",
      "| Epoch   3 |   107/  227 batches | Loss:   2.62 | Tokens:  1766 | Learning Rate: 2.1e-04 | Time: 1:21:17.497206 |\n",
      "| Epoch   3 |   108/  227 batches | Loss:   2.72 | Tokens:  1750 | Learning Rate: 2.1e-04 | Time: 1:22:01.943322 |\n",
      "| Epoch   3 |   109/  227 batches | Loss:   2.62 | Tokens:  1725 | Learning Rate: 2.1e-04 | Time: 1:22:46.677766 |\n",
      "| Epoch   3 |   110/  227 batches | Loss:   2.64 | Tokens:  1700 | Learning Rate: 2.1e-04 | Time: 1:23:30.030806 |\n",
      "| Epoch   3 |   111/  227 batches | Loss:   2.52 | Tokens:  1678 | Learning Rate: 2.1e-04 | Time: 1:24:15.539079 |\n",
      "| Epoch   3 |   112/  227 batches | Loss:   2.68 | Tokens:  1741 | Learning Rate: 2.1e-04 | Time: 1:25:03.029553 |\n",
      "| Epoch   3 |   113/  227 batches | Loss:   2.58 | Tokens:  1659 | Learning Rate: 2.1e-04 | Time: 1:25:47.692466 |\n",
      "| Epoch   3 |   114/  227 batches | Loss:   2.79 | Tokens:  1703 | Learning Rate: 2.1e-04 | Time: 1:26:31.136263 |\n",
      "| Epoch   3 |   115/  227 batches | Loss:   2.67 | Tokens:  1812 | Learning Rate: 2.1e-04 | Time: 1:27:17.671789 |\n",
      "| Epoch   3 |   116/  227 batches | Loss:   2.55 | Tokens:  1765 | Learning Rate: 2.1e-04 | Time: 1:28:02.855932 |\n",
      "| Epoch   3 |   117/  227 batches | Loss:   2.68 | Tokens:  1705 | Learning Rate: 2.1e-04 | Time: 1:28:45.257515 |\n",
      "| Epoch   3 |   118/  227 batches | Loss:   2.73 | Tokens:  1787 | Learning Rate: 2.2e-04 | Time: 1:29:30.808675 |\n",
      "| Epoch   3 |   119/  227 batches | Loss:   2.73 | Tokens:  1861 | Learning Rate: 2.2e-04 | Time: 1:30:15.471083 |\n",
      "| Epoch   3 |   120/  227 batches | Loss:   2.79 | Tokens:  1808 | Learning Rate: 2.2e-04 | Time: 1:31:02.001624 |\n",
      "| Epoch   3 |   121/  227 batches | Loss:   2.78 | Tokens:  1817 | Learning Rate: 2.2e-04 | Time: 1:31:48.779501 |\n",
      "| Epoch   3 |   122/  227 batches | Loss:   2.75 | Tokens:  1773 | Learning Rate: 2.2e-04 | Time: 1:32:33.787536 |\n",
      "| Epoch   3 |   123/  227 batches | Loss:   2.67 | Tokens:  1815 | Learning Rate: 2.2e-04 | Time: 1:33:17.613310 |\n",
      "| Epoch   3 |   124/  227 batches | Loss:   2.80 | Tokens:  1835 | Learning Rate: 2.2e-04 | Time: 1:34:03.331026 |\n",
      "| Epoch   3 |   125/  227 batches | Loss:   2.80 | Tokens:  1915 | Learning Rate: 2.2e-04 | Time: 1:34:46.511525 |\n",
      "| Epoch   3 |   126/  227 batches | Loss:   2.67 | Tokens:  1790 | Learning Rate: 2.2e-04 | Time: 1:35:31.571997 |\n",
      "| Epoch   3 |   127/  227 batches | Loss:   2.85 | Tokens:  1831 | Learning Rate: 2.2e-04 | Time: 1:36:14.805279 |\n",
      "| Epoch   3 |   128/  227 batches | Loss:   2.82 | Tokens:  1776 | Learning Rate: 2.2e-04 | Time: 1:37:00.226786 |\n",
      "| Epoch   3 |   129/  227 batches | Loss:   2.85 | Tokens:  1890 | Learning Rate: 2.2e-04 | Time: 1:37:45.693952 |\n",
      "| Epoch   3 |   130/  227 batches | Loss:   2.70 | Tokens:  1800 | Learning Rate: 2.2e-04 | Time: 1:38:32.015805 |\n",
      "| Epoch   3 |   131/  227 batches | Loss:   2.79 | Tokens:  1787 | Learning Rate: 2.2e-04 | Time: 1:39:16.190646 |\n",
      "| Epoch   3 |   132/  227 batches | Loss:   2.65 | Tokens:  1809 | Learning Rate: 2.2e-04 | Time: 1:40:00.258772 |\n",
      "| Epoch   3 |   133/  227 batches | Loss:   2.75 | Tokens:  1821 | Learning Rate: 2.2e-04 | Time: 1:40:44.904355 |\n",
      "| Epoch   3 |   134/  227 batches | Loss:   2.84 | Tokens:  1887 | Learning Rate: 2.2e-04 | Time: 1:41:28.822882 |\n",
      "| Epoch   3 |   135/  227 batches | Loss:   2.83 | Tokens:  1865 | Learning Rate: 2.2e-04 | Time: 1:42:15.892978 |\n",
      "| Epoch   3 |   136/  227 batches | Loss:   2.78 | Tokens:  1843 | Learning Rate: 2.2e-04 | Time: 1:43:02.735892 |\n",
      "| Epoch   3 |   137/  227 batches | Loss:   2.74 | Tokens:  1822 | Learning Rate: 2.2e-04 | Time: 1:43:45.852563 |\n",
      "| Epoch   3 |   138/  227 batches | Loss:   2.67 | Tokens:  1836 | Learning Rate: 2.2e-04 | Time: 1:44:28.729874 |\n",
      "| Epoch   3 |   139/  227 batches | Loss:   2.71 | Tokens:  1857 | Learning Rate: 2.2e-04 | Time: 1:45:15.807950 |\n",
      "| Epoch   3 |   140/  227 batches | Loss:   2.61 | Tokens:  1798 | Learning Rate: 2.2e-04 | Time: 1:45:59.720543 |\n",
      "| Epoch   3 |   141/  227 batches | Loss:   2.56 | Tokens:  1829 | Learning Rate: 2.2e-04 | Time: 1:46:46.065580 |\n",
      "| Epoch   3 |   142/  227 batches | Loss:   2.57 | Tokens:  1882 | Learning Rate: 2.2e-04 | Time: 1:47:32.761676 |\n",
      "| Epoch   3 |   143/  227 batches | Loss:   2.67 | Tokens:  1860 | Learning Rate: 2.2e-04 | Time: 1:48:20.657564 |\n",
      "| Epoch   3 |   144/  227 batches | Loss:   2.57 | Tokens:  1867 | Learning Rate: 2.2e-04 | Time: 1:49:05.996956 |\n",
      "| Epoch   3 |   145/  227 batches | Loss:   2.74 | Tokens:  1832 | Learning Rate: 2.2e-04 | Time: 1:49:52.450701 |\n",
      "| Epoch   3 |   146/  227 batches | Loss:   2.60 | Tokens:  1888 | Learning Rate: 2.2e-04 | Time: 1:50:38.868542 |\n",
      "| Epoch   3 |   147/  227 batches | Loss:   2.71 | Tokens:  1829 | Learning Rate: 2.2e-04 | Time: 1:51:24.731713 |\n",
      "| Epoch   3 |   148/  227 batches | Loss:   2.50 | Tokens:  1764 | Learning Rate: 2.2e-04 | Time: 1:52:08.270257 |\n",
      "| Epoch   3 |   149/  227 batches | Loss:   2.63 | Tokens:  1790 | Learning Rate: 2.2e-04 | Time: 1:52:52.245630 |\n",
      "| Epoch   3 |   150/  227 batches | Loss:   2.74 | Tokens:  1889 | Learning Rate: 2.2e-04 | Time: 1:53:39.717652 |\n",
      "| Epoch   3 |   151/  227 batches | Loss:   2.58 | Tokens:  1870 | Learning Rate: 2.2e-04 | Time: 1:54:28.874931 |\n",
      "| Epoch   3 |   152/  227 batches | Loss:   2.80 | Tokens:  1898 | Learning Rate: 2.2e-04 | Time: 1:55:15.323567 |\n",
      "| Epoch   3 |   153/  227 batches | Loss:   2.70 | Tokens:  1892 | Learning Rate: 2.2e-04 | Time: 1:56:01.131042 |\n",
      "| Epoch   3 |   154/  227 batches | Loss:   2.59 | Tokens:  1804 | Learning Rate: 2.2e-04 | Time: 1:56:45.918244 |\n",
      "| Epoch   3 |   155/  227 batches | Loss:   2.66 | Tokens:  1884 | Learning Rate: 2.3e-04 | Time: 1:57:32.312150 |\n",
      "| Epoch   3 |   156/  227 batches | Loss:   2.74 | Tokens:  1799 | Learning Rate: 2.3e-04 | Time: 1:58:17.324291 |\n",
      "| Epoch   3 |   157/  227 batches | Loss:   2.71 | Tokens:  1827 | Learning Rate: 2.3e-04 | Time: 1:59:03.258426 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch   3 |   158/  227 batches | Loss:   2.57 | Tokens:  1810 | Learning Rate: 2.3e-04 | Time: 1:59:46.647369 |\n",
      "| Epoch   3 |   159/  227 batches | Loss:   2.78 | Tokens:  1859 | Learning Rate: 2.3e-04 | Time: 2:00:31.097170 |\n",
      "| Epoch   3 |   160/  227 batches | Loss:   2.64 | Tokens:  1835 | Learning Rate: 2.3e-04 | Time: 2:01:15.170282 |\n",
      "| Epoch   3 |   161/  227 batches | Loss:   2.64 | Tokens:  1820 | Learning Rate: 2.3e-04 | Time: 2:02:03.145849 |\n",
      "| Epoch   3 |   162/  227 batches | Loss:   2.63 | Tokens:  1789 | Learning Rate: 2.3e-04 | Time: 2:02:47.502204 |\n",
      "| Epoch   3 |   163/  227 batches | Loss:   2.81 | Tokens:  1895 | Learning Rate: 2.3e-04 | Time: 2:03:33.069322 |\n",
      "| Epoch   3 |   164/  227 batches | Loss:   2.71 | Tokens:  1853 | Learning Rate: 2.3e-04 | Time: 2:04:19.061302 |\n",
      "| Epoch   3 |   165/  227 batches | Loss:   2.63 | Tokens:  1799 | Learning Rate: 2.3e-04 | Time: 2:05:00.910363 |\n",
      "| Epoch   3 |   166/  227 batches | Loss:   2.61 | Tokens:  1819 | Learning Rate: 2.3e-04 | Time: 2:05:46.022697 |\n",
      "| Epoch   3 |   167/  227 batches | Loss:   2.49 | Tokens:  1806 | Learning Rate: 2.3e-04 | Time: 2:06:31.206838 |\n",
      "| Epoch   3 |   168/  227 batches | Loss:   2.73 | Tokens:  1901 | Learning Rate: 2.3e-04 | Time: 2:07:16.584162 |\n",
      "| Epoch   3 |   169/  227 batches | Loss:   2.50 | Tokens:  1805 | Learning Rate: 2.3e-04 | Time: 2:08:01.848120 |\n",
      "| Epoch   3 |   170/  227 batches | Loss:   2.55 | Tokens:  1851 | Learning Rate: 2.3e-04 | Time: 2:08:47.714489 |\n",
      "| Epoch   3 |   171/  227 batches | Loss:   2.72 | Tokens:  1858 | Learning Rate: 2.3e-04 | Time: 2:09:31.981083 |\n",
      "| Epoch   3 |   172/  227 batches | Loss:   2.67 | Tokens:  1884 | Learning Rate: 2.3e-04 | Time: 2:10:15.638309 |\n",
      "| Epoch   3 |   173/  227 batches | Loss:   2.57 | Tokens:  1852 | Learning Rate: 2.3e-04 | Time: 2:11:01.879622 |\n",
      "| Epoch   3 |   174/  227 batches | Loss:   2.63 | Tokens:  1837 | Learning Rate: 2.3e-04 | Time: 2:11:48.642543 |\n",
      "| Epoch   3 |   175/  227 batches | Loss:   2.50 | Tokens:  1800 | Learning Rate: 2.3e-04 | Time: 2:12:33.537456 |\n",
      "| Epoch   3 |   176/  227 batches | Loss:   2.47 | Tokens:  1810 | Learning Rate: 2.3e-04 | Time: 2:13:20.109883 |\n",
      "| Epoch   3 |   177/  227 batches | Loss:   2.57 | Tokens:  1848 | Learning Rate: 2.3e-04 | Time: 2:14:04.641768 |\n",
      "| Epoch   3 |   178/  227 batches | Loss:   2.72 | Tokens:  1800 | Learning Rate: 2.3e-04 | Time: 2:14:52.789983 |\n",
      "| Epoch   3 |   179/  227 batches | Loss:   2.47 | Tokens:  1850 | Learning Rate: 2.3e-04 | Time: 2:15:42.629670 |\n",
      "| Epoch   3 |   180/  227 batches | Loss:   2.55 | Tokens:  1816 | Learning Rate: 2.3e-04 | Time: 2:16:30.240321 |\n",
      "| Epoch   3 |   181/  227 batches | Loss:   2.52 | Tokens:  1889 | Learning Rate: 2.3e-04 | Time: 2:17:17.013235 |\n",
      "| Epoch   3 |   182/  227 batches | Loss:   2.67 | Tokens:  1778 | Learning Rate: 2.3e-04 | Time: 2:18:06.922737 |\n",
      "| Epoch   3 |   183/  227 batches | Loss:   2.49 | Tokens:  1814 | Learning Rate: 2.3e-04 | Time: 2:18:53.416757 |\n",
      "| Epoch   3 |   184/  227 batches | Loss:   2.73 | Tokens:  1888 | Learning Rate: 2.3e-04 | Time: 2:19:39.070640 |\n",
      "| Epoch   3 |   185/  227 batches | Loss:   2.78 | Tokens:  1855 | Learning Rate: 2.3e-04 | Time: 2:20:24.458056 |\n",
      "| Epoch   3 |   186/  227 batches | Loss:   2.65 | Tokens:  1862 | Learning Rate: 2.3e-04 | Time: 2:21:08.848322 |\n",
      "| Epoch   3 |   187/  227 batches | Loss:   2.68 | Tokens:  1846 | Learning Rate: 2.3e-04 | Time: 2:21:55.969283 |\n",
      "| Epoch   3 |   188/  227 batches | Loss:   2.79 | Tokens:  1900 | Learning Rate: 2.3e-04 | Time: 2:22:39.110887 |\n",
      "| Epoch   3 |   189/  227 batches | Loss:   2.77 | Tokens:  1854 | Learning Rate: 2.3e-04 | Time: 2:23:26.321607 |\n",
      "| Epoch   3 |   190/  227 batches | Loss:   2.76 | Tokens:  1919 | Learning Rate: 2.3e-04 | Time: 2:24:14.285059 |\n",
      "| Epoch   3 |   191/  227 batches | Loss:   2.79 | Tokens:  1894 | Learning Rate: 2.3e-04 | Time: 2:24:59.759424 |\n",
      "| Epoch   3 |   192/  227 batches | Loss:   2.87 | Tokens:  2015 | Learning Rate: 2.4e-04 | Time: 2:25:41.698246 |\n",
      "| Epoch   3 |   193/  227 batches | Loss:   2.82 | Tokens:  1861 | Learning Rate: 2.4e-04 | Time: 2:26:24.993441 |\n",
      "| Epoch   3 |   194/  227 batches | Loss:   2.76 | Tokens:  1987 | Learning Rate: 2.4e-04 | Time: 2:27:11.533955 |\n",
      "| Epoch   3 |   195/  227 batches | Loss:   2.90 | Tokens:  1965 | Learning Rate: 2.4e-04 | Time: 2:27:57.330535 |\n",
      "| Epoch   3 |   196/  227 batches | Loss:   2.85 | Tokens:  2014 | Learning Rate: 2.4e-04 | Time: 2:28:41.486427 |\n",
      "| Epoch   3 |   197/  227 batches | Loss:   2.83 | Tokens:  1950 | Learning Rate: 2.4e-04 | Time: 2:29:28.030490 |\n",
      "| Epoch   3 |   198/  227 batches | Loss:   2.87 | Tokens:  1901 | Learning Rate: 2.4e-04 | Time: 2:30:13.909773 |\n",
      "| Epoch   3 |   199/  227 batches | Loss:   2.96 | Tokens:  2041 | Learning Rate: 2.4e-04 | Time: 2:31:01.913372 |\n",
      "| Epoch   3 |   200/  227 batches | Loss:   2.89 | Tokens:  1937 | Learning Rate: 2.4e-04 | Time: 2:31:47.049634 |\n",
      "| Epoch   3 |   201/  227 batches | Loss:   2.95 | Tokens:  1934 | Learning Rate: 2.4e-04 | Time: 2:32:32.811186 |\n",
      "| Epoch   3 |   202/  227 batches | Loss:   2.88 | Tokens:  1939 | Learning Rate: 2.4e-04 | Time: 2:33:19.627960 |\n",
      "| Epoch   3 |   203/  227 batches | Loss:   3.11 | Tokens:  2015 | Learning Rate: 2.4e-04 | Time: 2:34:05.094347 |\n",
      "| Epoch   3 |   204/  227 batches | Loss:   2.81 | Tokens:  1894 | Learning Rate: 2.4e-04 | Time: 2:34:48.560086 |\n",
      "| Epoch   3 |   205/  227 batches | Loss:   2.84 | Tokens:  1985 | Learning Rate: 2.4e-04 | Time: 2:35:36.698766 |\n",
      "| Epoch   3 |   206/  227 batches | Loss:   2.92 | Tokens:  1928 | Learning Rate: 2.4e-04 | Time: 2:36:20.809750 |\n"
     ]
    }
   ],
   "source": [
    "spacy_de, spacy_en = load_tokenizers()\n",
    "vocab_src, vocab_tgt = load_vocab(spacy_de, spacy_en)\n",
    "train_dataloader, valid_dataloader = create_dataloaders(vocab_src, vocab_tgt, spacy_de, spacy_en, batch_size=128, max_padding=72)\n",
    "\n",
    "src_vocab_size = len(vocab_src)\n",
    "tgt_vocab_size = len(vocab_tgt)\n",
    "pad_idx = vocab_src['<blank>']\n",
    "d_model = 512\n",
    "init_lr = 1.0\n",
    "warmup = 3000\n",
    "num_epochs = 8\n",
    "\n",
    "model = make_model(src_vocab_size, tgt_vocab_size, d_model)\n",
    "\n",
    "criterion = LabelSmoothingKL(vocab_size=tgt_vocab_size, padding_idx=pad_idx, smoothing=0.1)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=init_lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                           lr_lambda=lambda step: scheduler(step, d_model, factor=1, warmup=warmup))\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    train_loss = train_epoch(epoch,\n",
    "        train_dataloader,\n",
    "        model,\n",
    "        ComputeLoss(criterion),\n",
    "        optimizer,\n",
    "        lr_scheduler,\n",
    "        pad_idx\n",
    "    )\n",
    "    train_losses.append(train_loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = eval_epoch(epoch,\n",
    "        valid_dataloader,\n",
    "        model,\n",
    "        ComputeLoss(criterion),\n",
    "        pad_idx\n",
    "    )\n",
    "\n",
    "    val_losses.append(val_loss.item())\n",
    "    print('-' * 59)\n",
    "    print('| End of epoch {:3d} | Train Loss: {:8.3f} | Val Loss: {:8.3f} | time: {} |'\n",
    "          .format(epoch, train_loss, val_loss, timedelta(seconds=time.time()-start)))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='train_loss')\n",
    "plt.plot(val_losses, label='val_loss')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_py3.8",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "314.825px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
